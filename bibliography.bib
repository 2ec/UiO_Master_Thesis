@inproceedings{A800276Pdf,
  title = {A800276.Pdf},
  url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf},
  urldate = {2023-03-10}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  file = {/Users/emilchri/Zotero/storage/AZRC3A9V/Adadi og Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf;/Users/emilchri/Zotero/storage/H6YFR9XP/8466590.html}
}

@misc{agrawalVQAVisualQuestion2016,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
  date = {2016-10-26},
  number = {arXiv:1505.00468},
  eprint = {1505.00468},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1505.00468},
  urldate = {2022-10-03},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing \textasciitilde 0.25M images, \textasciitilde 0.76M questions, and \textasciitilde 10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/CXSE52DD/Agrawal et al. - 2016 - VQA Visual Question Answering.pdf;/Users/emilchri/Zotero/storage/QTNA84XM/1505.html}
}

@article{alyafeaiFullyautomatedDeepLearning2020,
  title = {A Fully-Automated Deep Learning Pipeline for Cervical Cancer Classification},
  author = {Alyafeai, Zaid and Ghouti, Lahouari},
  date = {2020-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {141},
  pages = {112951},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.112951},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417419306694},
  urldate = {2023-03-10},
  abstract = {Cervical cancer ranks the fourth most common cancer among females worldwide with roughly 528, 000 new cases yearly. Around 85\% of the new cases occurred in less-developed countries. In these countries, the high fatality rate is mainly attributed to the lack of skilled medical staff and appropriate medical pre-screening procedures. Images capturing the cervical region, known as cervigrams, are the gold-standard for the basic evaluation of cervical cancer presence. Cervigrams have high inter-rater variability especially among less skilled medical specialists. In this paper, we develop a fully-automated pipeline for cervix detection and cervical cancer classification from cervigram images. The proposed pipeline consists of two pre-trained deep learning models for the automatic cervix detection and cervical tumor classification. The first model detects the cervix region 1000 times faster than state-of-the-art data-driven models while achieving a detection accuracy of 0.68 in terms of intersection of union (IoU) measure. Self-extracted features are used by the second model to classify the cervix tumors. These features are learned using two lightweight models based on convolutional neural networks (CNN). The proposed deep learning classifier outperforms existing models in terms of classification accuracy and speed. Our classifier is characterized by an area under the curve (AUC) score of 0.82 while classifying each cervix region 20 times faster. Finally, the pipeline accuracy, speed and lightweight architecture make it very appropriate for mobile phone deployment. Such deployment is expected to drastically enhance the early detection of cervical cancer in less-developed countries.},
  langid = {english},
  keywords = {Cervical cancer,Cervical region-of-interest,Cervix detection,Convolutional neural networks,Deep learning,graph,Guanacaste and Intel&mobileodt cervigram datasets,imagenet},
  file = {/Users/emilchri/Zotero/storage/5NB4LLA2/Alyafeai og Ghouti - 2020 - A fully-automated deep learning pipeline for cervi.pdf;/Users/emilchri/Zotero/storage/NW7RMS3M/S0957417419306694.html}
}

@article{andresenJohnMcCarthyFather2002,
  title = {John {{McCarthy}}: Father of {{AI}}},
  shorttitle = {John {{McCarthy}}},
  author = {Andresen, S.L.},
  date = {2002-09},
  journaltitle = {IEEE Intelligent Systems},
  volume = {17},
  number = {5},
  pages = {84--85},
  issn = {1941-1294},
  doi = {10.1109/MIS.2002.1039837},
  abstract = {If John McCarthy, the father of AI, were to coin a new phrase for "artificial intelligence" today, he would probably use "computational intelligence." McCarthy is not just the father of AI, he is also the inventor of the Lisp (list processing) language. The author considers McCarthy's conception of Lisp and discusses McCarthy's recent research that involves elaboration tolerance, creativity by machines, free will of machines, and some improved ways of doing situation calculus.},
  eventtitle = {{{IEEE Intelligent Systems}}},
  keywords = {Artificial intelligence,Computer languages,Computer science,Friction,Humans,Laboratories,Logic,Mathematical programming,Mathematics,Time sharing computer systems},
  file = {/Users/emilchri/Zotero/storage/X5NE24X4/Andresen - 2002 - John McCarthy father of AI.pdf;/Users/emilchri/Zotero/storage/HLYSB3K7/1039837.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2023-01-17},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,rnn,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/7CY63FRD/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/Users/emilchri/Zotero/storage/RZDSQBE8/1409.html}
}

@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2020-06-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  urldate = {2022-10-25},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  langid = {english},
  keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  file = {/Users/emilchri/Zotero/storage/96GS5YMP/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf;/Users/emilchri/Zotero/storage/FU7UYCHV/S1566253519308103.html}
}

@article{ben-younesBLOCKBilinearSuperdiagonal2019,
  title = {{{BLOCK}}: {{Bilinear Superdiagonal Fusion}} for {{Visual Question Answering}} and {{Visual Relationship Detection}}},
  shorttitle = {{{BLOCK}}},
  author = {Ben-younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {8102--8109},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33018102},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4818},
  urldate = {2023-02-21},
  abstract = {Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.},
  issue = {01},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/I2ZSAFME/Ben-younes et al. - 2019 - BLOCK Bilinear Superdiagonal Fusion for Visual Qu.pdf}
}

@inproceedings{ben-younesMUTANMultimodalTucker2017,
  title = {{{MUTAN}}: {{Multimodal Tucker Fusion}} for {{Visual Question Answering}}},
  shorttitle = {{{MUTAN}}},
  author = {Ben-younes, Hedi and Cadene, Rémi and Cord, Matthieu and Thome, Nicolas},
  date = {2017-05-18},
  eprint = {1705.06676},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.06676},
  url = {http://arxiv.org/abs/1705.06676},
  urldate = {2023-02-21},
  abstract = {Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/FHAZGIG9/Ben-younes et al. - 2017 - MUTAN Multimodal Tucker Fusion for Visual Questio.pdf;/Users/emilchri/Zotero/storage/P28EPNS6/Ben-younes et al. - 2017 - MUTAN Multimodal Tucker Fusion for Visual Questio.pdf;/Users/emilchri/Zotero/storage/DM7CVQUP/1705.html}
}

@article{bianchiniComplexityNeuralNetwork2014,
  title = {On the {{Complexity}} of {{Neural Network Classifiers}}: {{A Comparison Between Shallow}} and {{Deep Architectures}}},
  shorttitle = {On the {{Complexity}} of {{Neural Network Classifiers}}},
  author = {Bianchini, Monica and Scarselli, Franco},
  date = {2014-08},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {8},
  pages = {1553--1565},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2013.2293637},
  abstract = {Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden layers. In fact, experimental results and heuristic considerations suggest that deep architectures are more suitable than shallow ones for modern applications, facing very complex problems, e.g., vision and human language understanding. However, the actual theoretical results supporting such a claim are still few and incomplete. In this paper, we propose a new approach to study how the depth of feedforward neural networks impacts on their ability in implementing high complexity functions. First, a new measure based on topological concepts is introduced, aimed at evaluating the complexity of the function implemented by a neural network, used for classification purposes. Then, deep and shallow neural architectures with common sigmoidal activation functions are compared, by deriving upper and lower bounds on their complexity, and studying how the complexity depends on the number of hidden units and the used activation function. The obtained results seem to support the idea that deep networks actually implements functions of higher complexity, so that they are able, with the same number of resources, to address more difficult problems.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Betti numbers,Biological neural networks,Complexity theory,Computer architecture,deep neural networks,function approximation,Neurons,Polynomials,topological complexity,Upper bound,Vapnik–Chervonenkis dimension (VC-dim),Vapnik–Chervonenkis dimension (VC-dim).},
  file = {/Users/emilchri/Zotero/storage/ALQG8WFV/Bianchini og Scarselli - 2014 - On the Complexity of Neural Network Classifiers A.pdf;/Users/emilchri/Zotero/storage/INKV5AAJ/6697897.html}
}

@article{bolognaCharacterizationSymbolicRules2017,
  title = {Characterization of Symbolic Rules Embedded in Deep {{DIMLP}} Networks : A Challenge to Transparency of Deep Learning},
  shorttitle = {Characterization of Symbolic Rules Embedded in Deep {{DIMLP}} Networks},
  author = {Bologna, G. and Hayashi, Y.},
  date = {2017},
  journaltitle = {Journal of Artificial Intelligence and Soft Computing Research},
  volume = {Vol. 7, No. 4},
  issn = {2083-2567},
  doi = {10.1515/jaiscr-2017-0019},
  url = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-14c8b34f-846b-4705-a3ab-8f41c7aa2e81},
  urldate = {2022-10-28},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/5WS7I8CH/Bologna og Hayashi - 2017 - Characterization of symbolic rules embedded in dee.pdf;/Users/emilchri/Zotero/storage/RUUA8VH5/bwmeta1.element.html}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  date = {2002-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2022-11-08},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  langid = {english},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/Users/emilchri/Zotero/storage/HCKFEUE2/Campbell et al. - 2002 - Deep Blue.pdf;/Users/emilchri/Zotero/storage/ZTUW54NT/S0004370201001291.html}
}

@inproceedings{caruanaIntelligibleModelsHealthCare2015,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  date = {2015-08-10},
  series = {{{KDD}} '15},
  pages = {1721--1730},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2783258.2788613},
  url = {https://doi.org/10.1145/2783258.2788613},
  urldate = {2022-10-25},
  abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  isbn = {978-1-4503-3664-2},
  keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction},
  file = {/Users/emilchri/Zotero/storage/QAC2UZVW/Caruana et al. - 2015 - Intelligible Models for HealthCare Predicting Pne.pdf}
}

@article{castelvecchiCanWeOpen2016,
  title = {Can We Open the Black Box of {{AI}}?},
  author = {Castelvecchi, Davide},
  date = {2016-10-06},
  journaltitle = {Nature News},
  volume = {538},
  number = {7623},
  pages = {20},
  doi = {10.1038/538020a},
  url = {http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
  urldate = {2022-10-24},
  abstract = {Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn.},
  langid = {english},
  annotation = {Cg\_type: Nature News},
  file = {/Users/emilchri/Zotero/storage/RJ68N9QP/Castelvecchi - 2016 - Can we open the black box of AI.pdf;/Users/emilchri/Zotero/storage/UP6SQMRP/can-we-open-the-black-box-of-ai-1.html}
}

@article{chengRumorGeneticMutation2021,
  title = {From Rumor to Genetic Mutation Detection with Explanations: A {{GAN}} Approach},
  shorttitle = {From Rumor to Genetic Mutation Detection with Explanations},
  author = {Cheng, Mingxi and Li, Yizhi and Nazarian, Shahin and Bogdan, Paul},
  date = {2021-03-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {5861},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-84993-1},
  url = {https://www.nature.com/articles/s41598-021-84993-1},
  urldate = {2022-09-12},
  abstract = {Social media have emerged as increasingly popular means and environments for information gathering and propagation. This vigorous growth of social media contributed not only to a pandemic (fast-spreading and far-reaching) of rumors and misinformation, but also to an urgent need for text-based rumor detection strategies. To speed up the detection of misinformation, traditional rumor detection methods based on hand-crafted feature selection need to be replaced by automatic artificial intelligence (AI) approaches. AI decision making systems require to provide explanations in order to assure users of their trustworthiness. Inspired by the thriving development of generative adversarial networks (GANs) on text applications, we propose a GAN-based layered model for rumor detection with explanations. To demonstrate the universality of the proposed approach, we demonstrate its benefits on a gene classification with mutation detection case study. Similarly to the rumor detection, the gene classification can also be formulated as a text-based classification problem. Unlike fake news detection that needs a previously collected verified news database, our model provides explanations in rumor detection based on tweet-level texts only without referring to a verified news database. The layered structure of both generative and discriminative models contributes to the outstanding performance. The layered generators produce rumors by intelligently inserting controversial information in non-rumors, and force the layered discriminators to detect detailed glitches and deduce exactly which parts in the sentence are problematic. On average, in the rumor detection task, our proposed model outperforms state-of-the-art baselines on PHEME dataset by \$\$26.85\textbackslash\%\$\$in terms of macro-f1. The excellent performance of our model for textural sequences is also demonstrated by the gene mutation case study on which it achieves \$\$72.69\textbackslash\%\$\$macro-f1 score.},
  issue = {1},
  langid = {english},
  keywords = {Andrea,Computer science,Mathematics and computing},
  file = {/Users/emilchri/Zotero/storage/JMNH3NFN/Cheng et al. - 2021 - From rumor to genetic mutation detection with expl.pdf;/Users/emilchri/Zotero/storage/8HUVB8GQ/s41598-021-84993-1.html}
}

@misc{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  options = {useprefix=true},
  date = {2014-09-02},
  number = {arXiv:1406.1078},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.1078},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2023-01-17},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/JIRNXW36/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/Users/emilchri/Zotero/storage/K8SXGTIF/1406.html}
}

@article{chunDeepLearningbasedImage2022,
  title = {A Deep Learning-Based Image Captioning Method to Automatically Generate Comprehensive Explanations of Bridge Damage},
  author = {Chun, Pang-Jo and Yamane, Tatsuro and Maemura, Yu},
  date = {2022},
  journaltitle = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {37},
  number = {11},
  pages = {1387--1401},
  issn = {1467-8667},
  doi = {10.1111/mice.12793},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12793},
  urldate = {2022-09-12},
  abstract = {Photographs of bridges can reveal considerable technical information such as the part of the structure that is damaged and the type of damage. Maintenance and inspection engineers can benefit greatly from a technology that can automatically extract and express such information in readable sentences. This is possibly the first study on developing a deep learning model that can generate sentences describing the damage condition of a bridge from images through an image captioning method. Our study shows that by introducing an attention mechanism into the deep learning model, highly accurate descriptive sentences can be generated. In addition, often multiple forms of damage can be observed in the images of bridges; hence, our algorithm is adapted to output multiple sentences to provide a comprehensive interpretation of complex images. In our dataset, the scores of Bilingual Evaluation Understudy (BLEU)-1 to BLEU-4 were 0.782, 0.749, 0.711, and 0.693, respectively, and the percentage of correctly output explanatory sentences is 69.3\%. All of these results are better than the model without the attention mechanism. The developed method makes it possible to provide user-friendly, text-based explanations of bridge damage in images, making it easier for engineers with relatively little experience and even administrative staff without extensive technical expertise to understand images of bridge damage. Future research in this field is expected to lead to the unification of field expertise with artificial intelligence (AI), which will be the foundation of the evolutionary development of bridge inspection AI.},
  langid = {english},
  keywords = {Andrea,Captioning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mice.12793},
  file = {/Users/emilchri/Zotero/storage/R252XL5Q/Chun et al. - 2022 - A deep learning-based image captioning method to a.pdf;/Users/emilchri/Zotero/storage/9BUM7VXG/mice.html}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  date = {1997-02-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0933365796003673},
  urldate = {2022-10-25},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  langid = {english},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia},
  file = {/Users/emilchri/Zotero/storage/KASBKT55/Cooper et al. - 1997 - An evaluation of machine-learning methods for pred.pdf;/Users/emilchri/Zotero/storage/X9ISILDH/S0933365796003673.html}
}

@article{cooperPredictingDireOutcomes2005,
  title = {Predicting Dire Outcomes of Patients with Community Acquired Pneumonia},
  author = {Cooper, Gregory F. and Abraham, Vijoy and Aliferis, Constantin F. and Aronis, John M. and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Janosky, Janine E. and Livingston, Gary and Mitchell, Tom and Monti, Stefano and Spirtes, Peter},
  date = {2005-10-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  series = {Clinical {{Machine Learning}}},
  volume = {38},
  number = {5},
  pages = {347--366},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2005.02.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046405000225},
  urldate = {2022-10-25},
  abstract = {Community-acquired pneumonia (CAP) is an important clinical condition with regard to patient mortality, patient morbidity, and healthcare resource utilization. The assessment of the likely clinical course of a CAP patient can significantly influence decision making about whether to treat the patient as an inpatient or as an outpatient. That decision can in turn influence resource utilization, as well as patient well being. Predicting dire outcomes, such as mortality or severe clinical complications, is a particularly important component in assessing the clinical course of patients. We used a training set of 1601 CAP patient cases to construct 11 statistical and machine-learning models that predict dire outcomes. We evaluated the resulting models on 686 additional CAP-patient cases. The primary goal was not to compare these learning algorithms as a study end point; rather, it was to develop the best model possible to predict dire outcomes. A special version of an artificial neural network (NN) model predicted dire outcomes the best. Using the 686 test cases, we estimated the expected healthcare quality and cost impact of applying the NN model in practice. The particular, quantitative results of this analysis are based on a number of assumptions that we make explicit; they will require further study and validation. Nonetheless, the general implication of the analysis seems robust, namely, that even small improvements in predictive performance for prevalent and costly diseases, such as CAP, are likely to result in significant improvements in the quality and efficiency of healthcare delivery. Therefore, seeking models with the highest possible level of predictive performance is important. Consequently, seeking ever better machine-learning and statistical modeling methods is of great practical significance.},
  langid = {english},
  keywords = {Community acquired pneumonia,Machine learning,Outcome prediction,Quality and cost of healthcare delivery},
  file = {/Users/emilchri/Zotero/storage/Z7J5ST3J/Cooper et al. - 2005 - Predicting dire outcomes of patients with communit.pdf;/Users/emilchri/Zotero/storage/CVPYI8RI/S1532046405000225.html}
}

@online{cooperSoftwareFrameworkRequirements2017,
  title = {Software {{Framework Requirements For Embedded Vision}}},
  author = {Cooper, Gordon},
  date = {2017-11-09T08:04:14+00:00},
  url = {https://semiengineering.com/software-framework-requirements-for-embedded-vision/},
  urldate = {2023-03-10},
  abstract = {Key factors to consider when choosing an embedded vision system.},
  langid = {american},
  organization = {{Semiconductor Engineering}},
  keywords = {graph,imagenet},
  file = {/Users/emilchri/Zotero/storage/LEQNUCUB/software-framework-requirements-for-embedded-vision.html}
}

@article{corniaExplainingTransformerbasedImage2022,
  title = {Explaining Transformer-Based Image Captioning Models: {{An}} Empirical Analysis},
  shorttitle = {Explaining Transformer-Based Image Captioning Models},
  author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  date = {2022-01-01},
  journaltitle = {AI Communications},
  volume = {35},
  number = {2},
  pages = {111--129},
  publisher = {{IOS Press}},
  issn = {0921-7126},
  doi = {10.3233/AIC-210172},
  url = {https://content.iospress.com/articles/ai-communications/aic210172},
  urldate = {2022-11-02},
  abstract = {Image Captioning is the task of translating an input image into a textual description. As such, it connects Vision and Language in a generative fashion, with applications that range from multi-modal search engines to help visually impaired people. Al},
  langid = {english},
  keywords = {transformer},
  file = {/Users/emilchri/Zotero/storage/HKMZBIEG/Cornia et al. - 2022 - Explaining transformer-based image captioning mode.pdf;/Users/emilchri/Zotero/storage/BJRNRVSJ/aic210172.html}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  date = {1967-01},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  issn = {1557-9654},
  doi = {10.1109/TIT.1967.1053964},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR\^\textbackslash ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR\^\textbackslash ast \textbackslash leq R \textbackslash leq R\^\textbackslash ast(2 –MR\^\textbackslash ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {knn,original},
  file = {/Users/emilchri/Zotero/storage/BQVJKD87/Cover og Hart - 1967 - Nearest neighbor pattern classification.pdf;/Users/emilchri/Zotero/storage/RY6RMCV4/Cover og Hart - 1967 - Nearest neighbor pattern classification.pdf;/Users/emilchri/Zotero/storage/IDEVUBG7/1053964.html}
}

@misc{dasOpportunitiesChallengesExplainable2020,
  title = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}}): {{A Survey}}},
  shorttitle = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Das, Arun and Rad, Paul},
  date = {2020-06-22},
  number = {arXiv:2006.11371},
  eprint = {2006.11371},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.11371},
  urldate = {2022-11-25},
  abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/ITQBVGJG/Das og Rad - 2020 - Opportunities and Challenges in Explainable Artifi.pdf;/Users/emilchri/Zotero/storage/5ELL3ACG/2006.html}
}

@article{dengImageNetLargeScaleHierarchical2009,
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009},
  shortjournal = {CVPR09},
  pages = {8},
  doi = {10.1109/CVPR.2009.5206848},
  url = {http://www.image-net.org/papers/imagenet_cvpr09.bib},
  urldate = {2022-10-03},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/5SYC4I8E/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf}
}

@article{dzindoletRoleTrustAutomation2003,
  title = {The Role of Trust in Automation Reliance},
  author = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
  date = {2003-06-01},
  journaltitle = {International Journal of Human-Computer Studies},
  shortjournal = {International Journal of Human-Computer Studies},
  series = {Trust and {{Technology}}},
  volume = {58},
  number = {6},
  pages = {697--718},
  issn = {1071-5819},
  doi = {10.1016/S1071-5819(03)00038-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
  urldate = {2022-10-26},
  abstract = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
  langid = {english},
  keywords = {Automation reliance,Automation trust,Disuse,Misuse},
  file = {/Users/emilchri/Zotero/storage/JXKR5HJF/Dzindolet et al. - 2003 - The role of trust in automation reliance.pdf;/Users/emilchri/Zotero/storage/27YLM5KL/S1071581903000387.html}
}

@inproceedings{fagerengPolypConnectImageInpainting2022,
  title = {{{PolypConnect}}: {{Image}} Inpainting for Generating Realistic Gastrointestinal Tract Images with Polyps},
  shorttitle = {{{PolypConnect}}},
  booktitle = {2022 {{IEEE}} 35th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Fagereng, Jan Andre and Thambawita, Vajira and Storås, Andrea M. and Parasa, Sravanthi and de Lange, Thomas and Halvorsen, Pål and Riegler, Michael A.},
  options = {useprefix=true},
  date = {2022-07},
  pages = {66--71},
  issn = {2372-9198},
  doi = {10.1109/CBMS55023.2022.00019},
  abstract = {Early identification of a polyp in the lower gas-trointestinal (GI) tract can lead to prevention of life-threatening colorectal cancer. Developing computer-aided diagnosis (CAD) systems to detect polyps can improve detection accuracy and efficiency and save the time of the domain experts called endoscopists. Lack of annotated data is a common challenge when building CAD systems. Generating synthetic medical data is an active research area to overcome the problem of having relatively few true positive cases in the medical domain. To be able to efficiently train machine learning (ML) models, which are the core of CAD systems, a considerable amount of data should be used. In this respect, we propose the PolypConnect pipeline, which can convert non-polyp images into polyp images to increase the size of training datasets for training. We present the whole pipeline with quantitative and qualitative evaluations involving endoscopists. The polyp segmentation model trained using synthetic data, and real data shows a 5.1\% improvement of mean intersection over union (mIOU), compared to the model trained only using real data. The codes of all the experiments are available on GitHub to reproduce the results.},
  eventtitle = {2022 {{IEEE}} 35th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  keywords = {Andrea,Computational modeling,Data models,fake polyp data,generative models,Image segmentation,Machine learning,medical,Pipelines,polyp,polyp inpainting,Solid modeling,synthetic medical data,synthetic polyps,Training},
  file = {/Users/emilchri/Zotero/storage/SD99F3LP/Fagereng et al. - 2022 - PolypConnect Image inpainting for generating real.pdf;/Users/emilchri/Zotero/storage/VYVHZ5RH/9866977.html}
}

@article{fixDiscriminatoryAnalysisNonparametric1989,
  title = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}: {{Consistency Properties}}},
  shorttitle = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}},
  author = {Fix, Evelyn and Hodges, J. L.},
  date = {1989},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  volume = {57},
  number = {3},
  eprint = {1403797},
  eprinttype = {jstor},
  pages = {238--247},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403797},
  keywords = {knn,orginal},
  file = {/Users/emilchri/Zotero/storage/KFESUNQ8/Fix og Hodges - 1989 - Discriminatory Analysis. Nonparametric Discriminat.pdf;/Users/emilchri/Zotero/storage/MV84YK7X/original_knn_paper.pdf}
}

@misc{fukuiMultimodalCompactBilinear2016,
  title = {Multimodal {{Compact Bilinear Pooling}} for {{Visual Question Answering}} and {{Visual Grounding}}},
  author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  date = {2016-09-23},
  number = {arXiv:1606.01847},
  eprint = {1606.01847},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.01847},
  url = {http://arxiv.org/abs/1606.01847},
  urldate = {2022-12-07},
  abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/7E2LV5KJ/Fukui et al. - 2016 - Multimodal Compact Bilinear Pooling for Visual Que.pdf;/Users/emilchri/Zotero/storage/VMKSFR2Y/1606.html}
}

@article{fukushimaCognitronSelforganizingMultilayered1975,
  title = {Cognitron: {{A}} Self-Organizing Multilayered Neural Network},
  shorttitle = {Cognitron},
  author = {Fukushima, Kunihiko},
  date = {1975-09-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {20},
  number = {3},
  pages = {121--136},
  issn = {1432-0770},
  doi = {10.1007/BF00342633},
  url = {https://doi.org/10.1007/BF00342633},
  urldate = {2022-10-04},
  abstract = {A new hypothesis for the organization of synapses between neurons is proposed: “The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y”. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named “cognitron”, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a “teacher” which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
  langid = {english},
  keywords = {Deep Layer,Final Layer,Individual Cell,Neural Network,Receptive Field,ReLu},
  file = {/Users/emilchri/Zotero/storage/3HR5Y5HL/Fukushima - 1975 - Cognitron A self-organizing multilayered neural n.pdf}
}

@misc{gaoCompactBilinearPooling2016,
  title = {Compact {{Bilinear Pooling}}},
  author = {Gao, Yang and Beijbom, Oscar and Zhang, Ning and Darrell, Trevor},
  date = {2016-04-11},
  number = {arXiv:1511.06062},
  eprint = {1511.06062},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06062},
  url = {http://arxiv.org/abs/1511.06062},
  urldate = {2022-11-28},
  abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/RUXE7X6E/Gao et al. - 2016 - Compact Bilinear Pooling.pdf;/Users/emilchri/Zotero/storage/NXSGRUNS/1511.html}
}

@inproceedings{ghorbaniAutomaticConceptbasedExplanations2019,
  title = {Towards {{Automatic Concept-based Explanations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
  urldate = {2022-10-05},
  abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.      Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \textbackslash emph\{concept\} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \textbackslash alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
  file = {/Users/emilchri/Zotero/storage/U8DUKKE3/Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014},
  pages = {580--587},
  url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {R-CNN},
  file = {/Users/emilchri/Zotero/storage/CHIZRS2J/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf;/Users/emilchri/Zotero/storage/H8DCR597/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html}
}

@inproceedings{gjestangSelflearningTeacherstudentFramework2021,
  title = {A Self-Learning Teacher-Student Framework for Gastrointestinal Image Classification},
  booktitle = {2021 {{IEEE}} 34th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Gjestang, Henrik L. and Hicks, Steven A. and Thambawita, Vajira and Halvorsen, Pål and Riegler, Michael A.},
  date = {2021-06},
  pages = {539--544},
  issn = {2372-9198},
  doi = {10.1109/CBMS52027.2021.00087},
  abstract = {We present a semi-supervised teacher-student framework to improve classification performance on gastrointestinal image data. As labeled data is scarce in medical settings, this framework is built specifically to take advantage of vast amounts of unlabeled data. It consists of three main steps: (1) train a teacher model with labeled data, (2) use the teacher model to infer pseudo labels with unlabeled data, and (3) train a new and larger student model with a combination of labeled images and inferred pseudo labels. These three steps are repeated several times by treating the student as a teacher to relabel the unlabeled data and consequently train a new student. We demonstrate that our framework can classify both video capsule endoscopy (VCE) and standard endoscopy images. Our results indicate that our teacher-student framework can significantly increase the performance compared to traditional supervised-learning-based models, i.e., an overall increase in the F1-score of 4.7\% for the Kvasir-Capsule VCE dataset and 3.2\% for the HyperKvasir colonoscopy dataset. We believe that our framework can use more of the data collected at hospitals without the need for expert labels, contributing to overall better models for medical multimedia systems for automatic disease detection.},
  eventtitle = {2021 {{IEEE}} 34th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  keywords = {capsule endoscopy,colonoscopy,Colonoscopy,computer vision,Data models,dataset,deep learning,Endoscopes,Gastrointestinal tract,generation,Hospitals,machine learning,Medical diagnostic imaging,Michael,Multimedia systems,self-training,Teacher-student framework},
  file = {/Users/emilchri/Zotero/storage/7XR4IAAP/Gjestang et al. - 2021 - A self-learning teacher-student framework for gast.pdf;/Users/emilchri/Zotero/storage/8RYMCHJX/9474707.html}
}

@article{gliksonHumanTrustArtificial2020,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  author = {Glikson, Ella and Woolley, Anita Williams},
  date = {2020},
  journaltitle = {Academy of Management Annals},
  volume = {14},
  number = {2},
  pages = {627--660},
  doi = {10.5465/annals.2018.0057},
  url = {https://journals.aom.org/doi/epdf/10.5465/annals.2018.0057},
  urldate = {2022-10-26},
  abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/P42NE95U/Human Trust in Artificial Intelligence Review of .pdf;/Users/emilchri/Zotero/storage/DEDCSYMY/annals.2018.html}
}

@misc{goodmanEuropeanUnionRegulations2016,
  title = {European {{Union}} Regulations on Algorithmic Decision-Making and a "Right to Explanation"},
  author = {Goodman, Bryce and Flaxman, Seth},
  date = {2016-08-31},
  eprint = {1606.08813},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1609/aimag.v38i3.2741},
  url = {http://arxiv.org/abs/1606.08813},
  urldate = {2022-10-24},
  abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also effectively create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,eu,Statistics - Machine Learning,xai},
  file = {/Users/emilchri/Zotero/storage/2XDGW9PR/Goodman og Flaxman - 2016 - European Union regulations on algorithmic decision.pdf;/Users/emilchri/Zotero/storage/FH9JB9KG/1606.html}
}

@misc{goyalMakingVQAMatter2017,
  title = {Making the {{V}} in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  shorttitle = {Making the {{V}} in {{VQA Matter}}},
  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  date = {2017-05-15},
  number = {arXiv:1612.00837},
  eprint = {1612.00837},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1612.00837},
  urldate = {2022-10-03},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/S556XBUF/Goyal et al. - 2017 - Making the V in VQA Matter Elevating the Role of .pdf;/Users/emilchri/Zotero/storage/QDYSAJNY/1612.html}
}

@article{guptaDeepLearningObject2021,
  title = {Deep Learning for Object Detection and Scene Perception in Self-Driving Cars: {{Survey}}, Challenges, and Open Issues},
  shorttitle = {Deep Learning for Object Detection and Scene Perception in Self-Driving Cars},
  author = {Gupta, Abhishek and Anpalagan, Alagan and Guan, Ling and Khwaja, Ahmed Shaharyar},
  date = {2021-07-01},
  journaltitle = {Array},
  shortjournal = {Array},
  volume = {10},
  pages = {100057},
  issn = {2590-0056},
  doi = {10.1016/j.array.2021.100057},
  url = {https://www.sciencedirect.com/science/article/pii/S2590005621000059},
  urldate = {2022-10-31},
  abstract = {This article presents a comprehensive survey of deep learning applications for object detection and scene perception in autonomous vehicles. Unlike existing review papers, we examine the theory underlying self-driving vehicles from deep learning perspective and current implementations, followed by their critical evaluations. Deep learning is one potential solution for object detection and scene perception problems, which can enable algorithm-driven and data-driven cars. In this article, we aim to bridge the gap between deep learning and self-driving cars through a comprehensive survey. We begin with an introduction to self-driving cars, deep learning, and computer vision followed by an overview of artificial general intelligence. Then, we classify existing powerful deep learning libraries and their role and significance in the growth of deep learning. Finally, we discuss several techniques that address the image perception issues in real-time driving, and critically evaluate recent implementations and tests conducted on self-driving cars. The findings and practices at various stages are summarized to correlate prevalent and futuristic techniques, and the applicability, scalability and feasibility of deep learning to self-driving cars for achieving safe driving without human intervention. Based on the current survey, several recommendations for further research are discussed at the end of this article.},
  langid = {english},
  keywords = {Autonomous driving initiatives,Computer vision,Convolutional neural networks,Deep learning,Levels of automation,LiDAR,Machine learning,Multimodal sensor fusion,Object detection,Scene perception,Self-driving cars},
  file = {/Users/emilchri/Zotero/storage/EGD7E4RF/Gupta et al. - 2021 - Deep learning for object detection and scene perce.pdf;/Users/emilchri/Zotero/storage/9ZL46JE2/S2590005621000059.html}
}

@article{haspielExplanationsExpectations2018,
  title = {Explanations and {{Expectations}}},
  author = {Haspiel, Jacob and Du, Na and Meyerson, Jill and Robert Jr., Lionel P. and Tilbury, Dawn and Yang, X. Jessie and Pradhan, Anuj K.},
  date = {2018},
  pages = {119--120},
  doi = {10.1145/3173386.3177057},
  keywords = {background,motivation,xai},
  file = {/Users/emilchri/Zotero/storage/V5GTGFM5/Peeking_Inside_the_Black-Box_A_Survey_on_Explainable_Artificial_Intelligence_XAI.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2023-02-06},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,resnet},
  file = {/Users/emilchri/Zotero/storage/YN6YYXT9/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/emilchri/Zotero/storage/GCTP54SY/1512.html}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  pages = {1026--1034},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html},
  urldate = {2023-03-13},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {deep learning,human,imagenet,level,surpassed},
  file = {/Users/emilchri/Zotero/storage/EC8AB3NW/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf}
}

@incollection{heImageCaptioningImage2021,
  title = {Image {{Captioning Through Image Transformer}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2020},
  author = {He, Sen and Liao, Wentong and Tavakoli, Hamed R. and Yang, Michael and Rosenhahn, Bodo and Pugeault, Nicolas},
  editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12625},
  pages = {153--169},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-69538-5_10},
  url = {http://link.springer.com/10.1007/978-3-030-69538-5_10},
  urldate = {2022-11-06},
  abstract = {Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect of captioning is the notion of attention: how to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous works have proposed the transformer architecture for image captioning. However, the structure between the semantic units in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt the transformer’s internal architecture to images. In this work, we introduce the image transformer , which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widens the original transformer layer’s inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks. The code is available at https://github.com/wtliao/ImageTransformer.},
  isbn = {978-3-030-69537-8 978-3-030-69538-5},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/8D5A7Q8N/He et al. - 2021 - Image Captioning Through Image Transformer.pdf}
}

@inproceedings{heRethinkingImageNetPreTraining2019,
  title = {Rethinking {{ImageNet Pre-Training}}},
  author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
  date = {2019},
  pages = {4918--4927},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html},
  urldate = {2023-02-27},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  file = {/Users/emilchri/Zotero/storage/YKAAZLND/He et al. - 2019 - Rethinking ImageNet Pre-Training.pdf}
}

@article{herlockerExplainingCollaborativeFiltering2000,
  title = {Explaining Collaborative Filtering Recommendations | {{Proceedings}} of the 2000 {{ACM}} Conference on {{Computer}} Supported Cooperative Work},
  author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Riedl, John},
  date = {2000-12-02},
  journaltitle = {CSCW '00: Proceedings of the 2000 ACM conference on Computer supported cooperative work},
  pages = {241--250},
  doi = {10.1145/358916.358995},
  url = {https://dl.acm.org/doi/abs/10.1145/358916.358995},
  urldate = {2022-10-26},
  file = {/Users/emilchri/Zotero/storage/8VZVYHIN/Explaining collaborative filtering recommendations.pdf;/Users/emilchri/Zotero/storage/Z4NMN3XK/358916.html}
}

@misc{hintonForwardForwardAlgorithmPreliminary2022,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  shorttitle = {The {{Forward-Forward Algorithm}}},
  author = {Hinton, Geoffrey},
  date = {2022-12-26},
  number = {arXiv:2212.13345},
  eprint = {2212.13345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13345},
  url = {http://arxiv.org/abs/2212.13345},
  urldate = {2023-03-13},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/F584UTN8/Hinton - 2022 - The Forward-Forward Algorithm Some Preliminary In.pdf;/Users/emilchri/Zotero/storage/KDBNVAU6/2212.html}
}

@inproceedings{hirotaGenderRacialBias2022,
  title = {Gender and {{Racial Bias}} in {{Visual Question Answering Datasets}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Hirota, Yusuke and Nakashima, Yuta and Garcia, Noa},
  date = {2022-06-21},
  eprint = {2205.08148},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1280--1292},
  doi = {10.1145/3531146.3533184},
  url = {http://arxiv.org/abs/2205.08148},
  urldate = {2023-01-17},
  abstract = {Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society},
  file = {/Users/emilchri/Zotero/storage/8U6CZWM6/Hirota et al. - 2022 - Gender and Racial Bias in Visual Question Answerin.pdf;/Users/emilchri/Zotero/storage/QCC6S3HN/2205.html}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long {{Short-term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-12-01},
  journaltitle = {Neural computation},
  shortjournal = {Neural computation},
  volume = {9},
  pages = {1735--80},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  keywords = {lstm,LSTM},
  file = {/Users/emilchri/Zotero/storage/3XYY2QCB/Hochreiter og Schmidhuber - 1997 - Long Short-term Memory.pdf;/Users/emilchri/Zotero/storage/XSLJ8PY4/authors.html}
}

@article{holzingerCausabilityExplainabilityArtificial2019,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
  date = {2019},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4795},
  doi = {10.1002/widm.1312},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312},
  urldate = {2022-10-31},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1312},
  file = {/Users/emilchri/Zotero/storage/IRCSK4SH/Holzinger et al. - 2019 - Causability and explainability of artificial intel.pdf;/Users/emilchri/Zotero/storage/YPI8BVNY/widm.html}
}

@misc{holzingerWhatWeNeed2017,
  title = {What Do We Need to Build Explainable {{AI}} Systems for the Medical Domain?},
  author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
  date = {2017-12-28},
  number = {arXiv:1712.09923},
  eprint = {1712.09923},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.09923},
  url = {http://arxiv.org/abs/1712.09923},
  urldate = {2022-10-28},
  abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/MJ2VZDD4/Holzinger et al. - 2017 - What do we need to build explainable AI systems fo.pdf;/Users/emilchri/Zotero/storage/KJDK9SCU/1712.html}
}

@misc{hudsonCompositionalAttentionNetworks2018,
  title = {Compositional {{Attention Networks}} for {{Machine Reasoning}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  date = {2018-04-24},
  number = {arXiv:1803.03067},
  eprint = {1803.03067},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.03067},
  url = {http://arxiv.org/abs/1803.03067},
  urldate = {2022-12-07},
  abstract = {We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9\% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/emilchri/Zotero/storage/7INVS2XJ/Hudson og Manning - 2018 - Compositional Attention Networks for Machine Reaso.pdf;/Users/emilchri/Zotero/storage/XU9QL6D8/1803.html}
}

@misc{hudsonGQANewDataset2019,
  title = {{{GQA}}: {{A New Dataset}} for {{Real-World Visual Reasoning}} and {{Compositional Question Answering}}},
  shorttitle = {{{GQA}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  date = {2019-05-10},
  number = {arXiv:1902.09506},
  eprint = {1902.09506},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.09506},
  url = {http://arxiv.org/abs/1902.09506},
  urldate = {2022-12-07},
  abstract = {We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1\%, and strong VQA models achieve 54.1\%, human performance tops at 89.3\%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/EQQCUXMU/Hudson og Manning - 2019 - GQA A New Dataset for Real-World Visual Reasoning.pdf;/Users/emilchri/Zotero/storage/XSXLMAUR/1902.html}
}

@misc{hughesMedicalTextClassification2017,
  title = {Medical {{Text Classification}} Using {{Convolutional Neural Networks}}},
  author = {Hughes, Mark and Li, Irene and Kotoulas, Spyros and Suzumura, Toyotaro},
  date = {2017-04-22},
  number = {arXiv:1704.06841},
  eprint = {1704.06841},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.06841},
  url = {http://arxiv.org/abs/1704.06841},
  urldate = {2022-10-24},
  abstract = {We present an approach to automatically classify clinical text at a sentence level. We are using deep convolutional neural networks to represent complex features. We train the network on a dataset providing a broad categorization of health information. Through a detailed evaluation, we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/emilchri/Zotero/storage/ZNPMTIEB/Hughes et al. - 2017 - Medical Text Classification using Convolutional Ne.pdf;/Users/emilchri/Zotero/storage/85JIFSTW/1704.html}
}

@online{HumanTrustArtificial,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  doi = {10.5465/annals.2018.0057},
  url = {https://journals.aom.org/doi/epdf/10.5465/annals.2018.0057},
  urldate = {2022-11-29},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/3LRCIKXH/annals.2018.html}
}

@article{jiangArtificialIntelligenceHealthcare2017,
  title = {Artificial Intelligence in Healthcare: Past, Present and Future},
  shorttitle = {Artificial Intelligence in Healthcare},
  author = {Jiang, Fei and Jiang, Yong and Zhi, Hui and Dong, Yi and Li, Hao and Ma, Sufeng and Wang, Yilong and Dong, Qiang and Shen, Haipeng and Wang, Yongjun},
  date = {2017-12-01},
  journaltitle = {Stroke and Vascular Neurology},
  shortjournal = {Stroke Vasc Neurol},
  volume = {2},
  number = {4},
  eprint = {29507784},
  eprinttype = {pmid},
  publisher = {{BMJ Specialist Journals}},
  issn = {2059-8688, 2059-8696},
  doi = {10.1136/svn-2017-000101},
  url = {https://svn.bmj.com/content/2/4/230},
  urldate = {2022-10-31},
  abstract = {Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.},
  langid = {english},
  keywords = {big data,deep learning,neural network,stroke,support vector machine},
  file = {/Users/emilchri/Zotero/storage/9TACYDK4/Jiang et al. - 2017 - Artificial intelligence in healthcare past, prese.pdf;/Users/emilchri/Zotero/storage/5MQD69LS/230.html}
}

@inproceedings{jingAutomaticGenerationMedical2018,
  title = {On the {{Automatic Generation}} of {{Medical Imaging Reports}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jing, Baoyu and Xie, Pengtao and Xing, Eric},
  date = {2018},
  eprint = {1711.08195},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {2577--2586},
  doi = {10.18653/v1/P18-1240},
  url = {http://arxiv.org/abs/1711.08195},
  urldate = {2022-10-05},
  abstract = {Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/42KDQ7FP/Jing et al. - 2018 - On the Automatic Generation of Medical Imaging Rep.pdf;/Users/emilchri/Zotero/storage/DKY86YSA/1711.html}
}

@inproceedings{johnsBecomingExpertInteractive2015,
  title = {Becoming the {{Expert}} - {{Interactive Multi-Class Machine Teaching}}},
  author = {Johns, Edward and Mac Aodha, Oisin and Brostow, Gabriel J.},
  date = {2015},
  pages = {2616--2624},
  url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Johns_Becoming_the_Expert_2015_CVPR_paper.html},
  urldate = {2022-10-25},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {expert,learning,Teach,teacher},
  file = {/Users/emilchri/Zotero/storage/SZWLLHFW/Johns et al. - 2015 - Becoming the Expert - Interactive Multi-Class Mach.pdf;/Users/emilchri/Zotero/storage/M38DB4XH/Johns_Becoming_the_Expert_2015_CVPR_paper.html}
}

@inproceedings{johnsonDenseCapFullyConvolutional2016,
  title = {{{DenseCap}}: {{Fully Convolutional Localization Networks}} for {{Dense Captioning}}},
  shorttitle = {{{DenseCap}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  date = {2016-06},
  pages = {4565--4574},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.494},
  url = {http://ieeexplore.ieee.org/document/7780863/},
  urldate = {2022-09-19},
  abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/7RMNHEXZ/Johnson et al. - 2016 - DenseCap Fully Convolutional Localization Network.pdf}
}

@article{joshiReviewExplainabilityMultimodal2021,
  title = {A {{Review}} on {{Explainability}} in {{Multimodal Deep Neural Nets}}},
  author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {59800--59821},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3070212},
  abstract = {Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Biomedical imaging,Data models,Deep learning,Deep multimodal learning,explainable AI,interpretability,Neural networks,survey,Task analysis,trends,vision and language research,Visualization,XAI},
  file = {/Users/emilchri/Zotero/storage/FPECNRA4/Joshi et al. - 2021 - A Review on Explainability in Multimodal Deep Neur.pdf;/Users/emilchri/Zotero/storage/2JKRCZZR/9391727.html}
}

@inproceedings{jungBetterExplanationsClass2021,
  title = {Towards {{Better Explanations}} of {{Class Activation Mapping}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Jung, Hyungsik and Oh, Youngrock},
  date = {2021-10},
  pages = {1316--1324},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00137},
  url = {https://ieeexplore.ieee.org/document/9710512/},
  urldate = {2022-09-19},
  abstract = {Increasing demands for understanding the internal behavior of convolutional neural networks (CNNs) have led to remarkable improvements in explanation methods. Particularly, several class activation mapping (CAM) based methods, which generate visual explanation maps by a linear combination of activation maps from CNNs, have been proposed. However, the majority of the methods lack a clear theoretical basis on how they assign the coefficients of the linear combination. In this paper, we revisit the intrinsic linearity of CAM with respect to the activation maps; we construct an explanation model of CNN as a linear function of binary variables that denote the existence of the corresponding activation maps. With this approach, the explanation model can be determined by additive feature attribution methods in an analytic manner. We then demonstrate the adequacy of SHAP values, which is a unique solution for the explanation model with a set of desirable properties, as the coefficients of CAM. Since the exact SHAP values are unattainable, we introduce an efficient approximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can estimate the SHAP values of the activation maps with high speed and accuracy. Furthermore, it greatly outperforms other previous CAM-based methods in both qualitative and quantitative aspects.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/88KQWWBC/Jung og Oh - 2021 - Towards Better Explanations of Class Activation Ma.pdf;/Users/emilchri/Zotero/storage/FTGRYLUU/Jung og Oh - 2021 - Towards Better Explanations of Class Activation Ma.pdf;/Users/emilchri/Zotero/storage/7LVS4YEV/Jung_Towards_Better_Explanations_of_Class_Activation_Mapping_ICCV_2021_paper.html}
}

@inproceedings{karpathyDeepVisualSemanticAlignments2015,
  title = {Deep {{Visual-Semantic Alignments}} for {{Generating Image Descriptions}}},
  author = {Karpathy, Andrej and Fei-Fei, Li},
  date = {2015-06},
  pages = {17},
  publisher = {{CVPR}},
  abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/25EKPTCR/Karpathy og Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Ima.pdf;/Users/emilchri/Zotero/storage/MLBXHUIK/Karpathy og Fei-Fei - Deep Visual-Semantic Alignments for Generating Ima.pdf;/Users/emilchri/Zotero/storage/3KYLYNIG/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html}
}

@inproceedings{kimInterpretabilityFeatureAttribution2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2018-07-03},
  pages = {2668--2677},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/kim18d.html},
  urldate = {2022-09-12},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Andrea},
  file = {/Users/emilchri/Zotero/storage/WUW96HPU/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf}
}

@misc{kimViLTVisionandLanguageTransformer2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  date = {2021-06-10},
  number = {arXiv:2102.03334},
  eprint = {2102.03334},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.03334},
  url = {http://arxiv.org/abs/2102.03334},
  urldate = {2023-02-23},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/ESFJVFV8/Kim et al. - 2021 - ViLT Vision-and-Language Transformer Without Conv.pdf;/Users/emilchri/Zotero/storage/FV5NTD3N/2102.html}
}

@article{koehlerExplanationImaginationConfidence1991,
  title = {Explanation, {{Imagination}}, and {{Confidence}} in {{Judgment}}},
  author = {Koehler, Derek},
  date = {1991-12-01},
  journaltitle = {Psychological bulletin},
  shortjournal = {Psychological bulletin},
  volume = {110},
  pages = {499--519},
  doi = {10.1037//0033-2909.110.3.499},
  abstract = {This article concerns a class of experimental manipulations that require people to generate explanations or imagine scenarios. A review of studies using such manipulations indicates that people who explain or imagine a possibility then express greater confidence in the truth of that possibility. It is argued that this effect results from the approach people take in the explanation or imagination task: They temporarily assume that the hypothesis is true and assess how plausibly it can account for the relevant evidence. From this view, any task that requires that a hypothesis be treated as if it were true is sufficient to increase confidence in the truth of that hypothesis. Such tasks cause increased confidence in the hypothesis at the expense of viable alternatives because of changes in problem representation, evidence evaluation, and information search that take place when the hypothesis is temporarily treated as if it were true.},
  file = {/Users/emilchri/Zotero/storage/S9DBEK3C/Koehler - 1991 - Explanation, Imagination, and Confidence in Judgme.pdf}
}

@inproceedings{krauseHierarchicalApproachGenerating2017,
  title = {A {{Hierarchical Approach}} for {{Generating Descriptive Image Paragraphs}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},
  date = {2017-07},
  pages = {3337--3345},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.356},
  url = {http://ieeexplore.ieee.org/document/8099839/},
  urldate = {2022-10-05},
  abstract = {Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/IWII2EIA/Krause et al. - 2017 - A Hierarchical Approach for Generating Descriptive.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  url = {https://doi.org/10.1145/3065386},
  urldate = {2022-10-03},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  keywords = {AlexNet},
  file = {/Users/emilchri/Zotero/storage/GLZIQX8Y/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{lancasterAutomatedLabelingHuman1997,
  title = {Automated Labeling of the Human Brain: {{A}} Preliminary Report on the Development and Evaluation of a Forward-Transform Method},
  shorttitle = {Automated Labeling of the Human Brain},
  author = {Lancaster, J.l. and Rainey, L.h. and Summerlin, J.l. and Freitas, C.s. and Fox, P.t. and Evans, A.c. and Toga, A.w. and Mazziotta, J.c.},
  date = {1997},
  journaltitle = {Human Brain Mapping},
  volume = {5},
  number = {4},
  pages = {238--242},
  issn = {1097-0193},
  doi = {10.1002/(SICI)1097-0193(1997)5:4<238::AID-HBM6>3.0.CO;2-4},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0193%281997%295%3A4%3C238%3A%3AAID-HBM6%3E3.0.CO%3B2-4},
  urldate = {2023-02-24},
  abstract = {A forward-transform method for retrieving brain labels from the 1988 Talairach Atlas using x-y-z coordinates is presented. A hierarchical volume-occupancy labeling scheme was created to simplify the organization of atlas labels using volume and subvolumetric components. Segmentation rules were developed to define boundaries that were not given explicitly in the atlas. The labeling scheme and segmentation rules guided the segmentation and labeling of 160 contiguous regions within the atlas. A unique three-dimensional (3-D) database label server called the Talairach Daemon (http://ric.uthscsa.edu/projects) was developed for serving labels keyed to the Talairach coordinate system. Given an x-y-z Talairach coordinate, a corresponding hierarchical listing of labels is returned by the server. The accuracy and precision of the forward-transform labeling method is now under evaluation. Hum. Brain Mapping 5:238–242, 1997. © 1997 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {brain,brain labeling,image,labeling,naming hierarchy,segmentation,spatial normalization,Talairach Atlas,Talairach Daemon},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0193\%281997\%295\%3A4\%3C238\%3A\%3AAID-HBM6\%3E3.0.CO\%3B2-4},
  file = {/Users/emilchri/Zotero/storage/4SP65W9M/Lancaster et al. - 1997 - Automated labeling of the human brain A prelimina.pdf;/Users/emilchri/Zotero/storage/5KWR9HLJ/(SICI)1097-0193(1997)54238AID-HBM63.0.html}
}

@article{lancellottiArtificialIntelligenceTissue2021,
  title = {Artificial {{Intelligence}} \& {{Tissue Biomarkers}}: {{Advantages}}, {{Risks}} and {{Perspectives}} for {{Pathology}}},
  shorttitle = {Artificial {{Intelligence}} \& {{Tissue Biomarkers}}},
  author = {Lancellotti, Cesare and Cancian, Pierandrea and Savevski, Victor and Kotha, Soumya and Fraggetta, Filippo and Graziano, Paolo and di Tommaso, Luca},
  options = {useprefix=true},
  date = {2021-04-02},
  journaltitle = {Cells},
  shortjournal = {Cells},
  volume = {10},
  pages = {787},
  doi = {10.3390/cells10040787},
  abstract = {Tissue Biomarkers are information written in the tissue and used in Pathology to recognize specific subsets of patients with diagnostic, prognostic or predictive purposes, thus representing the key elements of Personalized Medicine. The advent of Artificial Intelligence (AI) promises to further reinforce the role of Pathology in the scenario of Personalized Medicine: AI-based devices are expected to standardize the evaluation of tissue biomarkers and also to discover novel information, which would otherwise be ignored by human review, and use them to make specific predictions. In this review we will present how AI has been used to support Tissue Biomarkers evaluation in the specific field of Pathology, give an insight to the intriguing field of AI-based biomarkers and discuss possible advantages, risk and perspectives for Pathology.},
  file = {/Users/emilchri/Zotero/storage/5H73TCXM/Lancellotti et al. - 2021 - Artificial Intelligence & Tissue Biomarkers Advan.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,cnn,Feature extraction,Hidden Markov models,lenet-5,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/emilchri/Zotero/storage/B3D9ZWSS/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/emilchri/Zotero/storage/RP6J9G2H/726791.html}
}

@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  date = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
  urldate = {2022-10-04},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/emilchri/Zotero/storage/8VNVN5XN/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagat.pdf}
}

@misc{leiLessMoreClipBERT2021,
  title = {Less Is {{More}}: {{ClipBERT}} for {{Video-and-Language Learning}} via {{Sparse Sampling}}},
  shorttitle = {Less Is {{More}}},
  author = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
  date = {2021-02-11},
  number = {arXiv:2102.06183},
  eprint = {2102.06183},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.06183},
  urldate = {2023-01-03},
  abstract = {The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from vision models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video features, it is often difficult (or infeasible) to plug feature extractors directly into existing approaches for easy finetuning. To provide a remedy to this dilemma, we propose a generic framework ClipBERT that enables affordable end-to-end learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each training step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that ClipBERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learning with just a few sparsely sampled clips is often more accurate than using densely extracted offline features from full-length videos, proving the proverbial less-is-more principle. Videos in the datasets are from considerably different domains and lengths, ranging from 3-second generic domain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach. Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available at https://github.com/jayleicn/ClipBERT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/BRUSN6KM/Lei et al. - 2021 - Less is More ClipBERT for Video-and-Language Lear.pdf;/Users/emilchri/Zotero/storage/ZHH929K9/2102.html}
}

@inproceedings{linFocalLossDense2017,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  date = {2017},
  pages = {2980--2988},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {retinanet},
  file = {/Users/emilchri/Zotero/storage/C99R735F/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf;/Users/emilchri/Zotero/storage/7T9RK5T3/Lin_Focal_Loss_for_ICCV_2017_paper.html}
}

@misc{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-20},
  number = {arXiv:1405.0312},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2023-01-09},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {COCO,Computer Science - Computer Vision and Pattern Recognition,dataset},
  file = {/Users/emilchri/Zotero/storage/VTGRAD8X/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/emilchri/Zotero/storage/HC4VE6ZV/1405.html}
}

@inproceedings{liOscarObjectSemanticsAligned2020,
  title = {Oscar: {{Object-Semantics Aligned Pre-training}} for {{Vision-Language Tasks}}},
  shorttitle = {Oscar},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {121--137},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58577-8_8},
  abstract = {Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method~Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks (The code and pre-trained models are released: https://github.com/microsoft/Oscar).},
  isbn = {978-3-030-58577-8},
  langid = {english},
  keywords = {Object semantics,Pre-training,Vision-and-language},
  file = {/Users/emilchri/Zotero/storage/UAY23QXA/Li et al. - 2020 - Oscar Object-Semantics Aligned Pre-training for V.pdf}
}

@article{liptonMachineLearningConcept,
  title = {In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  author = {Lipton, Zachary C},
  journaltitle = {machine learning},
  pages = {28},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/W27ZLC4Q/Lipton - In machine learning, the concept of interpretabili.pdf}
}

@misc{liptonMythosModelInterpretability2017,
  title = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  date = {2017-03-06},
  number = {arXiv:1606.03490},
  eprint = {1606.03490},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.03490},
  url = {http://arxiv.org/abs/1606.03490},
  urldate = {2022-10-24},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/3W42HMBC/Lipton - 2017 - The Mythos of Model Interpretability.pdf;/Users/emilchri/Zotero/storage/Z838CCGA/1606.html}
}

@unpublished{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  date = {2017-11-24},
  eprint = {1705.07874},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.07874},
  urldate = {2022-02-02},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Michael,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/7LGZX9TJ/Lundberg og Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf;/Users/emilchri/Zotero/storage/HWDZ3SCT/1705.html}
}

@inproceedings{luNeuralBabyTalk2018,
  title = {Neural {{Baby Talk}}},
  author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  date = {2018},
  pages = {7219--7228},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Neural_Baby_Talk_CVPR_2018_paper.html},
  urldate = {2022-09-19},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/RG2P6J93/Lu et al. - 2018 - Neural Baby Talk.pdf;/Users/emilchri/Zotero/storage/EMVKK6XB/Lu_Neural_Baby_Talk_CVPR_2018_paper.html}
}

@inproceedings{malinowskiMultiWorldApproachQuestion2014,
  title = {A {{Multi-World Approach}} to {{Question Answering}} about {{Real-World Scenes}} Based on {{Uncertain Input}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Malinowski, Mateusz and Fritz, Mario},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html},
  urldate = {2023-02-24},
  abstract = {We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.},
  file = {/Users/emilchri/Zotero/storage/643DFN43/Malinowski og Fritz - 2014 - A Multi-World Approach to Question Answering about.pdf}
}

@article{manmadhanVisualQuestionAnswering2020,
  title = {Visual Question Answering: A State-of-the-Art Review},
  shorttitle = {Visual Question Answering},
  author = {Manmadhan, Sruthy and Kovoor, Binsu C.},
  date = {2020-12-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {53},
  number = {8},
  pages = {5705--5745},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09832-7},
  url = {https://doi.org/10.1007/s10462-020-09832-7},
  urldate = {2023-02-06},
  abstract = {Visual question answering (VQA) is a task that has received immense consideration from two major research communities: computer vision and natural language processing. Recently it has been widely accepted as an AI-complete task which can be used as an alternative to visual turing test. In its most common form, it is a multi-modal challenging task where a computer is required to provide the correct answer for a natural language question asked about an input image. It attracts many deep learning researchers after their remarkable achievements in text, voice and vision technologies. This review extensively and critically examines the current status of VQA research in terms of step by step solution methodologies, datasets and evaluation metrics. Finally, this paper also discusses future research directions for all the above-mentioned aspects of VQA separately.},
  langid = {english},
  keywords = {review,VQA},
  file = {/Users/emilchri/Zotero/storage/SZV53MDA/Manmadhan og Kovoor - 2020 - Visual question answering a state-of-the-art revi.pdf}
}

@misc{maoDeepCaptioningMultimodal2015,
  title = {Deep {{Captioning}} with {{Multimodal Recurrent Neural Networks}} (m-{{RNN}})},
  author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
  date = {2015-06-11},
  number = {arXiv:1412.6632},
  eprint = {1412.6632},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6632},
  url = {http://arxiv.org/abs/1412.6632},
  urldate = {2023-02-24},
  abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/\textasciitilde junhua.mao/m-RNN.html .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.2.7,vqa},
  file = {/Users/emilchri/Zotero/storage/P8KDGVI7/Mao et al. - 2015 - Deep Captioning with Multimodal Recurrent Neural N.pdf;/Users/emilchri/Zotero/storage/CQ8M8NNF/1412.html}
}

@article{mccarthyProposalDartmouthSummer2006,
  title = {A {{Proposal}} for the {{Dartmouth Summer Research Project}} on {{Artificial Intelligence}}, {{August}} 31, 1955},
  author = {McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
  date = {2006-12-15},
  journaltitle = {AI Magazine},
  volume = {27},
  number = {4},
  pages = {12--12},
  issn = {2371-9621},
  doi = {10.1609/aimag.v27i4.1904},
  url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904},
  urldate = {2023-03-10},
  abstract = {The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.},
  issue = {4},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/YWZFGRQQ/McCarthy et al. - 2006 - A Proposal for the Dartmouth Summer Research Proje.pdf}
}

@video{Metropolis1927IMDb,
  title = {Metropolis (1927) - {{IMDb}}},
  url = {https://www.imdb.com/title/tt0017136/},
  urldate = {2023-03-06},
  abstract = {Metropolis: Directed by Fritz Lang. With Alfred Abel, Gustav Fröhlich, Rudolf Klein-Rogge, Fritz Rasp. In a futuristic city sharply divided between the working class and the city planners, the son of the city's mastermind falls in love with a working-class prophet who predicts the coming of a savior to mediate their differences.},
  langid = {american},
  file = {/Users/emilchri/Zotero/storage/7IE9XUE3/tt0017136.html}
}

@thesis{mnihMachineLearningAerial,
  type = {phdthesis},
  title = {Machine {{Learning}} for {{Aerial Image Labeling}}},
  author = {Mnih, Volodymyr},
  institution = {{University of Toronto (Canada)}},
  location = {{Canada -- Ontario, CA}},
  url = {https://www.proquest.com/docview/1500835065/abstract/619BEBB8BCBA4755PQ/1},
  urldate = {2023-02-24},
  abstract = {Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.},
  isbn = {9780494961841},
  langid = {english},
  pagetotal = {110},
  keywords = {Applied sciences,Machine learning,Neural networks,Remote sensing},
  file = {/Users/emilchri/Zotero/storage/7KQN49HQ/Mnih - Machine Learning for Aerial Image Labeling.pdf}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/index.html},
  urldate = {2022-11-29},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/Users/emilchri/Zotero/storage/HJDDEAXK/index.html}
}

@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date = {2018},
  journaltitle = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  doi = {10.1016/j.dsp.2017.10.011},
  keywords = {background,motivation,responsible,xai},
  file = {/Users/emilchri/Zotero/storage/IXU4WQ8N/1910.10045-1.pdf}
}

@article{parkFairVQAFairnessAwareVisual2020,
  title = {Fair-{{VQA}}: {{Fairness-Aware Visual Question Answering Through Sensitive Attribute Prediction}}},
  shorttitle = {Fair-{{VQA}}},
  author = {Park, Sungho and Hwang, Sunhee and Hong, Jongkwang and Byun, Hyeran},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {215091--215099},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3041503},
  abstract = {Visual Question Answering (VQA) is a task that answers questions on given images. Although previous works achieve a great improvement in VQA performance, they do not consider the fairness of answers in terms of ethically sensitive attributes, such as gender. Therefore, we propose a Fair-VQA model that contains two modules: VQA module and SAP (Sensitive Attribute Prediction) module. On top of VQA module, which predicts various kinds of answers, SAP module predicts only sensitive attributes using the same inputs. The predictions of SAP module are utilized to rectify answers from VQA module to be fairer in terms of the sensitive attributes with graceful performance degradation. To validate the proposed method, we conduct extensive experiments on VQA, GQA, and our proposing VQA-Gender datasets. In all the experiments, our method shows the fairest results in various metrics for fairness. Moreover, we demonstrate that our method works interpretably through the analysis of visualized attention maps.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Data models,Face recognition,FAI,fairness,Knowledge discovery,Predictive models,Task analysis,visual question answering,Visualization,VQA},
  file = {/Users/emilchri/Zotero/storage/K5S465UT/Park et al. - 2020 - Fair-VQA Fairness-Aware Visual Question Answering.pdf;/Users/emilchri/Zotero/storage/7QJRTPNI/9274341.html}
}

@article{ponzioDealingLackTraining2019,
  title = {Dealing with {{Lack}} of {{Training Data}} for {{Convolutional Neural Networks}}: {{The Case}} of {{Digital Pathology}}},
  shorttitle = {Dealing with {{Lack}} of {{Training Data}} for {{Convolutional Neural Networks}}},
  author = {Ponzio, Francesco and Urgese, Gianvito and Ficarra, Elisa and Di Cataldo, Santa},
  date = {2019-02-26},
  journaltitle = {Electronics},
  shortjournal = {Electronics},
  volume = {8},
  pages = {256},
  doi = {10.3390/electronics8030256},
  abstract = {Thanks to their capability to learn generalizable descriptors directly from images, deep Convolutional Neural Networks (CNNs) seem the ideal solution to most pattern recognition problems. On the other hand, to learn the image representation, CNNs need huge sets of annotated samples that are unfeasible in many every-day scenarios. This is the case, for example, of Computer-Aided Diagnosis (CAD) systems for digital pathology, where additional challenges are posed by the high variability of the cancerous tissue characteristics. In our experiments, state-of-the-art CNNs trained from scratch on histological images were less accurate and less robust to variability than a traditional machine learning framework, highlighting all the issues of fully training deep networks with limited data from real patients. To solve this problem, we designed and compared three transfer learning frameworks, leveraging CNNs pre-trained on non-medical images. This approach obtained very high accuracy, requiring much less computational resource for the training. Our findings demonstrate that transfer learning is a solution to the automated classification of histological samples and solves the problem of designing accurate and computationally-efficient CAD systems with limited training data.},
  keywords = {computer-aided diagnosis systems,convolutional neural networks,deep learning,histological image analysis,transfer learning},
  file = {/Users/emilchri/Zotero/storage/222973QA/Ponzio et al. - 2019 - Dealing with Lack of Training Data for Convolution.pdf;/Users/emilchri/Zotero/storage/YB368QIX/Ponzio et al. - 2019 - Dealing with Lack of Training Data for Convolution.pdf;/Users/emilchri/Zotero/storage/CFQ9VNCA/256.html}
}

@inproceedings{rasouliEXPLANExplainingBlackbox2020,
  title = {{{EXPLAN}}: {{Explaining Black-box Classifiers}} Using {{Adaptive Neighborhood Generation}}},
  shorttitle = {{{EXPLAN}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Rasouli, Peyman and Yu, Ingrid Chieh},
  date = {2020-07},
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9206710},
  abstract = {Defining a representative locality is an urgent challenge in perturbation-based explanation methods, which influences the fidelity and soundness of explanations. We address this issue by proposing a robust and intuitive approach for EXPLaining black-box classifiers using Adaptive Neighborhood generation (EXPLAN). EXPLAN is a module-based algorithm consisted of dense data generation, representative data selection, data balancing, and rule-based interpretable model. It takes into account the adjacency information derived from the black-box decision function and the structure of the data for creating a representative neighborhood for the instance being explained. As a local model-agnostic explanation method, EXPLAN generates explanations in the form of logical rules that are highly interpretable and well-suited for qualitative analysis of the model's behavior. We discuss fidelity-interpretability trade-offs and demonstrate the performance of the proposed algorithm by a comprehensive comparison with state-of-the-art explanation methods LIME, LORE, and Anchor. The conducted experiments on real-world data sets show our method achieves solid empirical results in terms of fidelity, precision, and stability of explanations.},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Analytical models,Data models,Data Sampling,Decision trees,Interpretable Machine Learning,Machine learning,Neural networks,Perturbation-based Explanation Methods,Predictive models,Training data,XAI},
  file = {/Users/emilchri/Zotero/storage/FUGC4N8V/Rasouli og Yu - 2020 - EXPLAN Explaining Black-box Classifiers using Ada.pdf;/Users/emilchri/Zotero/storage/Y6K8436R/9206710.html}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016},
  pages = {779--788},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {object detection,yolo},
  file = {/Users/emilchri/Zotero/storage/U9GZQFRC/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/emilchri/Zotero/storage/M526BS3A/Redmon_You_Only_Look_CVPR_2016_paper.html}
}

@inproceedings{renFasterRCNNRealTime2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
  urldate = {2022-10-24},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  file = {/Users/emilchri/Zotero/storage/9T8ISWCK/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf}
}

@unpublished{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2022-02-02},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,LIME,Michael,Statistics - Machine Learning,XAI},
  file = {/Users/emilchri/Zotero/storage/KYA42FCA/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/Users/emilchri/Zotero/storage/N96AM4KH/1602.html}
}

@book{rothShapleyValueEssays1988,
  title = {The {{Shapley Value}}: {{Essays}} in {{Honor}} of {{Lloyd S}}. {{Shapley}}},
  shorttitle = {The {{Shapley Value}}},
  editor = {Roth, Alvin E.},
  date = {1988},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511528446},
  url = {https://www.cambridge.org/core/books/shapley-value/D3829B63B5C3108EFB62C4009E2B966E},
  urldate = {2022-11-29},
  abstract = {Composed in honour of the sixty-fifth birthday of Lloyd Shapley, this volume makes accessible the large body of work that has grown out of Shapley's seminal 1953 paper. Each of the twenty essays concerns some aspect of the Shapley value. Three of the chapters are reprints of the 'ancestral' papers: Chapter 2 is Shapley's original 1953 paper defining the value; Chapter 3 is the 1954 paper by Shapley and Shubik applying the value to voting models; and chapter 19 is Shapley's 1969 paper defining a value for games without transferable utility. The other seventeen chapters were contributed especially for this volume. The first chapter introduces the subject and the other essays in the volume, and contains a brief account of a few of Shapley's other major contributions to game theory. The other chapters cover the reformulations, interpretations and generalizations that have been inspired by the Shapley value, and its applications to the study of coalition formulation, to the organization of large markets, to problems of cost allocation, and to the study of games in which utility is not transferable.},
  isbn = {978-0-521-36177-4},
  file = {/Users/emilchri/Zotero/storage/36EG9VR6/D3829B63B5C3108EFB62C4009E2B966E.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-10-06},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  keywords = {backprop,backpropagation,Humanities and Social Sciences,multidisciplinary,rnn,Science},
  file = {/Users/emilchri/Zotero/storage/XL4G6Y63/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/Users/emilchri/Zotero/storage/UB383953/323533a0.html}
}

@article{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-12-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {115},
  number = {3},
  pages = {211--252},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  url = {https://doi.org/10.1007/s11263-015-0816-y},
  urldate = {2023-03-13},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
  langid = {english},
  keywords = {Benchmark,challenge,competition,Dataset,ImageNet,Large-scale,Object detection,Object recognition},
  file = {/Users/emilchri/Zotero/storage/QD8JEQN9/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf}
}

@inproceedings{sagirogluBigDataReview2013,
  title = {Big Data: {{A}} Review},
  shorttitle = {Big Data},
  booktitle = {2013 {{International Conference}} on {{Collaboration Technologies}} and {{Systems}} ({{CTS}})},
  author = {Sagiroglu, Seref and Sinanc, Duygu},
  date = {2013-05},
  pages = {42--47},
  doi = {10.1109/CTS.2013.6567202},
  abstract = {Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.},
  eventtitle = {2013 {{International Conference}} on {{Collaboration Technologies}} and {{Systems}} ({{CTS}})},
  keywords = {big data,Data handling,Data models,Data storage systems,Information management,Organizations,Security,value,variety,velocity,verification,volume},
  file = {/Users/emilchri/Zotero/storage/Y7TM48BR/Sagiroglu og Sinanc - 2013 - Big data A review.pdf;/Users/emilchri/Zotero/storage/BIBDPGXW/6567202.html}
}

@article{salehiSynthesizingTalkingChild2022,
  title = {Synthesizing a {{Talking Child Avatar}} to {{Train Interviewers Working}} with {{Maltreated Children}}},
  author = {Salehi, Pegah and Hassan, Syed Zohaib and Lammerse, Myrthe and Sabet, Saeed Shafiee and Riiser, Ingvild and Røed, Ragnhild Klingenberg and Johnson, Miriam S. and Thambawita, Vajira and Hicks, Steven A. and Powell, Martine and Lamb, Michael E. and Baugerud, Gunn Astrid and Halvorsen, Pål and Riegler, Michael A.},
  date = {2022-06},
  journaltitle = {Big Data and Cognitive Computing},
  volume = {6},
  number = {2},
  pages = {62},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-2289},
  doi = {10.3390/bdcc6020062},
  url = {https://www.mdpi.com/2504-2289/6/2/62},
  urldate = {2023-02-24},
  abstract = {When responding to allegations of child sexual, physical, and psychological abuse, Child Protection Service (CPS) workers and police personnel need to elicit detailed and accurate accounts of the abuse to assist in decision-making and prosecution. Current research emphasizes the importance of the interviewer’s ability to follow empirically based guidelines. In doing so, it is essential to implement economical and scientific training courses for interviewers. Due to recent advances in artificial intelligence, we propose to generate a realistic and interactive child avatar, aiming to mimic a child. Our ongoing research involves the integration and interaction of different components with each other, including how to handle the language, auditory, emotional, and visual components of the avatar. This paper presents three subjective studies that investigate and compare various state-of-the-art methods for implementing multiple aspects of the child avatar. The first user study evaluates the whole system and shows that the system is well received by the expert and highlights the importance of its realism. The second user study investigates the emotional component and how it can be integrated with video and audio, and the third user study investigates realism in the auditory and visual components of the avatar created by different methods. The insights and feedback from these studies have contributed to the refined and improved architecture of the child avatar system which we present here.},
  issue = {2},
  langid = {english},
  keywords = {Child Protection Services (CPS),generative adversarial networks (GANs),generative pre-trained transformer 3 (GPT-3),interview training,Michael,virtual child avatar},
  file = {/Users/emilchri/Zotero/storage/DVUFJ5RQ/Salehi et al. - 2022 - Synthesizing a Talking Child Avatar to Train Inter.pdf}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  url = {http://arxiv.org/abs/1610.02391},
  urldate = {2022-02-02},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Michael},
  file = {/Users/emilchri/Zotero/storage/XMKTRQ4A/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;/Users/emilchri/Zotero/storage/NZ2RIN7L/1610.html}
}

@misc{seoEndtoendGenerativePretraining2022,
  title = {End-to-End {{Generative Pretraining}} for {{Multimodal Video Captioning}}},
  author = {Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  date = {2022-05-10},
  number = {arXiv:2201.08264},
  eprint = {2201.08264},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.08264},
  urldate = {2022-09-12},
  abstract = {Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective -- we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.},
  archiveprefix = {arXiv},
  keywords = {Andrea,Captioning,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,End-to-end,Video,XAI},
  file = {/Users/emilchri/Zotero/storage/9WJ4FZFN/Seo et al. - 2022 - End-to-end Generative Pretraining for Multimodal V.pdf;/Users/emilchri/Zotero/storage/GZVL6PSC/2201.html}
}

@article{shapleyValueNpersonGames,
  title = {A Value for N-Person Games},
  author = {Shapley, Lloyd S},
  pages = {10},
  langid = {english},
  keywords = {Shapley},
  file = {/Users/emilchri/Zotero/storage/AEEQN3ZS/Shapley - A value for i-person games.pdf}
}

@article{sharmaImageCaptioningImproved2022,
  title = {Image Captioning Improved Visual Question Answering},
  author = {Sharma, Himanshu and Jalal, Anand Singh},
  date = {2022-10-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {81},
  number = {24},
  pages = {34775--34796},
  issn = {1573-7721},
  doi = {10.1007/s11042-021-11276-2},
  url = {https://doi.org/10.1007/s11042-021-11276-2},
  urldate = {2022-10-03},
  abstract = {Both Visual Question Answering (VQA) and image captioning are the problems which involve Computer Vision (CV) and Natural Language Processing (NLP) domains. In general, computer vision models are effectively utilized to represent visual contents. While NLP algorithms are used to represent the sentences. In recent years, VQA and image captioning tasks are tackled independently although they require similar type of algorithms. In this paper, a joint relationship between these two tasks is established and exploited. We present an image captioning based VQA model that uses the knowledge learnt from the image captioning task and transfers that knowledge to VQA task. We integrate the image captioning module into the VQA model by fusing the features obtained from captioning model and the attention-based visual feature. The experimental results demonstrate the improvement in the answer generation accuracy by a margin 3.45 \% on VQA 1.0, 3.33\% on VQA 2.0 and 1.73\% on VQA-CP v2 datasets over the state-of-the-art VQA models.},
  langid = {english},
  keywords = {Computer vision (CV),Image captioning,Natural language processing (NLP),Visual question answering (VQA)},
  file = {/Users/emilchri/Zotero/storage/UZKWM4XT/Sharma og Jalal - 2022 - Image captioning improved visual question answerin.pdf}
}

@article{silverGeneralReinforcementLearning2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aar6404},
  url = {https://www.science.org/doi/full/10.1126/science.aar6404},
  urldate = {2022-10-07},
  file = {/Users/emilchri/Zotero/storage/6JJJJ369/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2022-10-17},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,VGG},
  file = {/Users/emilchri/Zotero/storage/D3P9G8IV/Simonyan og Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/Users/emilchri/Zotero/storage/FPFGXZGM/1409.html}
}

@misc{soaresFairbydesignExplainableModels2019,
  title = {Fair-by-Design Explainable Models for Prediction of Recidivism},
  author = {Soares, Eduardo and Angelov, Plamen},
  date = {2019-09-18},
  number = {arXiv:1910.02043},
  eprint = {1910.02043},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.02043},
  urldate = {2022-11-25},
  abstract = {Recidivism prediction provides decision makers with an assessment of the likelihood that a criminal defendant will reoffend that can be used in pre-trial decision-making. It can also be used for prediction of locations where crimes most occur, profiles that are more likely to commit violent crimes. While such instruments are gaining increasing popularity, their use is controversial as they may present potential discriminatory bias in the risk assessment. In this paper we propose a new fair-by-design approach to predict recidivism. It is prototype-based, learns locally and extracts empirically the data distribution. The results show that the proposed method is able to reduce the bias and provide human interpretable rules to assist specialists in the explanation of the given results.},
  archiveprefix = {arXiv},
  keywords = {bias,Computer Science - Machine Learning,preaktiv prediksjon,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/Y2VFNGFF/Soares og Angelov - 2019 - Fair-by-design explainable models for prediction o.pdf;/Users/emilchri/Zotero/storage/XIU78Z6I/1910.html}
}

@misc{srinivasanGeneratingUserfriendlyExplanations2019,
  title = {Generating {{User-friendly Explanations}} for {{Loan Denials}} Using {{GANs}}},
  author = {Srinivasan, Ramya and Chander, Ajay and Pezeshkpour, Pouya},
  date = {2019-06-24},
  number = {arXiv:1906.10244},
  eprint = {1906.10244},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.10244},
  urldate = {2022-09-12},
  abstract = {Financial decisions impact our lives, and thus everyone from the regulator to the consumer is interested in fair, sound, and explainable decisions. There is increasing competitive desire and regulatory incentive to deploy AI mindfully within financial services. An important mechanism towards that end is to explain AI decisions to various stakeholders. State-of-the-art explainable AI systems mostly serve AI engineers and offer little to no value to business decision makers, customers, and other stakeholders. Towards addressing this gap, in this work we consider the scenario of explaining loan denials. We build the first-of-its-kind dataset that is representative of loan-applicant friendly explanations. We design a novel Generative Adversarial Network (GAN) that can accommodate smaller datasets, to generate user-friendly textual explanations. We demonstrate how our system can also generate explanations serving different purposes: those that help educate the loan applicants, or help them take appropriate action towards a future approval.},
  archiveprefix = {arXiv},
  keywords = {Andrea,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/2MB7T7BT/Srinivasan et al. - 2019 - Generating User-friendly Explanations for Loan Den.pdf;/Users/emilchri/Zotero/storage/EXRNA8BU/1906.html}
}

@article{sunExplainImproveLRPinference2022,
  title = {Explain and Improve: {{LRP-inference}} Fine-Tuning for Image Captioning Models},
  shorttitle = {Explain and Improve},
  author = {Sun, Jiamei and Lapuschkin, Sebastian and Samek, Wojciech and Binder, Alexander},
  date = {2022-01-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {77},
  pages = {233--246},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253521001494},
  urldate = {2022-09-20},
  abstract = {This paper analyzes the predictions of image captioning models with attention mechanisms beyond visualizing the attention itself. We develop variants of Layer-wise Relevance Propagation (LRP) and gradient-based explanation methods, tailored to image captioning models with attention mechanisms. We compare the interpretability of attention heatmaps systematically against the explanations provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide simultaneously pixel-wise image explanations (supporting and opposing pixels of the input image) and linguistic explanations (supporting and opposing words of the preceding sequence) for each word in the predicted captions. We demonstrate with extensive experiments that explanation methods (1) can reveal additional evidence used by the model to make decisions compared to attention; (2) correlate to object locations with high precision; (3) are helpful to “debug” the model, e.g. by analyzing the reasons for hallucinated object words. With the observed properties of explanations, we further design an LRP-inference fine-tuning strategy that reduces the issue of object hallucination in image captioning models, and meanwhile, maintains the sentence fluency. We conduct experiments with two widely used attention mechanisms: the adaptive attention mechanism calculated with the additive attention and the multi-head attention mechanism calculated with the scaled dot product.},
  langid = {english},
  keywords = {Attention,Explainable AI,Image captioning,Neural networks},
  file = {/Users/emilchri/Zotero/storage/AIH469ZX/Sun et al. - 2022 - Explain and improve LRP-inference fine-tuning for.pdf;/Users/emilchri/Zotero/storage/B9P978BS/S1566253521001494.html}
}

@article{sunUnderstandingImageCaptioning,
  title = {Understanding {{Image Captioning Models}} beyond {{Visualizing Attention}}},
  author = {Sun, Jiamei and Lapuschkin, Sebastian and Samek, Wojciech and Binder, Alexander},
  pages = {8},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/6GEBNQCL/Sun et al. - Understanding Image Captioning Models beyond Visua.pdf}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
  urldate = {2023-01-17},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  keywords = {rnn},
  file = {/Users/emilchri/Zotero/storage/AYDY2JAT/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  date = {2016},
  pages = {2818--2826},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html},
  urldate = {2022-10-26},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/ZT9JDQ2V/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf;/Users/emilchri/Zotero/storage/JYK2S7HK/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html}
}

@inproceedings{tanwaniRepsNetCombiningVision2022,
  title = {{{RepsNet}}: {{Combining Vision}} with {{Language}} for {{Automated Medical Reports}}},
  shorttitle = {{{RepsNet}}},
  author = {Tanwani, Ajay K. and Barral, Joelle and Freedman, Daniel},
  date = {2022},
  pages = {714--724},
  publisher = {{Springer}},
  file = {/Users/emilchri/Zotero/storage/YZDPIC4N/Tanwani et al. - 2022 - RepsNet Combining Vision with Language for Automated Medical Reports.pdf}
}

@misc{teneyTipsTricksVisual2017,
  title = {Tips and {{Tricks}} for {{Visual Question Answering}}: {{Learnings}} from the 2017 {{Challenge}}},
  shorttitle = {Tips and {{Tricks}} for {{Visual Question Answering}}},
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and van den Hengel, Anton},
  date = {2017-08-09},
  number = {arXiv:1708.02711},
  eprint = {1708.02711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1708.02711},
  url = {http://arxiv.org/abs/1708.02711},
  urldate = {2022-12-07},
  abstract = {This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/6H7PKFRP/Teney et al. - 2017 - Tips and Tricks for Visual Question Answering Lea.pdf;/Users/emilchri/Zotero/storage/KKXR7HAE/1708.html}
}

@article{teneyVisualQuestionAnswering2017,
  title = {Visual {{Question Answering}}: {{A Tutorial}}},
  shorttitle = {Visual {{Question Answering}}},
  author = {Teney, Damien and Wu, Qi and van den Hengel, Anton},
  options = {useprefix=true},
  date = {2017-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {63--75},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2739826},
  abstract = {The task of visual question answering (VQA) is receiving increasing interest from researchers in both the computer vision and natural language processing fields. Tremendous advances have been seen in the field of computer vision due to the success of deep learning, in particular on low- and midlevel tasks, such as image segmentation or object recognition. These advances have fueled researchers' confidence for tackling more complex tasks that combine vision with language and high-level reasoning. VQA is a prime example of this trend. This article presents the ongoing work in the field and the current approaches to VQA based on deep learning. VQA constitutes a test for deep visual understanding and a benchmark for general artificial intelligence (AI). While the field of VQA has seen recent successes, it remains a largely unsolved task.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {bias,Bioinformatics,Computer vision,Genomics,Machine learning,Visualization,vqa},
  file = {/Users/emilchri/Zotero/storage/8N95PY3A/Teney et al. - 2017 - Visual Question Answering A Tutorial.pdf;/Users/emilchri/Zotero/storage/LNW7MSC5/8103161.html}
}

@misc{tiongPlugandPlayVQAZeroshot2022a,
  title = {Plug-and-{{Play VQA}}: {{Zero-shot VQA}} by {{Conjoining Large Pretrained Models}} with {{Zero Training}}},
  shorttitle = {Plug-and-{{Play VQA}}},
  author = {Tiong, Anthony Meng Huat and Li, Junnan and Li, Boyang and Savarese, Silvio and Hoi, Steven C. H.},
  date = {2022-11-16},
  number = {arXiv:2210.08773},
  eprint = {2210.08773},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08773},
  url = {http://arxiv.org/abs/2210.08773},
  urldate = {2022-11-28},
  abstract = {Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5\% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1\% on GQA over FewVLM with 740M PLM parameters. Code is released at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,GradCAM,VQA},
  file = {/Users/emilchri/Zotero/storage/V5YTAZM5/Tiong et al. - 2022 - Plug-and-Play VQA Zero-shot VQA by Conjoining Lar.pdf;/Users/emilchri/Zotero/storage/V99KMHHL/2210.html}
}

@article{tjoaSurveyExplainableArtificial2021,
  title = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}}): {{Toward Medical XAI}}},
  shorttitle = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Tjoa, Erico and Guan, Cuntai},
  date = {2021-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {11},
  pages = {4793--4813},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3027314},
  abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Artificial intelligence,Explainable artificial intelligence (XAI),interpretability,Machine learning,machine learning (ML),Machine learning algorithms,medical information system,Medical information systems,survey},
  file = {/Users/emilchri/Zotero/storage/ACDPS3DS/Tjoa og Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf;/Users/emilchri/Zotero/storage/G92XNF3F/Tjoa og Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf;/Users/emilchri/Zotero/storage/R2HRIN58/9233366.html}
}

@inproceedings{tonekaboniWhatCliniciansWant2019,
  title = {What {{Clinicians Want}}: {{Contextualizing Explainable Machine Learning}} for {{Clinical End Use}}},
  shorttitle = {What {{Clinicians Want}}},
  booktitle = {Proceedings of the 4th {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D. and Goldenberg, Anna},
  date = {2019-10-28},
  pages = {359--380},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v106/tonekaboni19a.html},
  urldate = {2022-10-31},
  abstract = {Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians’ trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the eld suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians’ trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.},
  eventtitle = {Machine {{Learning}} for {{Healthcare Conference}}},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/KXB6F5AA/Tonekaboni et al. - 2019 - What Clinicians Want Contextualizing Explainable .pdf}
}

@inproceedings{tranRichImageCaptioning2016,
  title = {Rich {{Image Captioning}} in the {{Wild}}},
  author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian and Carapcea, Cornelia and Thrasher, Chris and Buehler, Chris and Sienkiewicz, Chris},
  date = {2016},
  pages = {49--56},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/html/Tran_Rich_Image_Captioning_CVPR_2016_paper.html},
  urldate = {2022-10-25},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {/Users/emilchri/Zotero/storage/QP242BQZ/Tran et al. - 2016 - Rich Image Captioning in the Wild.pdf;/Users/emilchri/Zotero/storage/NNWK37TZ/Tran_Rich_Image_Captioning_CVPR_2016_paper.html}
}

@article{vinyalsShowTellLessons2017,
  title = {Show and {{Tell}}: {{Lessons Learned}} from the 2015 {{MSCOCO Image Captioning Challenge}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2017-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {4},
  pages = {652--663},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2587640},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Computational modeling,Computer vision,Image captioning,language model,Logic gates,Natural languages,recurrent neural network,Recurrent neural networks,sequence-to-sequence,Training,Visualization},
  file = {/Users/emilchri/Zotero/storage/8JSIPJX6/Vinyals et al. - 2017 - Show and Tell Lessons Learned from the 2015 MSCOC.pdf;/Users/emilchri/Zotero/storage/K3FQNBNN/7505636.html}
}

@inproceedings{vinyalsShowTellNeural2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2015},
  pages = {3156--3164},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/8YC3V834/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf;/Users/emilchri/Zotero/storage/Q72N6XK5/Vinyals_Show_and_Tell_2015_CVPR_paper.html}
}

@misc{wangEndtoEndTransformerBased2022,
  title = {End-to-{{End Transformer Based Model}} for {{Image Captioning}}},
  author = {Wang, Yiyu and Xu, Jungang and Sun, Yingfei},
  date = {2022-03-29},
  number = {arXiv:2203.15350},
  eprint = {2203.15350},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.15350},
  urldate = {2022-11-06},
  abstract = {CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt SwinTransformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2\% (single model) and 141.0\% (ensemble of 4 models) CIDEr scores on `Karpathy' offline test split and 136.0\% (c5) and 138.3\% (c40) CIDEr scores on the official online test server. Trained models and source code will be released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/U5VJ2WZX/Wang et al. - 2022 - End-to-End Transformer Based Model for Image Capti.pdf;/Users/emilchri/Zotero/storage/WU3Z3JYC/2203.html}
}

@unpublished{watsonAttackagnosticAdversarialDetection2021,
  title = {Attack-Agnostic {{Adversarial Detection}} on {{Medical Data Using Explainable Machine Learning}}},
  author = {Watson, Matthew and Moubayed, Noura Al},
  date = {2021-05-05},
  number = {arXiv:2105.01959},
  eprint = {2105.01959},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.01959},
  urldate = {2022-05-29},
  abstract = {Explainable machine learning has become increasingly prevalent, especially in healthcare where explainable models are vital for ethical and trusted automated decision making. Work on the susceptibility of deep learning models to adversarial attacks has shown the ease of designing samples to mislead a model into making incorrect predictions. In this work, we propose a model agnostic explainability-based method for the accurate detection of adversarial samples on two datasets with different complexity and properties: Electronic Health Record (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR datasets, we report a detection accuracy of 77\% against the Longitudinal Adversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88\%; significantly improving on the state of the art of adversarial detection in both datasets by over 10\% in all settings. We propose an anomaly detection based method using explainability techniques to detect adversarial samples which is able to generalise to different attack methods without a need for retraining.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,I.2,I.4},
  file = {/Users/emilchri/Zotero/storage/9XCD2GTA/Watson og Moubayed - 2021 - Attack-agnostic Adversarial Detection on Medical D.pdf;/Users/emilchri/Zotero/storage/JFYEUUZX/2105.html}
}

@inproceedings{weldChallengeCraftingIntelligible2019,
  title = {The Challenge of Crafting Intelligible Intelligence},
  author = {Weld, Daniel S and Bansal, Gagan},
  date = {2019},
  series = {6},
  volume = {62},
  pages = {70--79},
  publisher = {{ACM New York, NY, USA}},
  url = {https://par.nsf.gov/servlets/purl/10074856},
  urldate = {2022-11-25},
  eventtitle = {Communications of the {{ACM}}},
  file = {/Users/emilchri/Zotero/storage/U3M2M6JL/10074856.pdf}
}

@online{WhatUnsupervisedLearning,
  title = {What Is {{Unsupervised Learning}}? | {{IBM}}},
  shorttitle = {What Is {{Unsupervised Learning}}?},
  url = {https://www.ibm.com/topics/unsupervised-learning},
  urldate = {2023-02-28},
  abstract = {Learn how unsupervised learning works and how it can be used to explore and cluster data},
  langid = {american},
  keywords = {machine learning}
}

@article{wickramanayakeFLEXFaithfulLinguistic2019,
  title = {{{FLEX}}: {{Faithful Linguistic Explanations}} for {{Neural Net Based Model Decisions}}},
  shorttitle = {{{FLEX}}},
  author = {Wickramanayake, Sandareka and Hsu, Wynne and Lee, Mong Li},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {2539--2546},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33012539},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4100},
  urldate = {2022-09-12},
  abstract = {Explaining the decisions of a Deep Learning Network is imperative to safeguard end-user trust. Such explanations must be intuitive, descriptive, and faithfully explain why a model makes its decisions. In this work, we propose a framework called FLEX (Faithful Linguistic EXplanations) that generates post-hoc linguistic justifications to rationalize the decision of a Convolutional Neural Network. FLEX explains a model’s decision in terms of features that are responsible for the decision. We derive a novel way to associate such features to words, and introduce a new decision-relevance metric that measures the faithfulness of an explanation to a model’s reasoning. Experiment results on two benchmark datasets demonstrate that the proposed framework can generate discriminative and faithful explanations compared to state-of-the-art explanation generators. We also show how FLEX can generate explanations for images of unseen classes as well as automatically annotate objects in images.},
  issue = {01},
  langid = {english},
  keywords = {Andrea},
  file = {/Users/emilchri/Zotero/storage/U65TKSV9/Wickramanayake et al. - 2019 - FLEX Faithful Linguistic Explanations for Neural .pdf}
}

@inproceedings{xuFinegrainedImageClassification2018,
  title = {Fine-Grained {{Image Classification}} by {{Visual-Semantic Embedding}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xu, Huapeng and Qi, Guilin and Li, Jingjing and Wang, Meng and Xu, Kang and Gao, Huan},
  date = {2018-07},
  pages = {1043--1049},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/145},
  url = {https://www.ijcai.org/proceedings/2018/145},
  urldate = {2023-02-27},
  abstract = {This paper investigates a challenging problem, which is known as fine-grained image classification (FGIC). Different from conventional computer vision problems, FGIC suffers from the large intraclass diversities and subtle inter-class differences. Existing FGIC approaches are limited to explore only the visual information embedded in the images. In this paper, we present a novel approach which can use handy prior knowledge from either structured knowledge bases or unstructured text to facilitate FGIC. Specifically, we propose a visualsemantic embedding model which explores semantic embedding from knowledge bases and text, and further trains a novel end-to-end CNN framework to linearly map image features to a rich semantic embedding space. Experimental results on a challenging large-scale UCSD Bird-200-2011 dataset verify that our approach outperforms several stateof-the-art methods with significant advances.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/4RT8GIRQ/Xu et al. - 2018 - Fine-grained Image Classification by Visual-Semant.pdf}
}

@article{yangImageCaptioningAsking2019,
  title = {Image {{Captioning}} by {{Asking Questions}}},
  author = {Yang, Xiaoshan and Xu, Changsheng},
  date = {2019-08-12},
  journaltitle = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  shortjournal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  volume = {15},
  pages = {1--19},
  issn = {1551-6857, 1551-6865},
  doi = {10.1145/3313873},
  url = {https://dl.acm.org/doi/10.1145/3313873},
  urldate = {2022-10-03},
  abstract = {Image captioning and visual question answering are typical tasks that connect computer vision and natural language processing. Both of them need to effectively represent the visual content using computer vision methods and smoothly process the text sentence using natural language processing skills. The key problem of these two tasks is to infer the target result based on the interactive understanding of the word sequence and the image. Though they practically use similar algorithms, they are studied independently in the past few years. In this article, we attempt to exploit the mutual correlation between these two tasks. We propose the first VQA-improved image-captioning method that transfers the knowledge learned from the VQA corpora to the image-captioning task. A VQA model is first pretrained on image--question--answer instances. Then, the pretrained VQA model is used to extract VQA-grounded semantic representations according to selected free-form open-ended visual question--answer pairs. The VQA-grounded features are complementary to the visual features, because they interpret images from a different perspective. We incorporate the VQA model into the image-captioning model by adaptively fusing the VQA-grounded feature and the attended visual feature. We show that such simple VQA-improved image-captioning (VQA-IIC) models perform better than conventional image-captioning methods on large-scale public datasets.},
  issue = {2s},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/EJ5J4WTM/Yang og Xu - 2019 - Image Captioning by Asking Questions.pdf}
}

@inproceedings{youImageCaptioningSemantic2016,
  title = {Image {{Captioning With Semantic Attention}}},
  author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  date = {2016},
  pages = {4651--4659},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/You_Image_Captioning_With_CVPR_2016_paper.html},
  urldate = {2023-01-17},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/KS8HNH5G/You et al. - 2016 - Image Captioning With Semantic Attention.pdf}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016},
  pages = {2921--2929},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html},
  urldate = {2022-10-06},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/CCHCT8LX/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf;/Users/emilchri/Zotero/storage/XHE5HEMV/Zhou_Learning_Deep_Features_CVPR_2016_paper.html}
}

@article{zhouUnifiedVisionLanguagePreTraining2020,
  title = {Unified {{Vision-Language Pre-Training}} for {{Image Captioning}} and {{VQA}}},
  author = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {13041--13049},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i07.7005},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/7005},
  urldate = {2022-10-18},
  abstract = {This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.},
  issue = {07},
  langid = {english},
  keywords = {image caption,VQA},
  file = {/Users/emilchri/Zotero/storage/VAKY2LXJ/Zhou et al. - 2020 - Unified Vision-Language Pre-Training for Image Cap.pdf}
}
