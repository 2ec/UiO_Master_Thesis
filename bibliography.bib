@inproceedings{A800276Pdf,
  title = {A800276.Pdf},
  url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf},
  urldate = {2023-03-10}
}

@online{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying {{Attention Flow}} in {{Transformers}}},
  author = {Abnar, Samira and Zuidema, Willem},
  date = {2020-05-31},
  eprint = {2005.00928},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.00928},
  url = {http://arxiv.org/abs/2005.00928},
  urldate = {2023-05-12},
  abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  pubstate = {preprint},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,transformer,xai},
  file = {/Users/emilchri/Zotero/storage/8AGHHYFL/Abnar og Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf;/Users/emilchri/Zotero/storage/IVPEJTJP/2005.html}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  file = {/Users/emilchri/Zotero/storage/AZRC3A9V/Adadi og Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf;/Users/emilchri/Zotero/storage/H6YFR9XP/8466590.html}
}

@online{agrawalVQAVisualQuestion2016,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
  date = {2016-10-26},
  eprint = {1505.00468},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.00468},
  urldate = {2022-10-03},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing \textasciitilde 0.25M images, \textasciitilde 0.76M questions, and \textasciitilde 10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/CXSE52DD/Agrawal et al. - 2016 - VQA Visual Question Answering.pdf;/Users/emilchri/Zotero/storage/QTNA84XM/1505.html}
}

@article{alyafeaiFullyautomatedDeepLearning2020,
  title = {A Fully-Automated Deep Learning Pipeline for Cervical Cancer Classification},
  author = {Alyafeai, Zaid and Ghouti, Lahouari},
  date = {2020-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {141},
  pages = {112951},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.112951},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417419306694},
  urldate = {2023-03-10},
  abstract = {Cervical cancer ranks the fourth most common cancer among females worldwide with roughly 528, 000 new cases yearly. Around 85\% of the new cases occurred in less-developed countries. In these countries, the high fatality rate is mainly attributed to the lack of skilled medical staff and appropriate medical pre-screening procedures. Images capturing the cervical region, known as cervigrams, are the gold-standard for the basic evaluation of cervical cancer presence. Cervigrams have high inter-rater variability especially among less skilled medical specialists. In this paper, we develop a fully-automated pipeline for cervix detection and cervical cancer classification from cervigram images. The proposed pipeline consists of two pre-trained deep learning models for the automatic cervix detection and cervical tumor classification. The first model detects the cervix region 1000 times faster than state-of-the-art data-driven models while achieving a detection accuracy of 0.68 in terms of intersection of union (IoU) measure. Self-extracted features are used by the second model to classify the cervix tumors. These features are learned using two lightweight models based on convolutional neural networks (CNN). The proposed deep learning classifier outperforms existing models in terms of classification accuracy and speed. Our classifier is characterized by an area under the curve (AUC) score of 0.82 while classifying each cervix region 20 times faster. Finally, the pipeline accuracy, speed and lightweight architecture make it very appropriate for mobile phone deployment. Such deployment is expected to drastically enhance the early detection of cervical cancer in less-developed countries.},
  langid = {english},
  keywords = {Cervical cancer,Cervical region-of-interest,Cervix detection,Convolutional neural networks,Deep learning,graph,Guanacaste and Intel\&mobileodt cervigram datasets,imagenet},
  file = {/Users/emilchri/Zotero/storage/5NB4LLA2/Alyafeai og Ghouti - 2020 - A fully-automated deep learning pipeline for cervi.pdf;/Users/emilchri/Zotero/storage/NW7RMS3M/S0957417419306694.html}
}

@online{AmazonMechanicalTurk,
  title = {Amazon {{Mechanical Turk Documentation}}},
  url = {https://docs.aws.amazon.com/mturk/index.html},
  urldate = {2023-05-05},
  file = {/Users/emilchri/Zotero/storage/R9FRPMUJ/index.html}
}

@article{andresenJohnMcCarthyFather2002,
  title = {John {{McCarthy}}: Father of {{AI}}},
  shorttitle = {John {{McCarthy}}},
  author = {Andresen, S.L.},
  date = {2002-09},
  journaltitle = {IEEE Intelligent Systems},
  volume = {17},
  number = {5},
  pages = {84--85},
  issn = {1941-1294},
  doi = {10.1109/MIS.2002.1039837},
  abstract = {If John McCarthy, the father of AI, were to coin a new phrase for "artificial intelligence" today, he would probably use "computational intelligence." McCarthy is not just the father of AI, he is also the inventor of the Lisp (list processing) language. The author considers McCarthy's conception of Lisp and discusses McCarthy's recent research that involves elaboration tolerance, creativity by machines, free will of machines, and some improved ways of doing situation calculus.},
  eventtitle = {{{IEEE Intelligent Systems}}},
  keywords = {Artificial intelligence,Computer languages,Computer science,Friction,Humans,Laboratories,Logic,Mathematical programming,Mathematics,Time sharing computer systems},
  file = {/Users/emilchri/Zotero/storage/X5NE24X4/Andresen - 2002 - John McCarthy father of AI.pdf;/Users/emilchri/Zotero/storage/HLYSB3K7/1039837.html}
}

@online{artemandreenko[@miolini]VeSucefullyRunned2023,
  type = {Tweet},
  title = {I've Sucefully Runned {{LLaMA 7B}} Model on My {{4GB RAM Raspberry Pi}} 4. {{It}}'s Super Slow about 10sec/Token. {{But}} It Looks We Can Run Powerful Cognitive Pipelines on a Cheap Hardware.},
  author = {{Artem Andreenko [@miolini]}},
  date = {2023-03-12},
  url = {https://twitter.com/miolini/status/1634982361757790209},
  urldate = {2023-04-08},
  langid = {english},
  organization = {{Twitter}},
  keywords = {raspberry pi},
  file = {/Users/emilchri/Zotero/storage/VJRK583H/1634982361757790209.html}
}

@online{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2023-01-17},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  pubstate = {preprint},
  keywords = {at,attention,Attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,rnn,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/7CY63FRD/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/Users/emilchri/Zotero/storage/RZDSQBE8/1409.html}
}

@online{barbastathisUseDeepLearning,
  title = {On the Use of Deep Learning for Computational Imaging},
  author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
  url = {https://opg.optica.org/optica/fulltext.cfm?uri=optica-6-8-921&id=416103},
  urldate = {2023-04-28},
  keywords = {computational,computer,imaging,ml}
}

@inproceedings{barkanGradSAMExplainingTransformers2021,
  title = {Grad-{{SAM}}: {{Explaining Transformers}} via {{Gradient Self-Attention Maps}}},
  shorttitle = {Grad-{{SAM}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Barkan, Oren and Hauon, Edan and Caciularu, Avi and Katz, Ori and Malkiel, Itzik and Armstrong, Omri and Koenigstein, Noam},
  date = {2021-10-30},
  series = {{{CIKM}} '21},
  pages = {2882--2887},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3459637.3482126},
  url = {https://dl.acm.org/doi/10.1145/3459637.3482126},
  urldate = {2023-05-12},
  abstract = {Transformer-based language models significantly advanced the state-of-the-art in many linguistic tasks. As this revolution continues, the ability to explain model predictions has become a major area of interest for the NLP community. In this work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the model's prediction the best. Extensive evaluations on various benchmarks show that Grad-SAM obtains significant improvements over state-of-the-art alternatives.},
  isbn = {978-1-4503-8446-9},
  keywords = {attention,bert,cam,deep learning,explainable \& interpretable ai,grad,gradcam,nlp,self-attention,transformer,transformers,transparent machine learning,xai},
  file = {/Users/emilchri/Zotero/storage/R87ZG4BZ/Barkan et al. - 2021 - Grad-SAM Explaining Transformers via Gradient Sel.pdf}
}

@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2020-06-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  urldate = {2022-10-25},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  langid = {english},
  keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  file = {/Users/emilchri/Zotero/storage/96GS5YMP/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf;/Users/emilchri/Zotero/storage/FU7UYCHV/S1566253519308103.html}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  date = {2019-08-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1903070116},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116},
  urldate = {2023-05-05},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  keywords = {data,more,worse},
  file = {/Users/emilchri/Zotero/storage/FS6S8HJD/Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf}
}

@article{ben-younesBLOCKBilinearSuperdiagonal2019,
  title = {{{BLOCK}}: {{Bilinear Superdiagonal Fusion}} for {{Visual Question Answering}} and {{Visual Relationship Detection}}},
  shorttitle = {{{BLOCK}}},
  author = {Ben-younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {8102--8109},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33018102},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4818},
  urldate = {2023-02-21},
  abstract = {Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.},
  issue = {01},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/I2ZSAFME/Ben-younes et al. - 2019 - BLOCK Bilinear Superdiagonal Fusion for Visual Qu.pdf}
}

@inproceedings{ben-younesMUTANMultimodalTucker2017,
  title = {{{MUTAN}}: {{Multimodal Tucker Fusion}} for {{Visual Question Answering}}},
  shorttitle = {{{MUTAN}}},
  author = {Ben-younes, Hedi and Cadene, Rémi and Cord, Matthieu and Thome, Nicolas},
  date = {2017-05-18},
  eprint = {1705.06676},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.06676},
  url = {http://arxiv.org/abs/1705.06676},
  urldate = {2023-02-21},
  abstract = {Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/FHAZGIG9/Ben-younes et al. - 2017 - MUTAN Multimodal Tucker Fusion for Visual Questio.pdf;/Users/emilchri/Zotero/storage/P28EPNS6/Ben-younes et al. - 2017 - MUTAN Multimodal Tucker Fusion for Visual Questio.pdf;/Users/emilchri/Zotero/storage/DM7CVQUP/1705.html}
}

@article{bianchiniComplexityNeuralNetwork2014,
  title = {On the {{Complexity}} of {{Neural Network Classifiers}}: {{A Comparison Between Shallow}} and {{Deep Architectures}}},
  shorttitle = {On the {{Complexity}} of {{Neural Network Classifiers}}},
  author = {Bianchini, Monica and Scarselli, Franco},
  date = {2014-08},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {8},
  pages = {1553--1565},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2013.2293637},
  abstract = {Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden layers. In fact, experimental results and heuristic considerations suggest that deep architectures are more suitable than shallow ones for modern applications, facing very complex problems, e.g., vision and human language understanding. However, the actual theoretical results supporting such a claim are still few and incomplete. In this paper, we propose a new approach to study how the depth of feedforward neural networks impacts on their ability in implementing high complexity functions. First, a new measure based on topological concepts is introduced, aimed at evaluating the complexity of the function implemented by a neural network, used for classification purposes. Then, deep and shallow neural architectures with common sigmoidal activation functions are compared, by deriving upper and lower bounds on their complexity, and studying how the complexity depends on the number of hidden units and the used activation function. The obtained results seem to support the idea that deep networks actually implements functions of higher complexity, so that they are able, with the same number of resources, to address more difficult problems.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Betti numbers,Biological neural networks,Complexity theory,Computer architecture,deep neural networks,function approximation,Neurons,Polynomials,topological complexity,Upper bound,Vapnik–Chervonenkis dimension (VC-dim),Vapnik–Chervonenkis dimension (VC-dim).},
  file = {/Users/emilchri/Zotero/storage/ALQG8WFV/Bianchini og Scarselli - 2014 - On the Complexity of Neural Network Classifiers A.pdf;/Users/emilchri/Zotero/storage/INKV5AAJ/6697897.html}
}

@article{biswasRoleChatGPT2023,
  title = {Role of {{Chat GPT}} in {{Public Health}}},
  author = {Biswas, Som S.},
  date = {2023-05-01},
  journaltitle = {Annals of Biomedical Engineering},
  shortjournal = {Ann Biomed Eng},
  volume = {51},
  number = {5},
  pages = {868--869},
  issn = {1573-9686},
  doi = {10.1007/s10439-023-03172-7},
  url = {https://doi.org/10.1007/s10439-023-03172-7},
  urldate = {2023-04-28},
  abstract = {ChatGPT, a language model developed by OpenAI, has the potential to play a role in public health. With its ability to generate human-like text based on large amounts of data, ChatGPT has the potential to support individuals and communities in making informed decisions about their health (Panch et al. Lancet Digit Health 1:e13–e14, 2019; Baclic et al. Canada Commun Dis Rep 46.6:161, 2020). However, as with any technology, there are limitations and challenges to consider when using ChatGPT in public health. In this overview, we will examine the potential uses of ChatGPT in public health, as well as the advantages and disadvantages of its use.},
  langid = {english},
  keywords = {AI,chatbot,chatGPT,Public health},
  file = {/Users/emilchri/Zotero/storage/Q8ZAY36X/Biswas - 2023 - Role of Chat GPT in Public Health.pdf}
}

@online{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  date = {2020-04-22},
  eprint = {2004.10934},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2004.10934},
  url = {http://arxiv.org/abs/2004.10934},
  urldate = {2023-04-09},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of \textasciitilde 65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/emilchri/Zotero/storage/BPK8DQMK/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf;/Users/emilchri/Zotero/storage/RB4NDNBL/2004.html}
}

@online{bohleHolisticallyExplainableVision2023,
  title = {Holistically {{Explainable Vision Transformers}}},
  author = {Böhle, Moritz and Fritz, Mario and Schiele, Bernt},
  date = {2023-01-20},
  eprint = {2301.08669},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2301.08669},
  url = {http://arxiv.org/abs/2301.08669},
  urldate = {2023-05-12},
  abstract = {Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon.},
  pubstate = {preprint},
  keywords = {Attention,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,transformer,xai},
  file = {/Users/emilchri/Zotero/storage/6LG66MWX/Böhle et al. - 2023 - Holistically Explainable Vision Transformers.pdf;/Users/emilchri/Zotero/storage/R6CXFP74/2301.html}
}

@article{bolognaCharacterizationSymbolicRules2017,
  title = {Characterization of Symbolic Rules Embedded in Deep {{DIMLP}} Networks : A Challenge to Transparency of Deep Learning},
  shorttitle = {Characterization of Symbolic Rules Embedded in Deep {{DIMLP}} Networks},
  author = {Bologna, G. and Hayashi, Y.},
  date = {2017},
  journaltitle = {Journal of Artificial Intelligence and Soft Computing Research},
  volume = {Vol. 7, No. 4},
  issn = {2083-2567},
  doi = {10.1515/jaiscr-2017-0019},
  url = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-14c8b34f-846b-4705-a3ab-8f41c7aa2e81},
  urldate = {2022-10-28},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/5WS7I8CH/Bologna og Hayashi - 2017 - Characterization of symbolic rules embedded in dee.pdf;/Users/emilchri/Zotero/storage/RUUA8VH5/bwmeta1.element.html}
}

@article{borgliHyperKvasirComprehensiveMulticlass2020,
  title = {{{HyperKvasir}}, a Comprehensive Multi-Class Image and Video Dataset for Gastrointestinal Endoscopy},
  author = {Borgli, Hanna and Thambawita, Vajira and Smedsrud, Pia H. and Hicks, Steven and Jha, Debesh and Eskeland, Sigrun L. and Randel, Kristin Ranheim and Pogorelov, Konstantin and Lux, Mathias and Nguyen, Duc Tien Dang and Johansen, Dag and Griwodz, Carsten and Stensland, Håkon K. and Garcia-Ceja, Enrique and Schmidt, Peter T. and Hammer, Hugo L. and Riegler, Michael A. and Halvorsen, Pål and family=Lange, given=Thomas, prefix=de, useprefix=true},
  date = {2020-08-28},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {7},
  number = {1},
  pages = {283},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-00622-y},
  url = {https://www.nature.com/articles/s41597-020-00622-y},
  urldate = {2023-03-17},
  abstract = {Artificial intelligence is currently a hot topic in medicine. However, medical data is often sparse and hard to obtain due to legal restrictions and lack of medical personnel for the cumbersome and tedious process to manually label training data. These constraints make it difficult to develop systems for automatic analysis, like detecting disease or other lesions. In this respect, this article presents HyperKvasir, the largest image and video dataset of the gastrointestinal tract available today. The data is collected during real gastro- and colonoscopy examinations at Bærum Hospital in Norway and partly labeled by experienced gastrointestinal endoscopists. The dataset contains 110,079 images and 374 videos, and represents anatomical landmarks as well as pathological and normal findings. The total number of images and video frames together is around 1 million. Initial experiments demonstrate the potential benefits of artificial intelligence-based computer-assisted diagnosis systems. The HyperKvasir dataset can play a valuable role in developing better algorithms and computer-assisted examination systems not only for gastro- and colonoscopy, but also for other fields in medicine.},
  issue = {1},
  langid = {english},
  keywords = {Gastrointestinal diseases,Health care,Michael},
  file = {/Users/emilchri/Zotero/storage/9B4QC6BR/Borgli et al. - 2020 - HyperKvasir, a comprehensive multi-class image and.pdf}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2023-03-16},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  keywords = {gpt,gpt-3,nlp,openai,transformer},
  file = {/Users/emilchri/Zotero/storage/DUDHTMLC/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{CaffeDeepLearning,
  title = {Caffe {{Deep Learning Framework}} and {{NVIDIA GPU Acceleration}}},
  url = {https://www.nvidia.com/en-au/data-center/gpu-accelerated-applications/caffe/},
  urldate = {2023-04-17},
  abstract = {Run deep learning training with Caffe up to 65\% faster on the latest NVIDIA Pascal GPUs. Learn more.},
  langid = {australian},
  organization = {{NVIDIA}},
  keywords = {caffe,cuda},
  file = {/Users/emilchri/Zotero/storage/7E6CA3A3/caffe.html}
}

@online{CaffeInstallation,
  title = {Caffe {{Installation}}},
  url = {http://caffe.berkeleyvision.org/installation.html#prerequisites},
  urldate = {2023-04-17},
  file = {/Users/emilchri/Zotero/storage/UCPZHK3D/installation.html}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  date = {2002-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2022-11-08},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  langid = {english},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/Users/emilchri/Zotero/storage/HCKFEUE2/Campbell et al. - 2002 - Deep Blue.pdf;/Users/emilchri/Zotero/storage/ZTUW54NT/S0004370201001291.html}
}

@book{capekRossumUniversalRobots,
  title = {R.{{U}}.{{R}}. ({{Rossum}}'s Universal Robots)},
  author = {Čapek, Karel},
  edition = {2004},
  publisher = {{Penguin Books}},
  location = {{London}},
  url = {https://www.gutenberg.org/files/59112/59112-h/59112-h.htm}
}

@inproceedings{caruanaIntelligibleModelsHealthCare2015,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  date = {2015-08-10},
  series = {{{KDD}} '15},
  pages = {1721--1730},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2783258.2788613},
  url = {https://doi.org/10.1145/2783258.2788613},
  urldate = {2022-10-25},
  abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  isbn = {978-1-4503-3664-2},
  keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction},
  file = {/Users/emilchri/Zotero/storage/QAC2UZVW/Caruana et al. - 2015 - Intelligible Models for HealthCare Predicting Pne.pdf}
}

@article{castelvecchiCanWeOpen2016,
  title = {Can We Open the Black Box of {{AI}}?},
  author = {Castelvecchi, Davide},
  date = {2016-10-06},
  journaltitle = {Nature News},
  volume = {538},
  number = {7623},
  pages = {20},
  doi = {10.1038/538020a},
  url = {http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
  urldate = {2022-10-24},
  abstract = {Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn.},
  langid = {english},
  annotation = {Cg\_type: Nature News},
  file = {/Users/emilchri/Zotero/storage/RJ68N9QP/Castelvecchi - 2016 - Can we open the black box of AI.pdf;/Users/emilchri/Zotero/storage/UP6SQMRP/can-we-open-the-black-box-of-ai-1.html}
}

@online{ChatGPT,
  title = {{{ChatGPT}}},
  url = {https://chat.openai.com},
  urldate = {2023-04-07},
  abstract = {A conversational AI system that listens, learns, and challenges},
  keywords = {openai},
  file = {/Users/emilchri/Zotero/storage/EMY7YSD4/login.html}
}

@inproceedings{cheferGenericAttentionModelExplainability2021,
  title = {Generic {{Attention-Model Explainability}} for {{Interpreting Bi-Modal}} and {{Encoder-Decoder Transformers}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  date = {2021},
  pages = {397--406},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.html},
  urldate = {2023-05-12},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  keywords = {computer,transformer,vision,xai},
  file = {/Users/emilchri/Zotero/storage/BX66TRQJ/Chefer et al. - 2021 - Generic Attention-Model Explainability for Interpr.pdf}
}

@inproceedings{cheferTransformerInterpretabilityAttention2021,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  date = {2021},
  pages = {782--791},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html},
  urldate = {2023-05-12},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  keywords = {attention,transformer,xai},
  file = {/Users/emilchri/Zotero/storage/8N9TB8RB/Chefer et al. - 2021 - Transformer Interpretability Beyond Attention Visu.pdf}
}

@article{chengRumorGeneticMutation2021,
  title = {From Rumor to Genetic Mutation Detection with Explanations: A {{GAN}} Approach},
  shorttitle = {From Rumor to Genetic Mutation Detection with Explanations},
  author = {Cheng, Mingxi and Li, Yizhi and Nazarian, Shahin and Bogdan, Paul},
  date = {2021-03-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {5861},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-84993-1},
  url = {https://www.nature.com/articles/s41598-021-84993-1},
  urldate = {2022-09-12},
  abstract = {Social media have emerged as increasingly popular means and environments for information gathering and propagation. This vigorous growth of social media contributed not only to a pandemic (fast-spreading and far-reaching) of rumors and misinformation, but also to an urgent need for text-based rumor detection strategies. To speed up the detection of misinformation, traditional rumor detection methods based on hand-crafted feature selection need to be replaced by automatic artificial intelligence (AI) approaches. AI decision making systems require to provide explanations in order to assure users of their trustworthiness. Inspired by the thriving development of generative adversarial networks (GANs) on text applications, we propose a GAN-based layered model for rumor detection with explanations. To demonstrate the universality of the proposed approach, we demonstrate its benefits on a gene classification with mutation detection case study. Similarly to the rumor detection, the gene classification can also be formulated as a text-based classification problem. Unlike fake news detection that needs a previously collected verified news database, our model provides explanations in rumor detection based on tweet-level texts only without referring to a verified news database. The layered structure of both generative and discriminative models contributes to the outstanding performance. The layered generators produce rumors by intelligently inserting controversial information in non-rumors, and force the layered discriminators to detect detailed glitches and deduce exactly which parts in the sentence are problematic. On average, in the rumor detection task, our proposed model outperforms state-of-the-art baselines on PHEME dataset by \$\$26.85\textbackslash\%\$\$in terms of macro-f1. The excellent performance of our model for textural sequences is also demonstrated by the gene mutation case study on which it achieves \$\$72.69\textbackslash\%\$\$macro-f1 score.},
  issue = {1},
  langid = {english},
  keywords = {Andrea,Computer science,Mathematics and computing},
  file = {/Users/emilchri/Zotero/storage/JMNH3NFN/Cheng et al. - 2021 - From rumor to genetic mutation detection with expl.pdf;/Users/emilchri/Zotero/storage/8HUVB8GQ/s41598-021-84993-1.html}
}

@online{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014-09-02},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.1078},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2023-01-17},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/JIRNXW36/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/Users/emilchri/Zotero/storage/K8SXGTIF/1406.html}
}

@online{chowdheryPaLMScalingLanguage2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022-10-05},
  eprint = {2204.02311},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.02311},
  url = {http://arxiv.org/abs/2204.02311},
  urldate = {2023-05-03},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,palm},
  file = {/Users/emilchri/Zotero/storage/DRVHNE8T/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf;/Users/emilchri/Zotero/storage/XWAXSEAW/2204.html}
}

@article{chunDeepLearningbasedImage2022,
  title = {A Deep Learning-Based Image Captioning Method to Automatically Generate Comprehensive Explanations of Bridge Damage},
  author = {Chun, Pang-Jo and Yamane, Tatsuro and Maemura, Yu},
  date = {2022},
  journaltitle = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {37},
  number = {11},
  pages = {1387--1401},
  issn = {1467-8667},
  doi = {10.1111/mice.12793},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12793},
  urldate = {2022-09-12},
  abstract = {Photographs of bridges can reveal considerable technical information such as the part of the structure that is damaged and the type of damage. Maintenance and inspection engineers can benefit greatly from a technology that can automatically extract and express such information in readable sentences. This is possibly the first study on developing a deep learning model that can generate sentences describing the damage condition of a bridge from images through an image captioning method. Our study shows that by introducing an attention mechanism into the deep learning model, highly accurate descriptive sentences can be generated. In addition, often multiple forms of damage can be observed in the images of bridges; hence, our algorithm is adapted to output multiple sentences to provide a comprehensive interpretation of complex images. In our dataset, the scores of Bilingual Evaluation Understudy (BLEU)-1 to BLEU-4 were 0.782, 0.749, 0.711, and 0.693, respectively, and the percentage of correctly output explanatory sentences is 69.3\%. All of these results are better than the model without the attention mechanism. The developed method makes it possible to provide user-friendly, text-based explanations of bridge damage in images, making it easier for engineers with relatively little experience and even administrative staff without extensive technical expertise to understand images of bridge damage. Future research in this field is expected to lead to the unification of field expertise with artificial intelligence (AI), which will be the foundation of the evolutionary development of bridge inspection AI.},
  langid = {english},
  keywords = {Andrea,Captioning},
  file = {/Users/emilchri/Zotero/storage/R252XL5Q/Chun et al. - 2022 - A deep learning-based image captioning method to a.pdf;/Users/emilchri/Zotero/storage/9BUM7VXG/mice.html}
}

@article{churchUnsolvableProblemElementary1936,
  title = {An {{Unsolvable Problem}} of {{Elementary Number Theory}}},
  author = {Church, Alonzo},
  date = {1936},
  journaltitle = {American Journal of Mathematics},
  volume = {58},
  number = {2},
  eprint = {2371045},
  eprinttype = {jstor},
  pages = {345--363},
  publisher = {{Johns Hopkins University Press}},
  issn = {0002-9327},
  doi = {10.2307/2371045},
  url = {https://www.jstor.org/stable/2371045},
  urldate = {2023-04-29},
  keywords = {church,thesis,turing},
  file = {/Users/emilchri/Zotero/storage/XTR85P34/Church - 1936 - An Unsolvable Problem of Elementary Number Theory.pdf}
}

@inproceedings{cireganMulticolumnDeepNeural2012,
  title = {Multi-Column Deep Neural Networks for Image Classification},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ciregan, Dan and Meier, Ueli and Schmidhuber, Jürgen},
  date = {2012-06},
  pages = {3642--3649},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2012.6248110},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  eventtitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Benchmark testing,cnn,Computer architecture,Error analysis,Graphics processing unit,human,Neurons,Training},
  file = {/Users/emilchri/Zotero/storage/BRIR8FWP/Ciregan et al. - 2012 - Multi-column deep neural networks for image classi.pdf;/Users/emilchri/Zotero/storage/G9CKINCB/6248110.html}
}

@online{ConfusionMatrix,
  title = {Confusion Matrix},
  url = {https://scikit-learn/stable/auto_examples/model_selection/plot_confusion_matrix.html},
  urldate = {2023-04-16},
  abstract = {Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is e...},
  langid = {english},
  organization = {{scikit-learn}},
  keywords = {confusion,matrix},
  file = {/Users/emilchri/Zotero/storage/R8WEHA4G/plot_confusion_matrix.html}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  date = {1997-02-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0933365796003673},
  urldate = {2022-10-25},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  langid = {english},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia},
  file = {/Users/emilchri/Zotero/storage/KASBKT55/Cooper et al. - 1997 - An evaluation of machine-learning methods for pred.pdf;/Users/emilchri/Zotero/storage/X9ISILDH/S0933365796003673.html}
}

@article{cooperPredictingDireOutcomes2005,
  title = {Predicting Dire Outcomes of Patients with Community Acquired Pneumonia},
  author = {Cooper, Gregory F. and Abraham, Vijoy and Aliferis, Constantin F. and Aronis, John M. and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Janosky, Janine E. and Livingston, Gary and Mitchell, Tom and Monti, Stefano and Spirtes, Peter},
  date = {2005-10-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  series = {Clinical {{Machine Learning}}},
  volume = {38},
  number = {5},
  pages = {347--366},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2005.02.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046405000225},
  urldate = {2022-10-25},
  abstract = {Community-acquired pneumonia (CAP) is an important clinical condition with regard to patient mortality, patient morbidity, and healthcare resource utilization. The assessment of the likely clinical course of a CAP patient can significantly influence decision making about whether to treat the patient as an inpatient or as an outpatient. That decision can in turn influence resource utilization, as well as patient well being. Predicting dire outcomes, such as mortality or severe clinical complications, is a particularly important component in assessing the clinical course of patients. We used a training set of 1601 CAP patient cases to construct 11 statistical and machine-learning models that predict dire outcomes. We evaluated the resulting models on 686 additional CAP-patient cases. The primary goal was not to compare these learning algorithms as a study end point; rather, it was to develop the best model possible to predict dire outcomes. A special version of an artificial neural network (NN) model predicted dire outcomes the best. Using the 686 test cases, we estimated the expected healthcare quality and cost impact of applying the NN model in practice. The particular, quantitative results of this analysis are based on a number of assumptions that we make explicit; they will require further study and validation. Nonetheless, the general implication of the analysis seems robust, namely, that even small improvements in predictive performance for prevalent and costly diseases, such as CAP, are likely to result in significant improvements in the quality and efficiency of healthcare delivery. Therefore, seeking models with the highest possible level of predictive performance is important. Consequently, seeking ever better machine-learning and statistical modeling methods is of great practical significance.},
  langid = {english},
  keywords = {Community acquired pneumonia,Machine learning,Outcome prediction,Quality and cost of healthcare delivery},
  file = {/Users/emilchri/Zotero/storage/Z7J5ST3J/Cooper et al. - 2005 - Predicting dire outcomes of patients with communit.pdf;/Users/emilchri/Zotero/storage/CVPYI8RI/S1532046405000225.html}
}

@online{cooperSoftwareFrameworkRequirements2017,
  title = {Software {{Framework Requirements For Embedded Vision}}},
  author = {Cooper, Gordon},
  date = {2017-11-09T08:04:14+00:00},
  url = {https://semiengineering.com/software-framework-requirements-for-embedded-vision/},
  urldate = {2023-03-10},
  abstract = {Key factors to consider when choosing an embedded vision system.},
  langid = {american},
  organization = {{Semiconductor Engineering}},
  keywords = {graph,imagenet},
  file = {/Users/emilchri/Zotero/storage/LEQNUCUB/software-framework-requirements-for-embedded-vision.html}
}

@article{corniaExplainingTransformerbasedImage2022,
  title = {Explaining Transformer-Based Image Captioning Models: {{An}} Empirical Analysis},
  shorttitle = {Explaining Transformer-Based Image Captioning Models},
  author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  date = {2022-01-01},
  journaltitle = {AI Communications},
  volume = {35},
  number = {2},
  pages = {111--129},
  publisher = {{IOS Press}},
  issn = {0921-7126},
  doi = {10.3233/AIC-210172},
  url = {https://content.iospress.com/articles/ai-communications/aic210172},
  urldate = {2022-11-02},
  abstract = {Image Captioning is the task of translating an input image into a textual description. As such, it connects Vision and Language in a generative fashion, with applications that range from multi-modal search engines to help visually impaired people. Al},
  langid = {english},
  keywords = {transformer},
  file = {/Users/emilchri/Zotero/storage/HKMZBIEG/Cornia et al. - 2022 - Explaining transformer-based image captioning mode.pdf;/Users/emilchri/Zotero/storage/BJRNRVSJ/aic210172.html}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  date = {1967-01},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  issn = {1557-9654},
  doi = {10.1109/TIT.1967.1053964},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR\^\textbackslash ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR\^\textbackslash ast \textbackslash leq R \textbackslash leq R\^\textbackslash ast(2 –MR\^\textbackslash ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {knn,original},
  file = {/Users/emilchri/Zotero/storage/BQVJKD87/Cover og Hart - 1967 - Nearest neighbor pattern classification.pdf;/Users/emilchri/Zotero/storage/RY6RMCV4/Cover og Hart - 1967 - Nearest neighbor pattern classification.pdf;/Users/emilchri/Zotero/storage/IDEVUBG7/1053964.html}
}

@online{dasOpportunitiesChallengesExplainable2020,
  title = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}}): {{A Survey}}},
  shorttitle = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Das, Arun and Rad, Paul},
  date = {2020-06-22},
  eprint = {2006.11371},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.11371},
  urldate = {2022-11-25},
  abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/ITQBVGJG/Das og Rad - 2020 - Opportunities and Challenges in Explainable Artifi.pdf;/Users/emilchri/Zotero/storage/5ELL3ACG/2006.html}
}

@article{dengImageNetLargeScaleHierarchical2009,
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009},
  journaltitle = {CVPR09},
  pages = {8},
  doi = {10.1109/CVPR.2009.5206848},
  url = {http://www.image-net.org/papers/imagenet_cvpr09.bib},
  urldate = {2022-10-03},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/5SYC4I8E/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf}
}

@online{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-03-16},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {preprint},
  keywords = {bert,Computer Science - Computation and Language,language model,transformer},
  file = {/Users/emilchri/Zotero/storage/4H4LRIKI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/emilchri/Zotero/storage/CIGXCH2K/1810.html}
}

@article{diceMeasuresAmountEcologic1945,
  title = {Measures of the {{Amount}} of {{Ecologic Association Between Species}}},
  author = {Dice, Lee R.},
  date = {1945},
  journaltitle = {Ecology},
  volume = {26},
  number = {3},
  eprint = {1932409},
  eprinttype = {jstor},
  pages = {297--302},
  publisher = {{Ecological Society of America}},
  issn = {0012-9658},
  doi = {10.2307/1932409},
  url = {https://www.jstor.org/stable/1932409},
  urldate = {2023-04-03},
  keywords = {dice score},
  file = {/Users/emilchri/Zotero/storage/4422NCRI/Dice - 1945 - Measures of the Amount of Ecologic Association Bet.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2023-03-16},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,image,transformer},
  file = {/Users/emilchri/Zotero/storage/RANZLKDX/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/emilchri/Zotero/storage/YQBS8RLA/2010.html}
}

@online{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  date = {2023-03-06},
  eprint = {2303.03378},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.03378},
  url = {http://arxiv.org/abs/2303.03378},
  urldate = {2023-04-09},
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,PaLM-E},
  file = {/Users/emilchri/Zotero/storage/D93WWTZS/Driess et al. - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf;/Users/emilchri/Zotero/storage/RYT9CYLK/2303.html}
}

@article{dubeyActivationFunctionsDeep2022,
  title = {Activation Functions in Deep Learning: {{A}} Comprehensive Survey and Benchmark},
  shorttitle = {Activation Functions in Deep Learning},
  author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  date = {2022-09-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {503},
  pages = {92--108},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.06.111},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222008426},
  urldate = {2023-05-05},
  abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.},
  langid = {english},
  keywords = {activation,Activation Functions,Convolutional neural networks,Deep learning,function,Neural networks,Overview,Recurrent Neural Networks,relu,sigmoid},
  file = {/Users/emilchri/Zotero/storage/276LBKFA/Dubey et al. - 2022 - Activation functions in deep learning A comprehen.pdf;/Users/emilchri/Zotero/storage/Z54FM7YT/S0925231222008426.html}
}

@online{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  date = {2018-01-11},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1603.07285},
  url = {http://arxiv.org/abs/1603.07285},
  urldate = {2023-05-01},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  pubstate = {preprint},
  keywords = {cnn,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,figure,graphic,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/22STNZP5/Dumoulin og Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf;/Users/emilchri/Zotero/storage/CNENZHK2/1603.html}
}

@article{dzindoletRoleTrustAutomation2003,
  title = {The Role of Trust in Automation Reliance},
  author = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
  date = {2003-06-01},
  journaltitle = {International Journal of Human-Computer Studies},
  shortjournal = {International Journal of Human-Computer Studies},
  series = {Trust and {{Technology}}},
  volume = {58},
  number = {6},
  pages = {697--718},
  issn = {1071-5819},
  doi = {10.1016/S1071-5819(03)00038-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
  urldate = {2022-10-26},
  abstract = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
  langid = {english},
  keywords = {Automation reliance,Automation trust,Disuse,Misuse},
  file = {/Users/emilchri/Zotero/storage/JXKR5HJF/Dzindolet et al. - 2003 - The role of trust in automation reliance.pdf;/Users/emilchri/Zotero/storage/27YLM5KL/S1071581903000387.html}
}

@inproceedings{fagerengPolypConnectImageInpainting2022,
  title = {{{PolypConnect}}: {{Image}} Inpainting for Generating Realistic Gastrointestinal Tract Images with Polyps},
  shorttitle = {{{PolypConnect}}},
  booktitle = {2022 {{IEEE}} 35th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Fagereng, Jan Andre and Thambawita, Vajira and Storås, Andrea M. and Parasa, Sravanthi and family=Lange, given=Thomas, prefix=de, useprefix=true and Halvorsen, Pål and Riegler, Michael A.},
  date = {2022-07},
  pages = {66--71},
  issn = {2372-9198},
  doi = {10.1109/CBMS55023.2022.00019},
  abstract = {Early identification of a polyp in the lower gas-trointestinal (GI) tract can lead to prevention of life-threatening colorectal cancer. Developing computer-aided diagnosis (CAD) systems to detect polyps can improve detection accuracy and efficiency and save the time of the domain experts called endoscopists. Lack of annotated data is a common challenge when building CAD systems. Generating synthetic medical data is an active research area to overcome the problem of having relatively few true positive cases in the medical domain. To be able to efficiently train machine learning (ML) models, which are the core of CAD systems, a considerable amount of data should be used. In this respect, we propose the PolypConnect pipeline, which can convert non-polyp images into polyp images to increase the size of training datasets for training. We present the whole pipeline with quantitative and qualitative evaluations involving endoscopists. The polyp segmentation model trained using synthetic data, and real data shows a 5.1\% improvement of mean intersection over union (mIOU), compared to the model trained only using real data. The codes of all the experiments are available on GitHub to reproduce the results.},
  eventtitle = {2022 {{IEEE}} 35th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  keywords = {Andrea,Computational modeling,Data models,fake polyp data,generative models,Image segmentation,Machine learning,medical,Pipelines,polyp,polyp inpainting,Solid modeling,synthetic medical data,synthetic polyps,Training},
  file = {/Users/emilchri/Zotero/storage/SD99F3LP/Fagereng et al. - 2022 - PolypConnect Image inpainting for generating real.pdf;/Users/emilchri/Zotero/storage/VYVHZ5RH/9866977.html}
}

@article{fixDiscriminatoryAnalysisNonparametric1989,
  title = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}: {{Consistency Properties}}},
  shorttitle = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}},
  author = {Fix, Evelyn and Hodges, J. L.},
  date = {1989},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  volume = {57},
  number = {3},
  eprint = {1403797},
  eprinttype = {jstor},
  pages = {238--247},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403797},
  url = {https://www.jstor.org/stable/1403797},
  urldate = {2022-10-28},
  keywords = {knn,orginal},
  file = {/Users/emilchri/Zotero/storage/KFESUNQ8/Fix og Hodges - 1989 - Discriminatory Analysis. Nonparametric Discriminat.pdf;/Users/emilchri/Zotero/storage/MV84YK7X/original_knn_paper.pdf}
}

@online{floresVariationalAutoencodersAre2019,
  title = {Variational {{Autoencoders}} Are {{Beautiful}} | {{Blogs}}},
  author = {Flores, Steven},
  date = {2019-04-15},
  url = {https://www.compthree.com/blog/autoencoder/},
  urldate = {2023-04-30},
  keywords = {auto,decoder,encoder},
  file = {/Users/emilchri/Zotero/storage/ZR5CYPWH/autoencoder.html}
}

@article{floridiAIItsNew2020,
  title = {{{AI}} and {{Its New Winter}}: From {{Myths}} to {{Realities}}},
  shorttitle = {{{AI}} and {{Its New Winter}}},
  author = {Floridi, Luciano},
  date = {2020-03-01},
  journaltitle = {Philosophy \& Technology},
  shortjournal = {Philos. Technol.},
  volume = {33},
  number = {1},
  pages = {1--3},
  issn = {2210-5441},
  doi = {10.1007/s13347-020-00396-6},
  url = {https://doi.org/10.1007/s13347-020-00396-6},
  urldate = {2023-05-05},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/UHENTABY/Floridi - 2020 - AI and Its New Winter from Myths to Realities.pdf}
}

@online{fukuiMultimodalCompactBilinear2016,
  title = {Multimodal {{Compact Bilinear Pooling}} for {{Visual Question Answering}} and {{Visual Grounding}}},
  author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  date = {2016-09-23},
  eprint = {1606.01847},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.01847},
  url = {http://arxiv.org/abs/1606.01847},
  urldate = {2022-12-07},
  abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/7E2LV5KJ/Fukui et al. - 2016 - Multimodal Compact Bilinear Pooling for Visual Que.pdf;/Users/emilchri/Zotero/storage/VMKSFR2Y/1606.html}
}

@article{fukushimaCognitronSelforganizingMultilayered1975,
  title = {Cognitron: {{A}} Self-Organizing Multilayered Neural Network},
  shorttitle = {Cognitron},
  author = {Fukushima, Kunihiko},
  date = {1975-09-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {20},
  number = {3},
  pages = {121--136},
  issn = {1432-0770},
  doi = {10.1007/BF00342633},
  url = {https://doi.org/10.1007/BF00342633},
  urldate = {2022-10-04},
  abstract = {A new hypothesis for the organization of synapses between neurons is proposed: “The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y”. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named “cognitron”, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a “teacher” which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
  langid = {english},
  keywords = {Deep Layer,Final Layer,Individual Cell,Neural Network,Receptive Field,ReLu},
  file = {/Users/emilchri/Zotero/storage/3HR5Y5HL/Fukushima - 1975 - Cognitron A self-organizing multilayered neural n.pdf}
}

@online{gaoCompactBilinearPooling2016,
  title = {Compact {{Bilinear Pooling}}},
  author = {Gao, Yang and Beijbom, Oscar and Zhang, Ning and Darrell, Trevor},
  date = {2016-04-11},
  eprint = {1511.06062},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06062},
  url = {http://arxiv.org/abs/1511.06062},
  urldate = {2022-11-28},
  abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/RUXE7X6E/Gao et al. - 2016 - Compact Bilinear Pooling.pdf;/Users/emilchri/Zotero/storage/NXSGRUNS/1511.html}
}

@inproceedings{garciaHarmsDemographicBias2019,
  title = {The {{Harms}} of {{Demographic Bias}} in {{Deep Face Recognition Research}}},
  booktitle = {2019 {{International Conference}} on {{Biometrics}} ({{ICB}})},
  author = {Garcia, Raul Vicente and Wandzik, Lukasz and Grabner, Louisa and Krueger, Joerg},
  date = {2019-06},
  pages = {1--6},
  issn = {2376-4201},
  doi = {10.1109/ICB45273.2019.8987334},
  abstract = {In this work we demonstrate the existence of demographic bias in the face representations of currently popular deep-learning-based face recognition models, exposing a bad research and development practice that may lead to a systematic discrimination of certain demographic groups in critical scenarios like automated border control. Furthermore, through the simulation of the template morphing attack, we reveal significant security risks that derive from demographic bias in current deep face models. This widely ignored problem poses important questions on fairness and accountability in face recognition.},
  eventtitle = {2019 {{International Conference}} on {{Biometrics}} ({{ICB}})},
  file = {/Users/emilchri/Zotero/storage/L63CNUQD/Garcia et al. - 2019 - The Harms of Demographic Bias in Deep Face Recogni.pdf;/Users/emilchri/Zotero/storage/RRDZD73P/8987334.html}
}

@article{garneloReconcilingDeepLearning2019,
  title = {Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations},
  shorttitle = {Reconciling Deep Learning with Symbolic Artificial Intelligence},
  author = {Garnelo, Marta and Shanahan, Murray},
  date = {2019-10-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Artificial {{Intelligence}}},
  volume = {29},
  pages = {17--23},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2018.12.010},
  url = {https://www.sciencedirect.com/science/article/pii/S2352154618301943},
  urldate = {2023-04-29},
  abstract = {In the history of the quest for human-level artificial intelligence, a number of rival paradigms have vied for supremacy. Symbolic artificial intelligence was dominant for much of the 20th century, but currently a connectionist paradigm is in the ascendant, namely machine learning with deep neural networks. However, both paradigms have strengths and weaknesses, and a significant challenge for the field today is to effect a reconciliation. A central tenet of the symbolic paradigm is that intelligence results from the manipulation of abstract compositional representations whose elements stand for objects and relations. If this is correct, then a key objective for deep learning is to develop architectures capable of discovering objects and relations in raw data, and learning how to represent them in ways that are useful for downstream processing. This short review highlights recent progress in this direction.},
  langid = {english},
  keywords = {ai,symbolic},
  file = {/Users/emilchri/Zotero/storage/SR7JRKHE/Garnelo og Shanahan - 2019 - Reconciling deep learning with symbolic artificial.pdf;/Users/emilchri/Zotero/storage/JQTLYXAM/S2352154618301943.html}
}

@inproceedings{ghorbaniAutomaticConceptbasedExplanations2019,
  title = {Towards {{Automatic Concept-based Explanations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
  urldate = {2022-10-05},
  abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.      Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \textbackslash emph\{concept\} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \textbackslash alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
  file = {/Users/emilchri/Zotero/storage/U8DUKKE3/Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf}
}

@inproceedings{girshickRichFeatureHierarchies2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014},
  pages = {580--587},
  url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {R-CNN},
  file = {/Users/emilchri/Zotero/storage/CHIZRS2J/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf;/Users/emilchri/Zotero/storage/H8DCR597/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html}
}

@inproceedings{gjestangSelflearningTeacherstudentFramework2021,
  title = {A Self-Learning Teacher-Student Framework for Gastrointestinal Image Classification},
  booktitle = {2021 {{IEEE}} 34th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Gjestang, Henrik L. and Hicks, Steven A. and Thambawita, Vajira and Halvorsen, Pål and Riegler, Michael A.},
  date = {2021-06},
  pages = {539--544},
  issn = {2372-9198},
  doi = {10.1109/CBMS52027.2021.00087},
  abstract = {We present a semi-supervised teacher-student framework to improve classification performance on gastrointestinal image data. As labeled data is scarce in medical settings, this framework is built specifically to take advantage of vast amounts of unlabeled data. It consists of three main steps: (1) train a teacher model with labeled data, (2) use the teacher model to infer pseudo labels with unlabeled data, and (3) train a new and larger student model with a combination of labeled images and inferred pseudo labels. These three steps are repeated several times by treating the student as a teacher to relabel the unlabeled data and consequently train a new student. We demonstrate that our framework can classify both video capsule endoscopy (VCE) and standard endoscopy images. Our results indicate that our teacher-student framework can significantly increase the performance compared to traditional supervised-learning-based models, i.e., an overall increase in the F1-score of 4.7\% for the Kvasir-Capsule VCE dataset and 3.2\% for the HyperKvasir colonoscopy dataset. We believe that our framework can use more of the data collected at hospitals without the need for expert labels, contributing to overall better models for medical multimedia systems for automatic disease detection.},
  eventtitle = {2021 {{IEEE}} 34th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  keywords = {capsule endoscopy,colonoscopy,Colonoscopy,computer vision,Data models,dataset,deep learning,Endoscopes,Gastrointestinal tract,generation,Hospitals,machine learning,Medical diagnostic imaging,Michael,Multimedia systems,self-training,Teacher-student framework},
  file = {/Users/emilchri/Zotero/storage/7XR4IAAP/Gjestang et al. - 2021 - A self-learning teacher-student framework for gast.pdf;/Users/emilchri/Zotero/storage/8RYMCHJX/9474707.html}
}

@article{gliksonHumanTrustArtificial2020,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  author = {Glikson, Ella and Woolley, Anita Williams},
  date = {2020},
  journaltitle = {Academy of Management Annals},
  volume = {14},
  number = {2},
  pages = {627--660},
  doi = {10.5465/annals.2018.0057},
  url = {https://journals.aom.org/doi/epdf/10.5465/annals.2018.0057},
  urldate = {2022-10-26},
  abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/P42NE95U/Human Trust in Artificial Intelligence Review of .pdf;/Users/emilchri/Zotero/storage/DEDCSYMY/annals.2018.html}
}

@online{goodmanEuropeanUnionRegulations2016,
  title = {European {{Union}} Regulations on Algorithmic Decision-Making and a "Right to Explanation"},
  author = {Goodman, Bryce and Flaxman, Seth},
  date = {2016-08-31},
  eprint = {1606.08813},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.1609/aimag.v38i3.2741},
  url = {http://arxiv.org/abs/1606.08813},
  urldate = {2022-10-24},
  abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also effectively create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,eu,Statistics - Machine Learning,xai},
  file = {/Users/emilchri/Zotero/storage/2XDGW9PR/Goodman og Flaxman - 2016 - European Union regulations on algorithmic decision.pdf;/Users/emilchri/Zotero/storage/FH9JB9KG/1606.html}
}

@online{goyalMakingVQAMatter2017,
  title = {Making the {{V}} in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  shorttitle = {Making the {{V}} in {{VQA Matter}}},
  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  date = {2017-05-15},
  eprint = {1612.00837},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.00837},
  urldate = {2022-10-03},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/S556XBUF/Goyal et al. - 2017 - Making the V in VQA Matter Elevating the Role of .pdf;/Users/emilchri/Zotero/storage/QDYSAJNY/1612.html}
}

@article{guptaDeepLearningObject2021,
  title = {Deep Learning for Object Detection and Scene Perception in Self-Driving Cars: {{Survey}}, Challenges, and Open Issues},
  shorttitle = {Deep Learning for Object Detection and Scene Perception in Self-Driving Cars},
  author = {Gupta, Abhishek and Anpalagan, Alagan and Guan, Ling and Khwaja, Ahmed Shaharyar},
  date = {2021-07-01},
  journaltitle = {Array},
  shortjournal = {Array},
  volume = {10},
  pages = {100057},
  issn = {2590-0056},
  doi = {10.1016/j.array.2021.100057},
  url = {https://www.sciencedirect.com/science/article/pii/S2590005621000059},
  urldate = {2022-10-31},
  abstract = {This article presents a comprehensive survey of deep learning applications for object detection and scene perception in autonomous vehicles. Unlike existing review papers, we examine the theory underlying self-driving vehicles from deep learning perspective and current implementations, followed by their critical evaluations. Deep learning is one potential solution for object detection and scene perception problems, which can enable algorithm-driven and data-driven cars. In this article, we aim to bridge the gap between deep learning and self-driving cars through a comprehensive survey. We begin with an introduction to self-driving cars, deep learning, and computer vision followed by an overview of artificial general intelligence. Then, we classify existing powerful deep learning libraries and their role and significance in the growth of deep learning. Finally, we discuss several techniques that address the image perception issues in real-time driving, and critically evaluate recent implementations and tests conducted on self-driving cars. The findings and practices at various stages are summarized to correlate prevalent and futuristic techniques, and the applicability, scalability and feasibility of deep learning to self-driving cars for achieving safe driving without human intervention. Based on the current survey, several recommendations for further research are discussed at the end of this article.},
  langid = {english},
  keywords = {Autonomous driving initiatives,Computer vision,Convolutional neural networks,Deep learning,Levels of automation,LiDAR,Machine learning,Multimodal sensor fusion,Object detection,Scene perception,Self-driving cars},
  file = {/Users/emilchri/Zotero/storage/EGD7E4RF/Gupta et al. - 2021 - Deep learning for object detection and scene perce.pdf;/Users/emilchri/Zotero/storage/9ZL46JE2/S2590005621000059.html}
}

@article{haspielExplanationsExpectations2018,
  title = {Explanations and {{Expectations}}},
  author = {Haspiel, Jacob and Du, Na and Meyerson, Jill and Robert Jr., Lionel P. and Tilbury, Dawn and Yang, X. Jessie and Pradhan, Anuj K.},
  date = {2018},
  pages = {119--120},
  doi = {10.1145/3173386.3177057},
  keywords = {background,motivation,xai},
  file = {/Users/emilchri/Zotero/storage/V5GTGFM5/Peeking_Inside_the_Black-Box_A_Survey_on_Explainable_Artificial_Intelligence_XAI.pdf}
}

@book{haugelandArtificialIntelligenceVery1989,
  title = {Artificial {{Intelligence}}: {{The Very Idea}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Haugeland, John},
  date = {1989-01-06},
  eprint = {AL1NEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{MIT Press}},
  abstract = {"Machines who think—how utterly preposterous," huff beleaguered humanists, defending their dwindling turf. "Artificial Intelligence—it's here and about to surpass our own," crow techno-visionaries, proclaiming dominion. It's so simple and obvious, each side maintains, only a fanatic could disagree.Deciding where the truth lies between these two extremes is the main purpose of John Haugeland's marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in non-technical terms, it neither oversimplifies the science nor evades the fundamental philosophical issues. Far from ducking the really hard questions, it takes them on, one by one.Artificial intelligence, Haugeland notes, is based on a very good idea, which might well be right, and just as well might not. That idea, the idea that human thinking and machine computing are "radically the same," provides the central theme for his illuminating and provocative book about this exciting new field. After a brief but revealing digression in intellectual history, Haugeland systematically tackles such basic questions as: What is a computer really? How can a physical object "mean" anything? What are the options for computational organization? and What structures have been proposed and tried as actual scientific models for intelligence?In a concluding chapter he takes up several outstanding problems and puzzles—including intelligence in action, imagery, feelings and personality—and their enigmatic prospects for solution.},
  isbn = {978-0-262-58095-3},
  langid = {english},
  pagetotal = {303},
  keywords = {Good-old-fashioned-ai,Psychology / Cognitive Psychology \& Cognition}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2023-02-06},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,resnet},
  file = {/Users/emilchri/Zotero/storage/YN6YYXT9/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/emilchri/Zotero/storage/GCTP54SY/1512.html}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  pages = {1026--1034},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html},
  urldate = {2023-03-13},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {deep learning,human,imagenet,level,surpassed},
  file = {/Users/emilchri/Zotero/storage/EC8AB3NW/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf}
}

@incollection{heImageCaptioningImage2021,
  title = {Image {{Captioning Through Image Transformer}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2020},
  author = {He, Sen and Liao, Wentong and Tavakoli, Hamed R. and Yang, Michael and Rosenhahn, Bodo and Pugeault, Nicolas},
  editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
  date = {2021},
  volume = {12625},
  pages = {153--169},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-69538-5_10},
  url = {http://link.springer.com/10.1007/978-3-030-69538-5_10},
  urldate = {2022-11-06},
  abstract = {Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect of captioning is the notion of attention: how to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous works have proposed the transformer architecture for image captioning. However, the structure between the semantic units in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt the transformer’s internal architecture to images. In this work, we introduce the image transformer , which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widens the original transformer layer’s inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks. The code is available at https://github.com/wtliao/ImageTransformer.},
  isbn = {978-3-030-69537-8 978-3-030-69538-5},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/8D5A7Q8N/He et al. - 2021 - Image Captioning Through Image Transformer.pdf}
}

@inproceedings{heRethinkingImageNetPreTraining2019,
  title = {Rethinking {{ImageNet Pre-Training}}},
  author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
  date = {2019},
  pages = {4918--4927},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html},
  urldate = {2023-02-27},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  file = {/Users/emilchri/Zotero/storage/YKAAZLND/He et al. - 2019 - Rethinking ImageNet Pre-Training.pdf}
}

@article{herlockerExplainingCollaborativeFiltering2000,
  title = {Explaining Collaborative Filtering Recommendations | {{Proceedings}} of the 2000 {{ACM}} Conference on {{Computer}} Supported Cooperative Work},
  author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Riedl, John},
  date = {2000-12-02},
  journaltitle = {CSCW '00: Proceedings of the 2000 ACM conference on Computer supported cooperative work},
  pages = {241--250},
  doi = {10.1145/358916.358995},
  url = {https://dl.acm.org/doi/abs/10.1145/358916.358995},
  urldate = {2022-10-26},
  file = {/Users/emilchri/Zotero/storage/8VZVYHIN/Explaining collaborative filtering recommendations.pdf;/Users/emilchri/Zotero/storage/Z4NMN3XK/358916.html}
}

@software{hicksImageCLEFmedMEDVQAGI20232023,
  title = {{{ImageCLEFmed-MEDVQA-GI-2023}}},
  author = {Hicks, Steven and Michael, Riegler and Vajira, Thambawita and Storås, Andrea M. and Halvorsen, Pål and family=Lange, given=Thomas, prefix=de, useprefix=false and Papachrysos, Nikolaos and Johanna, Schöler and Jha, Debesh},
  date = {2023-05-08T12:20:33Z},
  origdate = {2023-02-12T12:22:41Z},
  url = {https://github.com/simula/ImageCLEFmed-MEDVQA-GI-2023},
  urldate = {2023-05-12},
  organization = {{Simula}}
}

@online{hicksImageCLEFmedMEDVQAGIImageCLEF,
  title = {{{ImageCLEFmed MEDVQA-GI}} - {{ImageCLEF}} / {{LifeCLEF}} - {{Multimedia Retrieval}} in {{CLEF}}},
  author = {Hicks, Steven and Riegler, Michael A. and Vajira, Thambawita and Storås, Andrea M. and Halvorsen, Pål and family=Lange, given=Thomas, prefix=de, useprefix=false and Papachrysos, Nikolaos and Johanna, Schöler and Jha, Debesh},
  url = {https://www.imageclef.org/2023/medical/vqa},
  urldate = {2023-05-12},
  file = {/Users/emilchri/Zotero/storage/M67PPPWI/vqa.html}
}

@online{hintonForwardForwardAlgorithmPreliminary2022,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  shorttitle = {The {{Forward-Forward Algorithm}}},
  author = {Hinton, Geoffrey},
  date = {2022-12-26},
  eprint = {2212.13345},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.13345},
  url = {http://arxiv.org/abs/2212.13345},
  urldate = {2023-03-13},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/F584UTN8/Hinton - 2022 - The Forward-Forward Algorithm Some Preliminary In.pdf;/Users/emilchri/Zotero/storage/KDBNVAU6/2212.html}
}

@inproceedings{hirotaGenderRacialBias2022,
  title = {Gender and {{Racial Bias}} in {{Visual Question Answering Datasets}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Hirota, Yusuke and Nakashima, Yuta and Garcia, Noa},
  date = {2022-06-21},
  eprint = {2205.08148},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1280--1292},
  doi = {10.1145/3531146.3533184},
  url = {http://arxiv.org/abs/2205.08148},
  urldate = {2023-01-17},
  abstract = {Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society},
  file = {/Users/emilchri/Zotero/storage/8U6CZWM6/Hirota et al. - 2022 - Gender and Racial Bias in Visual Question Answerin.pdf;/Users/emilchri/Zotero/storage/QCC6S3HN/2205.html}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long {{Short-term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-12-01},
  journaltitle = {Neural computation},
  shortjournal = {Neural computation},
  volume = {9},
  pages = {1735--80},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  keywords = {lstm,LSTM},
  file = {/Users/emilchri/Zotero/storage/3XYY2QCB/Hochreiter og Schmidhuber - 1997 - Long Short-term Memory.pdf;/Users/emilchri/Zotero/storage/XSLJ8PY4/authors.html}
}

@inproceedings{hoffmannKnowledgeBasedWeakSupervision2011,
  title = {Knowledge-{{Based Weak Supervision}} for {{Information Extraction}} of {{Overlapping Relations}}},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Hoffmann, Raphael and Zhang, Congle and Ling, Xiao and Zettlemoyer, Luke and Weld, Daniel S.},
  date = {2011-06},
  pages = {541--550},
  publisher = {{Association for Computational Linguistics}},
  location = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1055},
  urldate = {2023-05-05},
  eventtitle = {{{ACL-HLT}} 2011},
  file = {/Users/emilchri/Zotero/storage/JEWYP6HD/Hoffmann et al. - 2011 - Knowledge-Based Weak Supervision for Information E.pdf}
}

@online{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and family=Driessche, given=George, prefix=van den, useprefix=false and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  date = {2022-03-29},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-05-03},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  pubstate = {preprint},
  keywords = {Chinchilla,Computer Science - Computation and Language,Computer Science - Machine Learning,scaling},
  file = {/Users/emilchri/Zotero/storage/ZDQM6RI4/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/Users/emilchri/Zotero/storage/GXLL2CGV/2203.html}
}

@article{holzingerCausabilityExplainabilityArtificial2019,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
  date = {2019},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4795},
  doi = {10.1002/widm.1312},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312},
  urldate = {2022-10-31},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine},
  file = {/Users/emilchri/Zotero/storage/IRCSK4SH/Holzinger et al. - 2019 - Causability and explainability of artificial intel.pdf;/Users/emilchri/Zotero/storage/YPI8BVNY/widm.html}
}

@online{holzingerWhatWeNeed2017,
  title = {What Do We Need to Build Explainable {{AI}} Systems for the Medical Domain?},
  author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
  date = {2017-12-28},
  eprint = {1712.09923},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1712.09923},
  url = {http://arxiv.org/abs/1712.09923},
  urldate = {2022-10-28},
  abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/MJ2VZDD4/Holzinger et al. - 2017 - What do we need to build explainable AI systems fo.pdf;/Users/emilchri/Zotero/storage/KJDK9SCU/1712.html}
}

@article{hoyAlexaSiriCortana2018,
  title = {Alexa, {{Siri}}, {{Cortana}}, and {{More}}: {{An Introduction}} to {{Voice Assistants}}},
  shorttitle = {Alexa, {{Siri}}, {{Cortana}}, and {{More}}},
  author = {Hoy, Matthew B.},
  date = {2018-01-02},
  journaltitle = {Medical Reference Services Quarterly},
  volume = {37},
  number = {1},
  eprint = {29327988},
  eprinttype = {pmid},
  pages = {81--88},
  publisher = {{Routledge}},
  issn = {0276-3869},
  doi = {10.1080/02763869.2018.1404391},
  url = {https://doi.org/10.1080/02763869.2018.1404391},
  urldate = {2023-04-28},
  abstract = {Voice assistants are software agents that can interpret human speech and respond via synthesized voices. Apple’s Siri, Amazon’s Alexa, Microsoft’s Cortana, and Google’s Assistant are the most popular voice assistants and are embedded in smartphones or dedicated home speakers. Users can ask their assistants questions, control home automation devices and media playback via voice, and manage other basic tasks such as email, to-do lists, and calendars with verbal commands. This column will explore the basic workings and common features of today’s voice assistants. It will also discuss some of the privacy and security issues inherent to voice assistants and some potential future uses for these devices. As voice assistants become more widely used, librarians will want to be familiar with their operation and perhaps consider them as a means to deliver library services and materials.},
  keywords = {assistanst,Human computer interaction,internet,libraries,software agents,speech recognition,voice,voice assistants},
  file = {/Users/emilchri/Zotero/storage/R45TX9AE/Hoy - 2018 - Alexa, Siri, Cortana, and More An Introduction to.pdf}
}

@online{hudsonCompositionalAttentionNetworks2018,
  title = {Compositional {{Attention Networks}} for {{Machine Reasoning}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  date = {2018-04-24},
  eprint = {1803.03067},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.03067},
  url = {http://arxiv.org/abs/1803.03067},
  urldate = {2022-12-07},
  abstract = {We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9\% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/emilchri/Zotero/storage/7INVS2XJ/Hudson og Manning - 2018 - Compositional Attention Networks for Machine Reaso.pdf;/Users/emilchri/Zotero/storage/XU9QL6D8/1803.html}
}

@online{hudsonGQANewDataset2019,
  title = {{{GQA}}: {{A New Dataset}} for {{Real-World Visual Reasoning}} and {{Compositional Question Answering}}},
  shorttitle = {{{GQA}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  date = {2019-05-10},
  eprint = {1902.09506},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.09506},
  url = {http://arxiv.org/abs/1902.09506},
  urldate = {2022-12-07},
  abstract = {We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1\%, and strong VQA models achieve 54.1\%, human performance tops at 89.3\%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/EQQCUXMU/Hudson og Manning - 2019 - GQA A New Dataset for Real-World Visual Reasoning.pdf;/Users/emilchri/Zotero/storage/XSXLMAUR/1902.html}
}

@online{HuggingFaceAI,
  title = {Hugging {{Face}} – {{The AI}} Community Building the Future.},
  url = {https://huggingface.co/},
  urldate = {2023-05-11},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  keywords = {face,hugging,huggingface},
  file = {/Users/emilchri/Zotero/storage/F54X5GVD/huggingface.co.html}
}

@online{hughesMedicalTextClassification2017,
  title = {Medical {{Text Classification}} Using {{Convolutional Neural Networks}}},
  author = {Hughes, Mark and Li, Irene and Kotoulas, Spyros and Suzumura, Toyotaro},
  date = {2017-04-22},
  eprint = {1704.06841},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.06841},
  url = {http://arxiv.org/abs/1704.06841},
  urldate = {2022-10-24},
  abstract = {We present an approach to automatically classify clinical text at a sentence level. We are using deep convolutional neural networks to represent complex features. We train the network on a dataset providing a broad categorization of health information. Through a detailed evaluation, we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/emilchri/Zotero/storage/ZNPMTIEB/Hughes et al. - 2017 - Medical Text Classification using Convolutional Ne.pdf;/Users/emilchri/Zotero/storage/85JIFSTW/1704.html}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-03-28},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,LoRA,rank},
  file = {/Users/emilchri/Zotero/storage/2BXTVLDD/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/emilchri/Zotero/storage/PGP5T2P2/2106.html}
}

@online{HumanTrustArtificial,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  doi = {10.5465/annals.2018.0057},
  url = {https://journals.aom.org/doi/epdf/10.5465/annals.2018.0057},
  urldate = {2022-11-29},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/3LRCIKXH/annals.2018.html}
}

@online{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2103.03206},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2023-03-16},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,multimodal,transformer},
  file = {/Users/emilchri/Zotero/storage/UYJTBQHX/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf;/Users/emilchri/Zotero/storage/GPZ4V8A8/2103.html}
}

@online{jainAttentionNotExplanation2019,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  date = {2019-05-08},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.10186},
  url = {http://arxiv.org/abs/1902.10186},
  urldate = {2023-05-12},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  pubstate = {preprint},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,transformer},
  file = {/Users/emilchri/Zotero/storage/NBMN2W9U/Jain og Wallace - 2019 - Attention is not Explanation.pdf;/Users/emilchri/Zotero/storage/4WFLGFH2/1902.html}
}

@article{jiangArtificialIntelligenceHealthcare2017,
  title = {Artificial Intelligence in Healthcare: Past, Present and Future},
  shorttitle = {Artificial Intelligence in Healthcare},
  author = {Jiang, Fei and Jiang, Yong and Zhi, Hui and Dong, Yi and Li, Hao and Ma, Sufeng and Wang, Yilong and Dong, Qiang and Shen, Haipeng and Wang, Yongjun},
  date = {2017-12-01},
  journaltitle = {Stroke and Vascular Neurology},
  shortjournal = {Stroke Vasc Neurol},
  volume = {2},
  number = {4},
  eprint = {29507784},
  eprinttype = {pmid},
  publisher = {{BMJ Specialist Journals}},
  issn = {2059-8688, 2059-8696},
  doi = {10.1136/svn-2017-000101},
  url = {https://svn.bmj.com/content/2/4/230},
  urldate = {2022-10-31},
  abstract = {Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.},
  langid = {english},
  keywords = {big data,deep learning,neural network,stroke,support vector machine},
  file = {/Users/emilchri/Zotero/storage/9TACYDK4/Jiang et al. - 2017 - Artificial intelligence in healthcare past, prese.pdf;/Users/emilchri/Zotero/storage/5MQD69LS/230.html}
}

@inproceedings{jingAutomaticGenerationMedical2018,
  title = {On the {{Automatic Generation}} of {{Medical Imaging Reports}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jing, Baoyu and Xie, Pengtao and Xing, Eric},
  date = {2018},
  eprint = {1711.08195},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2577--2586},
  doi = {10.18653/v1/P18-1240},
  url = {http://arxiv.org/abs/1711.08195},
  urldate = {2022-10-05},
  abstract = {Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/42KDQ7FP/Jing et al. - 2018 - On the Automatic Generation of Medical Imaging Rep.pdf;/Users/emilchri/Zotero/storage/DKY86YSA/1711.html}
}

@software{jocherYOLOUltralytics2023,
  title = {{{YOLO}} by {{Ultralytics}}},
  author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  date = {2023-01},
  origdate = {2022-09-11T16:39:45Z},
  url = {https://github.com/ultralytics/ultralytics},
  urldate = {2023-04-09},
  abstract = {NEW - YOLOv8 🚀 in PyTorch {$>$} ONNX {$>$} CoreML {$>$} TFLite},
  version = {8.0.0},
  keywords = {v8,YOLO,yolo v8}
}

@software{jocherYolov5,
  title = {Yolov5},
  author = {Jocher, Glenn},
  url = {https://github.com/ultralytics/yolov5},
  urldate = {2023-04-09},
  abstract = {YOLOv5 🚀 in PyTorch {$>$} ONNX {$>$} CoreML {$>$} TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.},
  keywords = {v5,YOLO,yolo v5},
  file = {/Users/emilchri/Zotero/storage/J5YU2ELT/yolov5.html}
}

@inproceedings{johnsBecomingExpertInteractive2015,
  title = {Becoming the {{Expert}} - {{Interactive Multi-Class Machine Teaching}}},
  author = {Johns, Edward and Mac Aodha, Oisin and Brostow, Gabriel J.},
  date = {2015},
  pages = {2616--2624},
  url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Johns_Becoming_the_Expert_2015_CVPR_paper.html},
  urldate = {2022-10-25},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {expert,learning,Teach,teacher},
  file = {/Users/emilchri/Zotero/storage/SZWLLHFW/Johns et al. - 2015 - Becoming the Expert - Interactive Multi-Class Mach.pdf;/Users/emilchri/Zotero/storage/M38DB4XH/Johns_Becoming_the_Expert_2015_CVPR_paper.html}
}

@inproceedings{johnsonDenseCapFullyConvolutional2016,
  title = {{{DenseCap}}: {{Fully Convolutional Localization Networks}} for {{Dense Captioning}}},
  shorttitle = {{{DenseCap}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  date = {2016-06},
  pages = {4565--4574},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.494},
  url = {http://ieeexplore.ieee.org/document/7780863/},
  urldate = {2022-09-19},
  abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/7RMNHEXZ/Johnson et al. - 2016 - DenseCap Fully Convolutional Localization Network.pdf}
}

@article{joshiReviewExplainabilityMultimodal2021,
  title = {A {{Review}} on {{Explainability}} in {{Multimodal Deep Neural Nets}}},
  author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {59800--59821},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3070212},
  abstract = {Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Biomedical imaging,Data models,Deep learning,Deep multimodal learning,explainable AI,interpretability,Neural networks,survey,Task analysis,trends,vision and language research,Visualization,XAI},
  file = {/Users/emilchri/Zotero/storage/FPECNRA4/Joshi et al. - 2021 - A Review on Explainability in Multimodal Deep Neur.pdf;/Users/emilchri/Zotero/storage/2JKRCZZR/9391727.html}
}

@inproceedings{jungBetterExplanationsClass2021,
  title = {Towards {{Better Explanations}} of {{Class Activation Mapping}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Jung, Hyungsik and Oh, Youngrock},
  date = {2021-10},
  pages = {1316--1324},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00137},
  url = {https://ieeexplore.ieee.org/document/9710512/},
  urldate = {2022-09-19},
  abstract = {Increasing demands for understanding the internal behavior of convolutional neural networks (CNNs) have led to remarkable improvements in explanation methods. Particularly, several class activation mapping (CAM) based methods, which generate visual explanation maps by a linear combination of activation maps from CNNs, have been proposed. However, the majority of the methods lack a clear theoretical basis on how they assign the coefficients of the linear combination. In this paper, we revisit the intrinsic linearity of CAM with respect to the activation maps; we construct an explanation model of CNN as a linear function of binary variables that denote the existence of the corresponding activation maps. With this approach, the explanation model can be determined by additive feature attribution methods in an analytic manner. We then demonstrate the adequacy of SHAP values, which is a unique solution for the explanation model with a set of desirable properties, as the coefficients of CAM. Since the exact SHAP values are unattainable, we introduce an efficient approximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can estimate the SHAP values of the activation maps with high speed and accuracy. Furthermore, it greatly outperforms other previous CAM-based methods in both qualitative and quantitative aspects.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/88KQWWBC/Jung og Oh - 2021 - Towards Better Explanations of Class Activation Ma.pdf;/Users/emilchri/Zotero/storage/FTGRYLUU/Jung og Oh - 2021 - Towards Better Explanations of Class Activation Ma.pdf;/Users/emilchri/Zotero/storage/7LVS4YEV/Jung_Towards_Better_Explanations_of_Class_Activation_Mapping_ICCV_2021_paper.html}
}

@book{kahnemanThinkingFastSlow2013,
  title = {Thinking, {{Fast}} and {{Slow}}},
  author = {Kahneman, Daniel},
  date = {2013-04-02},
  edition = {1st edition},
  publisher = {{Farrar, Straus and Giroux}},
  location = {{New York}},
  abstract = {System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. The impact of overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the profound effect of cognitive biases on everything from playing the stock market to planning our next vacation―each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives―and how we can use different techniques to guard against the mental glitches that often get us into trouble.},
  isbn = {978-0-374-53355-7},
  langid = {english},
  pagetotal = {499}
}

@inproceedings{karpathyDeepVisualSemanticAlignments2015,
  title = {Deep {{Visual-Semantic Alignments}} for {{Generating Image Descriptions}}},
  author = {Karpathy, Andrej and Fei-Fei, Li},
  date = {2015-06},
  pages = {17},
  publisher = {{CVPR}},
  abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/25EKPTCR/Karpathy og Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Ima.pdf;/Users/emilchri/Zotero/storage/MLBXHUIK/Karpathy og Fei-Fei - Deep Visual-Semantic Alignments for Generating Ima.pdf;/Users/emilchri/Zotero/storage/3KYLYNIG/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html}
}

@inproceedings{kimInterpretabilityFeatureAttribution2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2018-07-03},
  pages = {2668--2677},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/kim18d.html},
  urldate = {2022-09-12},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Andrea},
  file = {/Users/emilchri/Zotero/storage/WUW96HPU/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf}
}

@online{kimViLTVisionandLanguageTransformer2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  date = {2021-06-10},
  eprint = {2102.03334},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2102.03334},
  url = {http://arxiv.org/abs/2102.03334},
  urldate = {2023-02-23},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/ESFJVFV8/Kim et al. - 2021 - ViLT Vision-and-Language Transformer Without Conv.pdf;/Users/emilchri/Zotero/storage/FV5NTD3N/2102.html}
}

@online{kirchenbauerWatermarkLargeLanguage2023,
  title = {A {{Watermark}} for {{Large Language Models}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  date = {2023-01-27},
  eprint = {2301.10226},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.10226},
  url = {http://arxiv.org/abs/2301.10226},
  urldate = {2023-04-07},
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/8976FU55/Kirchenbauer et al. - 2023 - A Watermark for Large Language Models.pdf;/Users/emilchri/Zotero/storage/HWG5HYSZ/2301.html}
}

@article{koehlerExplanationImaginationConfidence1991,
  title = {Explanation, {{Imagination}}, and {{Confidence}} in {{Judgment}}},
  author = {Koehler, Derek},
  date = {1991-12-01},
  journaltitle = {Psychological bulletin},
  shortjournal = {Psychological bulletin},
  volume = {110},
  pages = {499--519},
  doi = {10.1037//0033-2909.110.3.499},
  abstract = {This article concerns a class of experimental manipulations that require people to generate explanations or imagine scenarios. A review of studies using such manipulations indicates that people who explain or imagine a possibility then express greater confidence in the truth of that possibility. It is argued that this effect results from the approach people take in the explanation or imagination task: They temporarily assume that the hypothesis is true and assess how plausibly it can account for the relevant evidence. From this view, any task that requires that a hypothesis be treated as if it were true is sufficient to increase confidence in the truth of that hypothesis. Such tasks cause increased confidence in the hypothesis at the expense of viable alternatives because of changes in problem representation, evidence evaluation, and information search that take place when the hypothesis is temporarily treated as if it were true.},
  file = {/Users/emilchri/Zotero/storage/S9DBEK3C/Koehler - 1991 - Explanation, Imagination, and Confidence in Judgme.pdf}
}

@inproceedings{krauseHierarchicalApproachGenerating2017,
  title = {A {{Hierarchical Approach}} for {{Generating Descriptive Image Paragraphs}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},
  date = {2017-07},
  pages = {3337--3345},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.356},
  url = {http://ieeexplore.ieee.org/document/8099839/},
  urldate = {2022-10-05},
  abstract = {Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/IWII2EIA/Krause et al. - 2017 - A Hierarchical Approach for Generating Descriptive.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  url = {https://doi.org/10.1145/3065386},
  urldate = {2022-10-03},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  keywords = {AlexNet},
  file = {/Users/emilchri/Zotero/storage/GLZIQX8Y/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@video{kubrick2001SpaceOdyssey1969,
  type = {Adventure, Sci-Fi},
  entrysubtype = {film},
  title = {2001: {{A Space Odyssey}}},
  shorttitle = {2001},
  editor = {Kubrick, Stanley},
  editortype = {director},
  editora = {Kubrick, Stanley and Clarke, Arthur C. and Dullea, Keir and Lockwood, Gary and Sylvester, William},
  editoratype = {scriptwriter},
  date = {1969-02-28},
  publisher = {{Metro-Goldwyn-Mayer (MGM), Stanley Kubrick Productions}},
  abstract = {After uncovering a mysterious artifact buried beneath the Lunar surface, a spacecraft is sent to Jupiter to find its origins - a spacecraft manned by two men and the supercomputer H.A.L. 9000.},
  keywords = {computer,famous line,human versus computer,monolith,star child},
  annotation = {Translated title: 2001: En romodyssé IMDb ID: tt0062622 event-location: United Kingdom, United States}
}

@article{lancasterAutomatedLabelingHuman1997,
  title = {Automated Labeling of the Human Brain: {{A}} Preliminary Report on the Development and Evaluation of a Forward-Transform Method},
  shorttitle = {Automated Labeling of the Human Brain},
  author = {Lancaster, J.l. and Rainey, L.h. and Summerlin, J.l. and Freitas, C.s. and Fox, P.t. and Evans, A.c. and Toga, A.w. and Mazziotta, J.c.},
  date = {1997},
  journaltitle = {Human Brain Mapping},
  volume = {5},
  number = {4},
  pages = {238--242},
  issn = {1097-0193},
  doi = {10.1002/(SICI)1097-0193(1997)5:4<238::AID-HBM6>3.0.CO;2-4},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0193%281997%295%3A4%3C238%3A%3AAID-HBM6%3E3.0.CO%3B2-4},
  urldate = {2023-02-24},
  abstract = {A forward-transform method for retrieving brain labels from the 1988 Talairach Atlas using x-y-z coordinates is presented. A hierarchical volume-occupancy labeling scheme was created to simplify the organization of atlas labels using volume and subvolumetric components. Segmentation rules were developed to define boundaries that were not given explicitly in the atlas. The labeling scheme and segmentation rules guided the segmentation and labeling of 160 contiguous regions within the atlas. A unique three-dimensional (3-D) database label server called the Talairach Daemon (http://ric.uthscsa.edu/projects) was developed for serving labels keyed to the Talairach coordinate system. Given an x-y-z Talairach coordinate, a corresponding hierarchical listing of labels is returned by the server. The accuracy and precision of the forward-transform labeling method is now under evaluation. Hum. Brain Mapping 5:238–242, 1997. © 1997 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {brain,brain labeling,image,labeling,naming hierarchy,segmentation,spatial normalization,Talairach Atlas,Talairach Daemon},
  file = {/Users/emilchri/Zotero/storage/4SP65W9M/Lancaster et al. - 1997 - Automated labeling of the human brain A prelimina.pdf;/Users/emilchri/Zotero/storage/5KWR9HLJ/(SICI)1097-0193(1997)54238AID-HBM63.0.html}
}

@article{lancellottiArtificialIntelligenceTissue2021,
  title = {Artificial {{Intelligence}} \& {{Tissue Biomarkers}}: {{Advantages}}, {{Risks}} and {{Perspectives}} for {{Pathology}}},
  shorttitle = {Artificial {{Intelligence}} \& {{Tissue Biomarkers}}},
  author = {Lancellotti, Cesare and Cancian, Pierandrea and Savevski, Victor and Kotha, Soumya and Fraggetta, Filippo and Graziano, Paolo and family=Tommaso, given=Luca, prefix=di, useprefix=true},
  date = {2021-04-02},
  journaltitle = {Cells},
  shortjournal = {Cells},
  volume = {10},
  pages = {787},
  doi = {10.3390/cells10040787},
  abstract = {Tissue Biomarkers are information written in the tissue and used in Pathology to recognize specific subsets of patients with diagnostic, prognostic or predictive purposes, thus representing the key elements of Personalized Medicine. The advent of Artificial Intelligence (AI) promises to further reinforce the role of Pathology in the scenario of Personalized Medicine: AI-based devices are expected to standardize the evaluation of tissue biomarkers and also to discover novel information, which would otherwise be ignored by human review, and use them to make specific predictions. In this review we will present how AI has been used to support Tissue Biomarkers evaluation in the specific field of Pathology, give an insight to the intriguing field of AI-based biomarkers and discuss possible advantages, risk and perspectives for Pathology.},
  file = {/Users/emilchri/Zotero/storage/5H73TCXM/Lancellotti et al. - 2021 - Artificial Intelligence & Tissue Biomarkers Advan.pdf}
}

@video{langMetropolis1927a,
  type = {Drama, Sci-Fi},
  entrysubtype = {film},
  title = {Metropolis},
  editor = {Lang, Fritz},
  editortype = {director},
  editora = {family=Harbou, given=Thea, prefix=von, useprefix=false and Lang, Fritz and Helm, Brigitte and Abel, Alfred and Fröhlich, Gustav},
  editoratype = {scriptwriter},
  date = {1927-02-28},
  publisher = {{Universum Film (UFA)}},
  abstract = {In a futuristic city sharply divided between the working class and the city planners, the son of the city\&apos;s mastermind falls in love with a working-class prophet who predicts the coming of a savior to mediate their differences.},
  keywords = {art deco,class differences,dystopia,robot,silent film},
  annotation = {IMDb ID: tt0017136 event-location: Germany}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,cnn,Feature extraction,Hidden Markov models,lenet-5,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/emilchri/Zotero/storage/B3D9ZWSS/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/emilchri/Zotero/storage/RP6J9G2H/726791.html}
}

@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  date = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
  urldate = {2022-10-04},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/emilchri/Zotero/storage/8VNVN5XN/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagat.pdf}
}

@online{lecunRNNsGRUsLSTMs2020,
  title = {{{RNNs}}, {{GRUs}}, {{LSTMs}}, {{Attention}}, {{Seq2Seq}}, and {{Memory Networks}} · {{Deep Learning}}},
  author = {LeCun, Yann},
  date = {2020},
  url = {https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-2/},
  urldate = {2023-05-05},
  keywords = {attention,lecun,yann},
  file = {/Users/emilchri/Zotero/storage/UJFPBZ84/06-2.html}
}

@online{leiLessMoreClipBERT2021,
  title = {Less Is {{More}}: {{ClipBERT}} for {{Video-and-Language Learning}} via {{Sparse Sampling}}},
  shorttitle = {Less Is {{More}}},
  author = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
  date = {2021-02-11},
  eprint = {2102.06183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.06183},
  urldate = {2023-01-03},
  abstract = {The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from vision models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video features, it is often difficult (or infeasible) to plug feature extractors directly into existing approaches for easy finetuning. To provide a remedy to this dilemma, we propose a generic framework ClipBERT that enables affordable end-to-end learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each training step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that ClipBERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learning with just a few sparsely sampled clips is often more accurate than using densely extracted offline features from full-length videos, proving the proverbial less-is-more principle. Videos in the datasets are from considerably different domains and lengths, ranging from 3-second generic domain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach. Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available at https://github.com/jayleicn/ClipBERT},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/BRUSN6KM/Lei et al. - 2021 - Less is More ClipBERT for Video-and-Language Lear.pdf;/Users/emilchri/Zotero/storage/ZHH929K9/2102.html}
}

@online{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019-10-29},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.13461},
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2023-03-16},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  pubstate = {preprint},
  keywords = {bart,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,transformer},
  file = {/Users/emilchri/Zotero/storage/YX5VWG6X/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf;/Users/emilchri/Zotero/storage/QHVIM8MY/1910.html}
}

@inproceedings{liEntangledTransformerImage2019,
  title = {Entangled {{Transformer}} for {{Image Captioning}}},
  author = {Li, Guang and Zhu, Linchao and Liu, Ping and Yang, Yi},
  date = {2019},
  pages = {8928--8937},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html},
  urldate = {2023-05-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  keywords = {attention,captioning,gru,image,transformer},
  file = {/Users/emilchri/Zotero/storage/I6R6KFRG/Li et al. - 2019 - Entangled Transformer for Image Captioning.pdf}
}

@online{lievinCanLargeLanguage2023,
  title = {Can Large Language Models Reason about Medical Questions?},
  author = {Liévin, Valentin and Hother, Christoffer Egeberg and Winther, Ole},
  date = {2023-01-24},
  eprint = {2207.08143},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.08143},
  url = {http://arxiv.org/abs/2207.08143},
  urldate = {2023-04-11},
  abstract = {Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\textbackslash{} too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2\%, MedMCQA: 62.7\% and PubMedQA: 78.2\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,gpt,I.2.1,I.2.7,llm,medical,xai},
  file = {/Users/emilchri/Zotero/storage/AUWCAKBD/Liévin et al. - 2023 - Can large language models reason about medical que.pdf;/Users/emilchri/Zotero/storage/RI2YY3S5/2207.html}
}

@inproceedings{linFocalLossDense2017,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  date = {2017},
  pages = {2980--2988},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {retinanet},
  file = {/Users/emilchri/Zotero/storage/C99R735F/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf;/Users/emilchri/Zotero/storage/7T9RK5T3/Lin_Focal_Loss_for_ICCV_2017_paper.html}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-20},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2023-01-09},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {preprint},
  keywords = {COCO,Computer Science - Computer Vision and Pattern Recognition,dataset},
  file = {/Users/emilchri/Zotero/storage/VTGRAD8X/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/emilchri/Zotero/storage/HC4VE6ZV/1405.html}
}

@inproceedings{liOscarObjectSemanticsAligned2020,
  title = {Oscar: {{Object-Semantics Aligned Pre-training}} for {{Vision-Language Tasks}}},
  shorttitle = {Oscar},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {121--137},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58577-8_8},
  abstract = {Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method~Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks (The code and pre-trained models are released: https://github.com/microsoft/Oscar).},
  isbn = {978-3-030-58577-8},
  langid = {english},
  keywords = {Object semantics,Pre-training,Vision-and-language},
  file = {/Users/emilchri/Zotero/storage/UAY23QXA/Li et al. - 2020 - Oscar Object-Semantics Aligned Pre-training for V.pdf}
}

@article{liptonMachineLearningConcept,
  title = {In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  author = {Lipton, Zachary C},
  journaltitle = {machine learning},
  pages = {28},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/W27ZLC4Q/Lipton - In machine learning, the concept of interpretabili.pdf}
}

@online{liptonMythosModelInterpretability2017,
  title = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  date = {2017-03-06},
  eprint = {1606.03490},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03490},
  url = {http://arxiv.org/abs/1606.03490},
  urldate = {2022-10-24},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/3W42HMBC/Lipton - 2017 - The Mythos of Model Interpretability.pdf;/Users/emilchri/Zotero/storage/Z838CCGA/1606.html}
}

@online{liVisualBERTSimplePerformant2019,
  title = {{{VisualBERT}}: {{A Simple}} and {{Performant Baseline}} for {{Vision}} and {{Language}}},
  shorttitle = {{{VisualBERT}}},
  author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  date = {2019-08-09},
  eprint = {1908.03557},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.03557},
  url = {http://arxiv.org/abs/1908.03557},
  urldate = {2023-03-16},
  abstract = {We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.},
  pubstate = {preprint},
  keywords = {bert,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,transformer},
  file = {/Users/emilchri/Zotero/storage/TYWF6E6B/Li et al. - 2019 - VisualBERT A Simple and Performant Baseline for V.pdf;/Users/emilchri/Zotero/storage/DZDESTUJ/1908.html}
}

@online{liYOLOv6SingleStageObject2022,
  title = {{{YOLOv6}}: {{A Single-Stage Object Detection Framework}} for {{Industrial Applications}}},
  shorttitle = {{{YOLOv6}}},
  author = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and Li, Yiduo and Zhang, Bo and Liang, Yufei and Zhou, Linyuan and Xu, Xiaoming and Chu, Xiangxiang and Wei, Xiaoming and Wei, Xiaolin},
  date = {2022-09-07},
  eprint = {2209.02976},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.02976},
  url = {http://arxiv.org/abs/2209.02976},
  urldate = {2023-04-09},
  abstract = {For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application. Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9\% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5\% AP at 495 FPS, outperforming other mainstream detectors at the same scale\textasciitilde (YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3\% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5\%/52.3\%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/ZCXLZJEP/Li et al. - 2022 - YOLOv6 A Single-Stage Object Detection Framework .pdf;/Users/emilchri/Zotero/storage/MQWYPLNC/2209.html}
}

@online{LLaMA,
  title = {{{LLaMA}}},
  url = {https://huggingface.co/docs/transformers/main/model_doc/llama},
  urldate = {2023-05-11},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  keywords = {llama},
  file = {/Users/emilchri/Zotero/storage/IVFST6LK/llama.html}
}

@online{LLaMATokenizer,
  title = {{{LLaMA}}\_tokenizer},
  url = {https://huggingface.co/docs/transformers/main/model_doc/llama},
  urldate = {2023-04-10},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  keywords = {tokenizer},
  file = {/Users/emilchri/Zotero/storage/F6GSTZ9B/llama.html}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2023-05-04},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {preprint},
  keywords = {adam,adamw,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,llama,Mathematics - Optimization and Control,weight},
  file = {/Users/emilchri/Zotero/storage/5FI6DDNA/Loshchilov og Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/emilchri/Zotero/storage/HTJICQFB/1711.html}
}

@unpublished{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  date = {2017-11-24},
  eprint = {1705.07874},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.07874},
  urldate = {2022-02-02},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Michael,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/7LGZX9TJ/Lundberg og Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf;/Users/emilchri/Zotero/storage/HWDZ3SCT/1705.html}
}

@inproceedings{luNeuralBabyTalk2018,
  title = {Neural {{Baby Talk}}},
  author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  date = {2018},
  pages = {7219--7228},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Neural_Baby_Talk_CVPR_2018_paper.html},
  urldate = {2022-09-19},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/RG2P6J93/Lu et al. - 2018 - Neural Baby Talk.pdf;/Users/emilchri/Zotero/storage/EMVKK6XB/Lu_Neural_Baby_Talk_CVPR_2018_paper.html}
}

@inproceedings{malinowskiMultiWorldApproachQuestion2014,
  title = {A {{Multi-World Approach}} to {{Question Answering}} about {{Real-World Scenes}} Based on {{Uncertain Input}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Malinowski, Mateusz and Fritz, Mario},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html},
  urldate = {2023-02-24},
  abstract = {We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.},
  file = {/Users/emilchri/Zotero/storage/643DFN43/Malinowski og Fritz - 2014 - A Multi-World Approach to Question Answering about.pdf}
}

@article{manmadhanVisualQuestionAnswering2020,
  title = {Visual Question Answering: A State-of-the-Art Review},
  shorttitle = {Visual Question Answering},
  author = {Manmadhan, Sruthy and Kovoor, Binsu C.},
  date = {2020-12-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {53},
  number = {8},
  pages = {5705--5745},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09832-7},
  url = {https://doi.org/10.1007/s10462-020-09832-7},
  urldate = {2023-02-06},
  abstract = {Visual question answering (VQA) is a task that has received immense consideration from two major research communities: computer vision and natural language processing. Recently it has been widely accepted as an AI-complete task which can be used as an alternative to visual turing test. In its most common form, it is a multi-modal challenging task where a computer is required to provide the correct answer for a natural language question asked about an input image. It attracts many deep learning researchers after their remarkable achievements in text, voice and vision technologies. This review extensively and critically examines the current status of VQA research in terms of step by step solution methodologies, datasets and evaluation metrics. Finally, this paper also discusses future research directions for all the above-mentioned aspects of VQA separately.},
  langid = {english},
  keywords = {review,VQA},
  file = {/Users/emilchri/Zotero/storage/SZV53MDA/Manmadhan og Kovoor - 2020 - Visual question answering a state-of-the-art revi.pdf}
}

@online{maoDeepCaptioningMultimodal2015,
  title = {Deep {{Captioning}} with {{Multimodal Recurrent Neural Networks}} (m-{{RNN}})},
  author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
  date = {2015-06-11},
  eprint = {1412.6632},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6632},
  url = {http://arxiv.org/abs/1412.6632},
  urldate = {2023-02-24},
  abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/\textasciitilde junhua.mao/m-RNN.html .},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.2.7,vqa},
  file = {/Users/emilchri/Zotero/storage/P8KDGVI7/Mao et al. - 2015 - Deep Captioning with Multimodal Recurrent Neural N.pdf;/Users/emilchri/Zotero/storage/CQ8M8NNF/1412.html}
}

@article{mccarthyProposalDartmouthSummer2006,
  title = {A {{Proposal}} for the {{Dartmouth Summer Research Project}} on {{Artificial Intelligence}}, {{August}} 31, 1955},
  author = {McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
  date = {2006-12-15},
  journaltitle = {AI Magazine},
  volume = {27},
  number = {4},
  pages = {12--12},
  issn = {2371-9621},
  doi = {10.1609/aimag.v27i4.1904},
  url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904},
  urldate = {2023-03-10},
  abstract = {The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.},
  issue = {4},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/YWZFGRQQ/McCarthy et al. - 2006 - A Proposal for the Dartmouth Summer Research Proje.pdf}
}

@incollection{mccloskeyCatastrophicInterferenceConnectionist1989,
  title = {Catastrophic {{Interference}} in {{Connectionist Networks}}: {{The Sequential Learning Problem}}},
  shorttitle = {Catastrophic {{Interference}} in {{Connectionist Networks}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {McCloskey, Michael and Cohen, Neal J.},
  editor = {Bower, Gordon H.},
  date = {1989-01-01},
  volume = {24},
  pages = {109--165},
  publisher = {{Academic Press}},
  doi = {10.1016/S0079-7421(08)60536-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
  urldate = {2023-05-10},
  abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
  langid = {english},
  keywords = {catastrophic,connectionist,forgetting,llm,transformer},
  file = {/Users/emilchri/Zotero/storage/S69MYJBV/McCloskey og Cohen - 1989 - Catastrophic Interference in Connectionist Network.pdf;/Users/emilchri/Zotero/storage/JMDDIKUC/S0079742108605368.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  date = {1943-12-01},
  journaltitle = {The bulletin of mathematical biophysics},
  shortjournal = {Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  url = {https://doi.org/10.1007/BF02478259},
  urldate = {2023-04-29},
  abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  langid = {english},
  keywords = {Artificial,Excitatory Synapse,Inhibitory Synapse,mcculloch,Nervous Activity,neuron,pitts,Spatial Summation,Temporal Summation},
  file = {/Users/emilchri/Zotero/storage/K5I5XH7S/McCulloch og Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf}
}

@online{MigrateTensorFlowTensorFlow,
  title = {Migrate to {{TensorFlow}} 2 | {{TensorFlow Core}}},
  url = {https://www.tensorflow.org/guide/migrate},
  urldate = {2023-04-17},
  abstract = {Learn how to migrate your TensorFlow code from TensorFlow 1.x to TensorFlow 2.},
  langid = {english},
  organization = {{TensorFlow}},
  keywords = {migrate,tensorflow,tf},
  file = {/Users/emilchri/Zotero/storage/Q3XUBJBM/migrate.html}
}

@online{MLNodesUniversitetet,
  title = {ML nodes - Universitetet i Oslo},
  url = {https://www.uio.no/tjenester/it/forskning/kompetansehuber/uio-ai-hub-node-project/it-resources/ml-nodes/index.html},
  urldate = {2023-04-17},
  abstract = {The AI HUB provides resources and services for machine learning and deep learning tasks at UiO. This page describes the available resources, how to get access to them, how to use them and how to get support in using them.},
  langid = {norsk},
  keywords = {ml,nodes},
  file = {/Users/emilchri/Zotero/storage/JVVT9CMZ/index.html}
}

@thesis{mnihMachineLearningAerial,
  type = {phdthesis},
  title = {Machine {{Learning}} for {{Aerial Image Labeling}}},
  author = {Mnih, Volodymyr},
  institution = {{University of Toronto (Canada)}},
  location = {{Canada -- Ontario, CA}},
  url = {https://www.proquest.com/docview/1500835065/abstract/619BEBB8BCBA4755PQ/1},
  urldate = {2023-02-24},
  abstract = {Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.},
  isbn = {9780494961841},
  langid = {english},
  pagetotal = {110},
  keywords = {Applied sciences,Machine learning,Neural networks,Remote sensing},
  file = {/Users/emilchri/Zotero/storage/7KQN49HQ/Mnih - Machine Learning for Aerial Image Labeling.pdf}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  date = {2022},
  edition = {2},
  url = {https://christophm.github.io/interpretable-ml-book},
  urldate = {2022-11-29},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/Users/emilchri/Zotero/storage/HJDDEAXK/index.html}
}

@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date = {2018},
  journaltitle = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  doi = {10.1016/j.dsp.2017.10.011},
  keywords = {background,motivation,responsible,xai},
  file = {/Users/emilchri/Zotero/storage/IXU4WQ8N/1910.10045-1.pdf}
}

@article{nakkiranDeepDoubleDescent2021,
  title = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  shorttitle = {Deep Double Descent},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  date = {2021-12},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2021},
  number = {12},
  pages = {124003},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ac3a74},
  url = {https://dx.doi.org/10.1088/1742-5468/ac3a74},
  urldate = {2023-05-05},
  abstract = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  langid = {english},
  keywords = {data,more,performence,worse},
  file = {/Users/emilchri/Zotero/storage/C2ISR7T8/Nakkiran et al. - 2021 - Deep double descent where bigger models and more .pdf}
}

@online{nguyenExplainingMachineLearning2022,
  title = {Explaining {{Machine Learning Models}} in {{Natural Conversations}}: {{Towards}} a {{Conversational XAI Agent}}},
  shorttitle = {Explaining {{Machine Learning Models}} in {{Natural Conversations}}},
  author = {Nguyen, Van Bach and Schlötterer, Jörg and Seifert, Christin},
  date = {2022-09-06},
  eprint = {2209.02552},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.02552},
  url = {http://arxiv.org/abs/2209.02552},
  urldate = {2023-03-28},
  abstract = {The goal of Explainable AI (XAI) is to design methods to provide insights into the reasoning process of black-box models, such as deep neural networks, in order to explain them to humans. Social science research states that such explanations should be conversational, similar to human-to-human explanations. In this work, we show how to incorporate XAI in a conversational agent, using a standard design for the agent comprising natural language understanding and generation components. We build upon an XAI question bank which we extend by quality-controlled paraphrases to understand the user's information needs. We further systematically survey the literature for suitable explanation methods that provide the information to answer those questions, and present a comprehensive list of suggestions. Our work is the first step towards truly natural conversations about machine learning models with an explanation agent. The comprehensive list of XAI questions and the corresponding explanation methods may support other researchers in providing the necessary information to address users' demands.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,llm,xai},
  file = {/Users/emilchri/Zotero/storage/IPZ4WS2D/Nguyen et al. - 2022 - Explaining Machine Learning Models in Natural Conv.pdf;/Users/emilchri/Zotero/storage/5YG6LN6W/2209.html}
}

@online{OpenAIAPI,
  title = {{{OpenAI API}}},
  url = {https://platform.openai.com},
  urldate = {2023-04-07},
  abstract = {An API for accessing new AI models developed by OpenAI},
  langid = {english},
  keywords = {openai,text-davinci-003 model},
  file = {/Users/emilchri/Zotero/storage/S9X7T5MZ/gpt-3-5.html}
}

@online{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  date = {2023-03-15},
  eprint = {2303.08774},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-03-16},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,gpt,gpt-4,openai,transformer},
  file = {/Users/emilchri/Zotero/storage/D5M33AE8/OpenAI - 2023 - GPT-4 Technical Report.pdf;/Users/emilchri/Zotero/storage/PHV2YAG6/2303.html}
}

@article{parkFairVQAFairnessAwareVisual2020,
  title = {Fair-{{VQA}}: {{Fairness-Aware Visual Question Answering Through Sensitive Attribute Prediction}}},
  shorttitle = {Fair-{{VQA}}},
  author = {Park, Sungho and Hwang, Sunhee and Hong, Jongkwang and Byun, Hyeran},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {215091--215099},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3041503},
  abstract = {Visual Question Answering (VQA) is a task that answers questions on given images. Although previous works achieve a great improvement in VQA performance, they do not consider the fairness of answers in terms of ethically sensitive attributes, such as gender. Therefore, we propose a Fair-VQA model that contains two modules: VQA module and SAP (Sensitive Attribute Prediction) module. On top of VQA module, which predicts various kinds of answers, SAP module predicts only sensitive attributes using the same inputs. The predictions of SAP module are utilized to rectify answers from VQA module to be fairer in terms of the sensitive attributes with graceful performance degradation. To validate the proposed method, we conduct extensive experiments on VQA, GQA, and our proposing VQA-Gender datasets. In all the experiments, our method shows the fairest results in various metrics for fairness. Moreover, we demonstrate that our method works interpretably through the analysis of visualized attention maps.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Data models,Face recognition,FAI,fairness,Knowledge discovery,Predictive models,Task analysis,visual question answering,Visualization,VQA},
  file = {/Users/emilchri/Zotero/storage/K5S465UT/Park et al. - 2020 - Fair-VQA Fairness-Aware Visual Question Answering.pdf;/Users/emilchri/Zotero/storage/7QJRTPNI/9274341.html}
}

@online{PeronaLabCUB2002011,
  title = {Perona {{Lab}} - {{CUB-200-2011}}},
  url = {https://www.vision.caltech.edu/datasets/cub_200_2011/},
  urldate = {2023-04-17},
  keywords = {CUB},
  file = {/Users/emilchri/Zotero/storage/Z7L79WIT/cub_200_2011.html}
}

@incollection{piccininiFirstComputationalTheory2020,
  title = {The {{First Computational Theory}} of {{Cognition}}: {{McCulloch}} and {{Pitts}}’s “{{A Logical Calculus}} of the {{Ideas Immanent}} in {{Nervous Activity}}”},
  shorttitle = {{{C5The First Computational Theory}} of {{Cognition}}},
  booktitle = {Neurocognitive {{Mechanisms}}: {{Explaining Biological Cognition}}},
  author = {Piccinini, Gualtiero},
  editor = {Piccinini, Gualtiero},
  date = {2020-11-12},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198866282.003.0006},
  url = {https://doi.org/10.1093/oso/9780198866282.003.0006},
  urldate = {2023-04-29},
  abstract = {McCulloch and Pitts were the first to use and Alan Turing’s notion of computation to understand neural, and thus cognitive, activity. McCulloch and Pitts’s contributions included (i) a formalism whose refinement and generalization led to the notion of finite automata, which is an important formalism in computability theory, (ii) a technique that inspired the notion of logic design, which is a fundamental part of modern computer design, (iii) the first use of computation to address the mind–body problem, and (iv) the first modern computational theory of cognition, which posits that neurons are equivalent to logic gates and neural networks are digital circuits.},
  isbn = {978-0-19-886628-2},
  keywords = {artificial,first,mcculloch,neuron},
  file = {/Users/emilchri/Zotero/storage/NMJJKU4F/267882205.html}
}

@online{Podman,
  title = {Podman},
  url = {https://podman.io/},
  urldate = {2023-04-17},
  file = {/Users/emilchri/Zotero/storage/M4QUYMNA/podman.io.html}
}

@article{ponzioDealingLackTraining2019,
  title = {Dealing with {{Lack}} of {{Training Data}} for {{Convolutional Neural Networks}}: {{The Case}} of {{Digital Pathology}}},
  shorttitle = {Dealing with {{Lack}} of {{Training Data}} for {{Convolutional Neural Networks}}},
  author = {Ponzio, Francesco and Urgese, Gianvito and Ficarra, Elisa and Di Cataldo, Santa},
  date = {2019-02-26},
  journaltitle = {Electronics},
  shortjournal = {Electronics},
  volume = {8},
  pages = {256},
  doi = {10.3390/electronics8030256},
  abstract = {Thanks to their capability to learn generalizable descriptors directly from images, deep Convolutional Neural Networks (CNNs) seem the ideal solution to most pattern recognition problems. On the other hand, to learn the image representation, CNNs need huge sets of annotated samples that are unfeasible in many every-day scenarios. This is the case, for example, of Computer-Aided Diagnosis (CAD) systems for digital pathology, where additional challenges are posed by the high variability of the cancerous tissue characteristics. In our experiments, state-of-the-art CNNs trained from scratch on histological images were less accurate and less robust to variability than a traditional machine learning framework, highlighting all the issues of fully training deep networks with limited data from real patients. To solve this problem, we designed and compared three transfer learning frameworks, leveraging CNNs pre-trained on non-medical images. This approach obtained very high accuracy, requiring much less computational resource for the training. Our findings demonstrate that transfer learning is a solution to the automated classification of histological samples and solves the problem of designing accurate and computationally-efficient CAD systems with limited training data.},
  keywords = {computer-aided diagnosis systems,convolutional neural networks,deep learning,histological image analysis,transfer learning},
  file = {/Users/emilchri/Zotero/storage/222973QA/Ponzio et al. - 2019 - Dealing with Lack of Training Data for Convolution.pdf;/Users/emilchri/Zotero/storage/YB368QIX/Ponzio et al. - 2019 - Dealing with Lack of Training Data for Convolution.pdf;/Users/emilchri/Zotero/storage/CFQ9VNCA/256.html}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  journaltitle = {OpenAI},
  pages = {12},
  url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  urldate = {2023-03-16},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {gpt,gpt-1,nlp,openai,transformer},
  file = {/Users/emilchri/Zotero/storage/XJ3M2QT7/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  journaltitle = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  urldate = {2023-03-16},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/DKNZIT5B/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@online{radfordLearningGenerateReviews2017,
  title = {Learning to {{Generate Reviews}} and {{Discovering Sentiment}}},
  author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  date = {2017-04-06},
  eprint = {1704.01444},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.01444},
  url = {http://arxiv.org/abs/1704.01444},
  urldate = {2023-05-12},
  abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,linear,model,neuron,sentiment,transformer},
  file = {/Users/emilchri/Zotero/storage/4H6B4YYL/Radford et al. - 2017 - Learning to Generate Reviews and Discovering Senti.pdf;/Users/emilchri/Zotero/storage/FK6SPPK3/1704.html}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2023-03-28},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {preprint},
  keywords = {clip,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/2QPLP53B/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/Users/emilchri/Zotero/storage/LICZQPU2/2103.html}
}

@online{raschkaChapterIntroductionMachine2020,
  title = {Chapter 1: {{Introduction}} to {{Machine Learning}} and {{Deep Learning}}},
  shorttitle = {Chapter 1},
  author = {Raschka, Sebastian},
  date = {2020-04-05},
  url = {https://sebastianraschka.com/blog/2020/intro-to-dl-ch01.html},
  urldate = {2023-03-16},
  abstract = {Note: This is a write-up of the lecture slides I created for the deep learning class I am teaching at UW-Madison. You can find a recent version of these slid...},
  langid = {english},
  organization = {{Sebastian Raschka, PhD}},
  keywords = {ai,intersection,machine learning},
  file = {/Users/emilchri/Zotero/storage/GVL6RB4F/intro-to-dl-ch01.html}
}

@inproceedings{rasouliEXPLANExplainingBlackbox2020,
  title = {{{EXPLAN}}: {{Explaining Black-box Classifiers}} Using {{Adaptive Neighborhood Generation}}},
  shorttitle = {{{EXPLAN}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Rasouli, Peyman and Yu, Ingrid Chieh},
  date = {2020-07},
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9206710},
  abstract = {Defining a representative locality is an urgent challenge in perturbation-based explanation methods, which influences the fidelity and soundness of explanations. We address this issue by proposing a robust and intuitive approach for EXPLaining black-box classifiers using Adaptive Neighborhood generation (EXPLAN). EXPLAN is a module-based algorithm consisted of dense data generation, representative data selection, data balancing, and rule-based interpretable model. It takes into account the adjacency information derived from the black-box decision function and the structure of the data for creating a representative neighborhood for the instance being explained. As a local model-agnostic explanation method, EXPLAN generates explanations in the form of logical rules that are highly interpretable and well-suited for qualitative analysis of the model's behavior. We discuss fidelity-interpretability trade-offs and demonstrate the performance of the proposed algorithm by a comprehensive comparison with state-of-the-art explanation methods LIME, LORE, and Anchor. The conducted experiments on real-world data sets show our method achieves solid empirical results in terms of fidelity, precision, and stability of explanations.},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Analytical models,Data models,Data Sampling,Decision trees,Interpretable Machine Learning,Machine learning,Neural networks,Perturbation-based Explanation Methods,Predictive models,Training data,XAI},
  file = {/Users/emilchri/Zotero/storage/FUGC4N8V/Rasouli og Yu - 2020 - EXPLAN Explaining Black-box Classifiers using Ada.pdf;/Users/emilchri/Zotero/storage/Y6K8436R/9206710.html}
}

@online{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016-12-25},
  eprint = {1612.08242},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.08242},
  url = {http://arxiv.org/abs/1612.08242},
  urldate = {2023-04-09},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/TGQ6FKAD/Redmon og Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf;/Users/emilchri/Zotero/storage/YD54F39C/1612.html}
}

@online{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018-04-08},
  eprint = {1804.02767},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1804.02767},
  url = {http://arxiv.org/abs/1804.02767},
  urldate = {2023-04-09},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/K4YMQ9RD/Redmon og Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf;/Users/emilchri/Zotero/storage/XJUKIUWH/1804.html}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016},
  pages = {779--788},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {object detection,yolo},
  file = {/Users/emilchri/Zotero/storage/U9GZQFRC/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/emilchri/Zotero/storage/M526BS3A/Redmon_You_Only_Look_CVPR_2016_paper.html}
}

@inproceedings{renFasterRCNNRealTime2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
  urldate = {2022-10-24},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  file = {/Users/emilchri/Zotero/storage/9T8ISWCK/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf}
}

@unpublished{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2022-02-02},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,LIME,Michael,Statistics - Machine Learning,XAI},
  file = {/Users/emilchri/Zotero/storage/KYA42FCA/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/Users/emilchri/Zotero/storage/N96AM4KH/1602.html}
}

@book{rothShapleyValueEssays1988,
  title = {The {{Shapley Value}}: {{Essays}} in {{Honor}} of {{Lloyd S}}. {{Shapley}}},
  shorttitle = {The {{Shapley Value}}},
  editor = {Roth, Alvin E.},
  date = {1988},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511528446},
  url = {https://www.cambridge.org/core/books/shapley-value/D3829B63B5C3108EFB62C4009E2B966E},
  urldate = {2022-11-29},
  abstract = {Composed in honour of the sixty-fifth birthday of Lloyd Shapley, this volume makes accessible the large body of work that has grown out of Shapley's seminal 1953 paper. Each of the twenty essays concerns some aspect of the Shapley value. Three of the chapters are reprints of the 'ancestral' papers: Chapter 2 is Shapley's original 1953 paper defining the value; Chapter 3 is the 1954 paper by Shapley and Shubik applying the value to voting models; and chapter 19 is Shapley's 1969 paper defining a value for games without transferable utility. The other seventeen chapters were contributed especially for this volume. The first chapter introduces the subject and the other essays in the volume, and contains a brief account of a few of Shapley's other major contributions to game theory. The other chapters cover the reformulations, interpretations and generalizations that have been inspired by the Shapley value, and its applications to the study of coalition formulation, to the organization of large markets, to problems of cost allocation, and to the study of games in which utility is not transferable.},
  isbn = {978-0-521-36177-4},
  file = {/Users/emilchri/Zotero/storage/36EG9VR6/D3829B63B5C3108EFB62C4009E2B966E.html}
}

@online{ruizDreamBoothFineTuning2023,
  title = {{{DreamBooth}}: {{Fine Tuning Text-to-Image Diffusion Models}} for {{Subject-Driven Generation}}},
  shorttitle = {{{DreamBooth}}},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  date = {2023-03-15},
  eprint = {2208.12242},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.12242},
  url = {http://arxiv.org/abs/2208.12242},
  urldate = {2023-04-11},
  abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,lora,Stable diffusion}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-10-06},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  keywords = {backprop,backpropagation,Humanities and Social Sciences,multidisciplinary,rnn,Science},
  file = {/Users/emilchri/Zotero/storage/XL4G6Y63/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/Users/emilchri/Zotero/storage/UB383953/323533a0.html}
}

@article{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-12-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {115},
  number = {3},
  pages = {211--252},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  url = {https://doi.org/10.1007/s11263-015-0816-y},
  urldate = {2023-03-13},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
  langid = {english},
  keywords = {Benchmark,challenge,competition,Dataset,ImageNet,Large-scale,Object detection,Object recognition},
  file = {/Users/emilchri/Zotero/storage/QD8JEQN9/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf}
}

@inproceedings{sagirogluBigDataReview2013,
  title = {Big Data: {{A}} Review},
  shorttitle = {Big Data},
  booktitle = {2013 {{International Conference}} on {{Collaboration Technologies}} and {{Systems}} ({{CTS}})},
  author = {Sagiroglu, Seref and Sinanc, Duygu},
  date = {2013-05},
  pages = {42--47},
  doi = {10.1109/CTS.2013.6567202},
  abstract = {Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.},
  eventtitle = {2013 {{International Conference}} on {{Collaboration Technologies}} and {{Systems}} ({{CTS}})},
  keywords = {big data,Data handling,Data models,Data storage systems,Information management,Organizations,Security,value,variety,velocity,verification,volume},
  file = {/Users/emilchri/Zotero/storage/Y7TM48BR/Sagiroglu og Sinanc - 2013 - Big data A review.pdf;/Users/emilchri/Zotero/storage/BIBDPGXW/6567202.html}
}

@inproceedings{sakuradaAnomalyDetectionUsing2014,
  title = {Anomaly {{Detection Using Autoencoders}} with {{Nonlinear Dimensionality Reduction}}},
  booktitle = {Proceedings of the {{MLSDA}} 2014 2nd {{Workshop}} on {{Machine Learning}} for {{Sensory Data Analysis}}},
  author = {Sakurada, Mayu and Yairi, Takehisa},
  date = {2014-12-02},
  series = {{{MLSDA}}'14},
  pages = {4--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2689746.2689747},
  url = {https://dl.acm.org/doi/10.1145/2689746.2689747},
  urldate = {2023-05-05},
  abstract = {This paper proposes to use autoencoders with nonlinear dimensionality reduction in the anomaly detection task. The authors apply dimensionality reduction by using an autoencoder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property. The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data. This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails. Also, autoencoders can increase their accuracy by extending them to denoising autoenconders. Moreover, autoencoders can be useful as nonlinear techniques without complex computation as kernel PCA requires. Finaly, the authors examine the learned features in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.},
  isbn = {978-1-4503-3159-3},
  keywords = {anomaly detection,auto-assosiative neural network,autoencoder,denoising autoencoder,dimensionality reduction,dimention,fault detection,nonlinear,novelty detection,pca,reduction,spacecrafts},
  file = {/Users/emilchri/Zotero/storage/G2MPYYDQ/Sakurada og Yairi - 2014 - Anomaly Detection Using Autoencoders with Nonlinea.pdf}
}

@article{salehiSynthesizingTalkingChild2022,
  title = {Synthesizing a {{Talking Child Avatar}} to {{Train Interviewers Working}} with {{Maltreated Children}}},
  author = {Salehi, Pegah and Hassan, Syed Zohaib and Lammerse, Myrthe and Sabet, Saeed Shafiee and Riiser, Ingvild and Røed, Ragnhild Klingenberg and Johnson, Miriam S. and Thambawita, Vajira and Hicks, Steven A. and Powell, Martine and Lamb, Michael E. and Baugerud, Gunn Astrid and Halvorsen, Pål and Riegler, Michael A.},
  date = {2022-06},
  journaltitle = {Big Data and Cognitive Computing},
  volume = {6},
  number = {2},
  pages = {62},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-2289},
  doi = {10.3390/bdcc6020062},
  url = {https://www.mdpi.com/2504-2289/6/2/62},
  urldate = {2023-02-24},
  abstract = {When responding to allegations of child sexual, physical, and psychological abuse, Child Protection Service (CPS) workers and police personnel need to elicit detailed and accurate accounts of the abuse to assist in decision-making and prosecution. Current research emphasizes the importance of the interviewer’s ability to follow empirically based guidelines. In doing so, it is essential to implement economical and scientific training courses for interviewers. Due to recent advances in artificial intelligence, we propose to generate a realistic and interactive child avatar, aiming to mimic a child. Our ongoing research involves the integration and interaction of different components with each other, including how to handle the language, auditory, emotional, and visual components of the avatar. This paper presents three subjective studies that investigate and compare various state-of-the-art methods for implementing multiple aspects of the child avatar. The first user study evaluates the whole system and shows that the system is well received by the expert and highlights the importance of its realism. The second user study investigates the emotional component and how it can be integrated with video and audio, and the third user study investigates realism in the auditory and visual components of the avatar created by different methods. The insights and feedback from these studies have contributed to the refined and improved architecture of the child avatar system which we present here.},
  issue = {2},
  langid = {english},
  keywords = {Child Protection Services (CPS),generative adversarial networks (GANs),generative pre-trained transformer 3 (GPT-3),interview training,Michael,virtual child avatar},
  file = {/Users/emilchri/Zotero/storage/DVUFJ5RQ/Salehi et al. - 2022 - Synthesizing a Talking Child Avatar to Train Inter.pdf}
}

@online{schulmanIntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  author = {Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob},
  url = {https://openai.com/blog/chatgpt},
  urldate = {2023-04-07},
  abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  langid = {american},
  keywords = {openai},
  file = {/Users/emilchri/Zotero/storage/3V5KCALC/chatgpt.html}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  url = {http://arxiv.org/abs/1610.02391},
  urldate = {2022-02-02},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Michael},
  file = {/Users/emilchri/Zotero/storage/XMKTRQ4A/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;/Users/emilchri/Zotero/storage/NZ2RIN7L/1610.html}
}

@online{Sentencepiece,
  title = {Sentencepiece},
  url = {https://github.com/google/sentencepiece},
  urldate = {2023-04-10},
  abstract = {Unsupervised text tokenizer for Neural Network-based text generation. - sentencepiece/normalization.md at master · google/sentencepiece},
  langid = {english},
  organization = {{GitHub}},
  file = {/Users/emilchri/Zotero/storage/XYNW8AB3/sentencepiece.html}
}

@online{seoEndtoendGenerativePretraining2022,
  title = {End-to-End {{Generative Pretraining}} for {{Multimodal Video Captioning}}},
  author = {Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  date = {2022-05-10},
  eprint = {2201.08264},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.08264},
  urldate = {2022-09-12},
  abstract = {Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective -- we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.},
  pubstate = {preprint},
  keywords = {Andrea,Captioning,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,End-to-end,Video,XAI},
  file = {/Users/emilchri/Zotero/storage/9WJ4FZFN/Seo et al. - 2022 - End-to-end Generative Pretraining for Multimodal V.pdf;/Users/emilchri/Zotero/storage/GZVL6PSC/2201.html}
}

@online{SequenceContextLength,
  title = {Sequence/Context Length of This Model? · {{Issue}} \#16 · Facebookresearch/Llama},
  url = {https://github.com/facebookresearch/llama/issues/16},
  urldate = {2023-04-11},
  keywords = {LLaMA,llm,token length},
  file = {/Users/emilchri/Zotero/storage/M4Z724GB/16.html}
}

@online{serranoAttentionInterpretable2019,
  title = {Is {{Attention Interpretable}}?},
  author = {Serrano, Sofia and Smith, Noah A.},
  date = {2019-06-09},
  eprint = {1906.03731},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.03731},
  url = {http://arxiv.org/abs/1906.03731},
  urldate = {2023-05-12},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  pubstate = {preprint},
  keywords = {attention,Computer Science - Computation and Language,transformer,xai},
  file = {/Users/emilchri/Zotero/storage/2GG2RMMS/Serrano og Smith - 2019 - Is Attention Interpretable.pdf;/Users/emilchri/Zotero/storage/9U8P9FA6/1906.html}
}

@article{shapleyValueNpersonGames,
  title = {A Value for N-Person Games},
  author = {Shapley, Lloyd S},
  pages = {10},
  langid = {english},
  keywords = {Shapley},
  file = {/Users/emilchri/Zotero/storage/AEEQN3ZS/Shapley - A value for i-person games.pdf}
}

@article{sharmaImageCaptioningImproved2022,
  title = {Image Captioning Improved Visual Question Answering},
  author = {Sharma, Himanshu and Jalal, Anand Singh},
  date = {2022-10-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {81},
  number = {24},
  pages = {34775--34796},
  issn = {1573-7721},
  doi = {10.1007/s11042-021-11276-2},
  url = {https://doi.org/10.1007/s11042-021-11276-2},
  urldate = {2022-10-03},
  abstract = {Both Visual Question Answering (VQA) and image captioning are the problems which involve Computer Vision (CV) and Natural Language Processing (NLP) domains. In general, computer vision models are effectively utilized to represent visual contents. While NLP algorithms are used to represent the sentences. In recent years, VQA and image captioning tasks are tackled independently although they require similar type of algorithms. In this paper, a joint relationship between these two tasks is established and exploited. We present an image captioning based VQA model that uses the knowledge learnt from the image captioning task and transfers that knowledge to VQA task. We integrate the image captioning module into the VQA model by fusing the features obtained from captioning model and the attention-based visual feature. The experimental results demonstrate the improvement in the answer generation accuracy by a margin 3.45 \% on VQA 1.0, 3.33\% on VQA 2.0 and 1.73\% on VQA-CP v2 datasets over the state-of-the-art VQA models.},
  langid = {english},
  keywords = {Computer vision (CV),Image captioning,Natural language processing (NLP),Visual question answering (VQA)},
  file = {/Users/emilchri/Zotero/storage/UZKWM4XT/Sharma og Jalal - 2022 - Image captioning improved visual question answerin.pdf}
}

@online{shazeerGLUVariantsImprove2020,
  title = {{{GLU Variants Improve Transformer}}},
  author = {Shazeer, Noam},
  date = {2020-02-12},
  eprint = {2002.05202},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.05202},
  url = {http://arxiv.org/abs/2002.05202},
  urldate = {2023-05-03},
  abstract = {Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.},
  pubstate = {preprint},
  keywords = {activation,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,function,glu,Statistics - Machine Learning,swiglu},
  file = {/Users/emilchri/Zotero/storage/RRQJP2HB/Shazeer - 2020 - GLU Variants Improve Transformer.pdf;/Users/emilchri/Zotero/storage/R5ZZU49C/2002.html}
}

@book{shelleyFrankensteinModernPrometheus1992,
  title = {Frankenstein, or, {{The}} Modern {{Prometheus}}},
  author = {Shelley, Mary Wollstonecraft},
  date = {1992},
  publisher = {{Knopf: New York}}
}

@article{silverGeneralReinforcementLearning2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aar6404},
  url = {https://www.science.org/doi/full/10.1126/science.aar6404},
  urldate = {2022-10-07},
  keywords = {alpha,chess,zero},
  file = {/Users/emilchri/Zotero/storage/6JJJJ369/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf;/Users/emilchri/Zotero/storage/9J2Q4BFR/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2022-10-17},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,VGG,VGG16},
  file = {/Users/emilchri/Zotero/storage/D3P9G8IV/Simonyan og Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/Users/emilchri/Zotero/storage/FPFGXZGM/1409.html}
}

@online{soaresFairbydesignExplainableModels2019,
  title = {Fair-by-Design Explainable Models for Prediction of Recidivism},
  author = {Soares, Eduardo and Angelov, Plamen},
  date = {2019-09-18},
  eprint = {1910.02043},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.02043},
  urldate = {2022-11-25},
  abstract = {Recidivism prediction provides decision makers with an assessment of the likelihood that a criminal defendant will reoffend that can be used in pre-trial decision-making. It can also be used for prediction of locations where crimes most occur, profiles that are more likely to commit violent crimes. While such instruments are gaining increasing popularity, their use is controversial as they may present potential discriminatory bias in the risk assessment. In this paper we propose a new fair-by-design approach to predict recidivism. It is prototype-based, learns locally and extracts empirically the data distribution. The results show that the proposed method is able to reduce the bias and provide human interpretable rules to assist specialists in the explanation of the given results.},
  pubstate = {preprint},
  keywords = {bias,Computer Science - Machine Learning,preaktiv prediksjon,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/Y2VFNGFF/Soares og Angelov - 2019 - Fair-by-design explainable models for prediction o.pdf;/Users/emilchri/Zotero/storage/XIU78Z6I/1910.html}
}

@article{sorensenMethodEstablishingGroups1948,
  title = {A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species and Its Application to Analyses of the Vegetation on {{Danish}} Commons},
  author = {Sørensen, Thorvald},
  date = {1948},
  journaltitle = {Kongelige Danske Videnskabernes Selskab},
  volume = {5},
  number = {4},
  pages = {1--34},
  url = {https://www.royalacademy.dk/Publications/High/295_S%C3%B8rensen,%20Thorvald.pdf},
  urldate = {2023-04-04},
  file = {/Users/emilchri/Zotero/storage/7K335L4L/295_Sørensen, Thorvald.pdf}
}

@online{srinivasanGeneratingUserfriendlyExplanations2019,
  title = {Generating {{User-friendly Explanations}} for {{Loan Denials}} Using {{GANs}}},
  author = {Srinivasan, Ramya and Chander, Ajay and Pezeshkpour, Pouya},
  date = {2019-06-24},
  eprint = {1906.10244},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.10244},
  urldate = {2022-09-12},
  abstract = {Financial decisions impact our lives, and thus everyone from the regulator to the consumer is interested in fair, sound, and explainable decisions. There is increasing competitive desire and regulatory incentive to deploy AI mindfully within financial services. An important mechanism towards that end is to explain AI decisions to various stakeholders. State-of-the-art explainable AI systems mostly serve AI engineers and offer little to no value to business decision makers, customers, and other stakeholders. Towards addressing this gap, in this work we consider the scenario of explaining loan denials. We build the first-of-its-kind dataset that is representative of loan-applicant friendly explanations. We design a novel Generative Adversarial Network (GAN) that can accommodate smaller datasets, to generate user-friendly textual explanations. We demonstrate how our system can also generate explanations serving different purposes: those that help educate the loan applicants, or help them take appropriate action towards a future approval.},
  pubstate = {preprint},
  keywords = {Andrea,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/emilchri/Zotero/storage/2MB7T7BT/Srinivasan et al. - 2019 - Generating User-friendly Explanations for Loan Den.pdf;/Users/emilchri/Zotero/storage/EXRNA8BU/1906.html}
}

@article{sunExplainImproveLRPinference2022,
  title = {Explain and Improve: {{LRP-inference}} Fine-Tuning for Image Captioning Models},
  shorttitle = {Explain and Improve},
  author = {Sun, Jiamei and Lapuschkin, Sebastian and Samek, Wojciech and Binder, Alexander},
  date = {2022-01-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {77},
  pages = {233--246},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253521001494},
  urldate = {2022-09-20},
  abstract = {This paper analyzes the predictions of image captioning models with attention mechanisms beyond visualizing the attention itself. We develop variants of Layer-wise Relevance Propagation (LRP) and gradient-based explanation methods, tailored to image captioning models with attention mechanisms. We compare the interpretability of attention heatmaps systematically against the explanations provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide simultaneously pixel-wise image explanations (supporting and opposing pixels of the input image) and linguistic explanations (supporting and opposing words of the preceding sequence) for each word in the predicted captions. We demonstrate with extensive experiments that explanation methods (1) can reveal additional evidence used by the model to make decisions compared to attention; (2) correlate to object locations with high precision; (3) are helpful to “debug” the model, e.g. by analyzing the reasons for hallucinated object words. With the observed properties of explanations, we further design an LRP-inference fine-tuning strategy that reduces the issue of object hallucination in image captioning models, and meanwhile, maintains the sentence fluency. We conduct experiments with two widely used attention mechanisms: the adaptive attention mechanism calculated with the additive attention and the multi-head attention mechanism calculated with the scaled dot product.},
  langid = {english},
  keywords = {Attention,Explainable AI,Image captioning,Neural networks},
  file = {/Users/emilchri/Zotero/storage/AIH469ZX/Sun et al. - 2022 - Explain and improve LRP-inference fine-tuning for.pdf;/Users/emilchri/Zotero/storage/B9P978BS/S1566253521001494.html}
}

@article{sunUnderstandingImageCaptioning,
  title = {Understanding {{Image Captioning Models}} beyond {{Visualizing Attention}}},
  author = {Sun, Jiamei and Lapuschkin, Sebastian and Samek, Wojciech and Binder, Alexander},
  pages = {8},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/6GEBNQCL/Sun et al. - Understanding Image Captioning Models beyond Visua.pdf}
}

@online{suRoFormerEnhancedTransformer2022,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  date = {2022-08-08},
  eprint = {2104.09864},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.09864},
  url = {http://arxiv.org/abs/2104.09864},
  urldate = {2023-05-04},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \textbackslash url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  pubstate = {preprint},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,rope,rotary,self},
  file = {/Users/emilchri/Zotero/storage/E9J2W9UG/Su et al. - 2022 - RoFormer Enhanced Transformer with Rotary Positio.pdf;/Users/emilchri/Zotero/storage/2HISB8AH/2104.html}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
  urldate = {2023-01-17},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  keywords = {rnn},
  file = {/Users/emilchri/Zotero/storage/AYDY2JAT/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@online{suVLBERTPretrainingGeneric2020,
  title = {{{VL-BERT}}: {{Pre-training}} of {{Generic Visual-Linguistic Representations}}},
  shorttitle = {{{VL-BERT}}},
  author = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  date = {2020-02-17},
  eprint = {1908.08530},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.08530},
  url = {http://arxiv.org/abs/1908.08530},
  urldate = {2023-03-16},
  abstract = {We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \textbackslash url\{https://github.com/jackroos/VL-BERT\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,transformer},
  file = {/Users/emilchri/Zotero/storage/VBWT5Y7P/Su et al. - 2020 - VL-BERT Pre-training of Generic Visual-Linguistic.pdf;/Users/emilchri/Zotero/storage/T9TCAXX9/1908.html}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  date = {2016},
  pages = {2818--2826},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html},
  urldate = {2022-10-26},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/ZT9JDQ2V/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf;/Users/emilchri/Zotero/storage/JYK2S7HK/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html}
}

@inproceedings{tanwaniRepsNetCombiningVision2022,
  title = {{{RepsNet}}: {{Combining Vision}} with {{Language}} for {{Automated Medical Reports}}},
  shorttitle = {{{RepsNet}}},
  author = {Tanwani, Ajay K. and Barral, Joelle and Freedman, Daniel},
  date = {2022},
  pages = {714--724},
  publisher = {{Springer}},
  file = {/Users/emilchri/Zotero/storage/YZDPIC4N/Tanwani et al. - 2022 - RepsNet Combining Vision with Language for Automated Medical Reports.pdf}
}

@software{taoriStanfordAlpacaInstructionfollowing2023,
  title = {Stanford {{Alpaca}}: {{An Instruction-following LLaMA Model}}},
  shorttitle = {Stanford {{Alpaca}}},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  date = {2023-04-07T14:24:17Z},
  origdate = {2023-03-10T23:33:09Z},
  url = {https://github.com/tatsu-lab/stanford_alpaca},
  urldate = {2023-04-07},
  abstract = {Code and documentation to train Stanford's Alpaca models, and generate the data.},
  organization = {{Tatsu's shared repositories}},
  keywords = {Alpaca,deep-learning,github,instruction-following,language-model,llm,Stanford}
}

@online{taoriStanfordCRFM,
  title = {Stanford {{CRFM}}},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
  urldate = {2023-04-07},
  abstract = {We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce ({$<$}600\$).},
  organization = {{Alpaca: A Strong, Replicable Instruction-Following Model}},
  keywords = {Alpaca,llm,Stanford},
  file = {/Users/emilchri/Zotero/storage/VC8848IT/alpaca.html}
}

@online{teneyTipsTricksVisual2017,
  title = {Tips and {{Tricks}} for {{Visual Question Answering}}: {{Learnings}} from the 2017 {{Challenge}}},
  shorttitle = {Tips and {{Tricks}} for {{Visual Question Answering}}},
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and family=Hengel, given=Anton, prefix=van den, useprefix=false},
  date = {2017-08-09},
  eprint = {1708.02711},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02711},
  url = {http://arxiv.org/abs/1708.02711},
  urldate = {2022-12-07},
  abstract = {This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/6H7PKFRP/Teney et al. - 2017 - Tips and Tricks for Visual Question Answering Lea.pdf;/Users/emilchri/Zotero/storage/KKXR7HAE/1708.html}
}

@article{teneyVisualQuestionAnswering2017,
  title = {Visual {{Question Answering}}: {{A Tutorial}}},
  shorttitle = {Visual {{Question Answering}}},
  author = {Teney, Damien and Wu, Qi and family=Hengel, given=Anton, prefix=van den, useprefix=true},
  date = {2017-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {63--75},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2739826},
  abstract = {The task of visual question answering (VQA) is receiving increasing interest from researchers in both the computer vision and natural language processing fields. Tremendous advances have been seen in the field of computer vision due to the success of deep learning, in particular on low- and midlevel tasks, such as image segmentation or object recognition. These advances have fueled researchers' confidence for tackling more complex tasks that combine vision with language and high-level reasoning. VQA is a prime example of this trend. This article presents the ongoing work in the field and the current approaches to VQA based on deep learning. VQA constitutes a test for deep visual understanding and a benchmark for general artificial intelligence (AI). While the field of VQA has seen recent successes, it remains a largely unsolved task.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {bias,Bioinformatics,Computer vision,Genomics,Machine learning,Visualization,vqa},
  file = {/Users/emilchri/Zotero/storage/8N95PY3A/Teney et al. - 2017 - Visual Question Answering A Tutorial.pdf;/Users/emilchri/Zotero/storage/LNW7MSC5/8103161.html}
}

@article{thrallArtificialIntelligenceMachine2018,
  title = {Artificial {{Intelligence}} and {{Machine Learning}} in {{Radiology}}: {{Opportunities}}, {{Challenges}}, {{Pitfalls}}, and {{Criteria}} for {{Success}}},
  shorttitle = {Artificial {{Intelligence}} and {{Machine Learning}} in {{Radiology}}},
  author = {Thrall, James H. and Li, Xiang and Li, Quanzheng and Cruz, Cinthia and Do, Synho and Dreyer, Keith and Brink, James},
  date = {2018-03-01},
  journaltitle = {Journal of the American College of Radiology},
  shortjournal = {Journal of the American College of Radiology},
  series = {Data {{Science}}: {{Big Data Machine Learning}} and {{Artificial Intelligence}}},
  volume = {15},
  pages = {504--508},
  issn = {1546-1440},
  doi = {10.1016/j.jacr.2017.12.026},
  url = {https://www.sciencedirect.com/science/article/pii/S154614401731671X},
  urldate = {2023-04-28},
  abstract = {Worldwide interest in artificial intelligence (AI) applications, including imaging, is high and growing rapidly, fueled by availability of large datasets (“big data”), substantial advances in computing power, and new deep-learning algorithms. Apart from developing new AI methods per se, there are many opportunities and challenges for the imaging community, including the development of a common nomenclature, better ways to share image data, and standards for validating AI program use across different imaging platforms and patient populations. AI surveillance programs may help radiologists prioritize work lists by identifying suspicious or positive cases for early review. AI programs can be used to extract “radiomic” information from images not discernible by visual inspection, potentially increasing the diagnostic and prognostic value derived from image datasets. Predictions have been made that suggest AI will put radiologists out of business. This issue has been overstated, and it is much more likely that radiologists will beneficially incorporate AI methods into their practices. Current limitations in availability of technical expertise and even computing power will be resolved over time and can also be addressed by remote access solutions. Success for AI in imaging will be measured by value created: increased diagnostic certainty, faster turnaround, better outcomes for patients, and better quality of work life for radiologists. AI offers a new and promising set of methods for analyzing image data. Radiologists will explore these new pathways and are likely to play a leading role in medical applications of AI.},
  issue = {3, Part B},
  langid = {english},
  keywords = {Artificial intelligence,challenges,machine learning,opportunities,pitfalls},
  file = {/Users/emilchri/Zotero/storage/JXTWAYKE/Thrall et al. - 2018 - Artificial Intelligence and Machine Learning in Ra.pdf;/Users/emilchri/Zotero/storage/TH9PVRQZ/S154614401731671X.html}
}

@online{tiongPlugandPlayVQAZeroshot2022a,
  title = {Plug-and-{{Play VQA}}: {{Zero-shot VQA}} by {{Conjoining Large Pretrained Models}} with {{Zero Training}}},
  shorttitle = {Plug-and-{{Play VQA}}},
  author = {Tiong, Anthony Meng Huat and Li, Junnan and Li, Boyang and Savarese, Silvio and Hoi, Steven C. H.},
  date = {2022-11-16},
  eprint = {2210.08773},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.08773},
  url = {http://arxiv.org/abs/2210.08773},
  urldate = {2022-11-28},
  abstract = {Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5\% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1\% on GQA over FewVLM with 740M PLM parameters. Code is released at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,GradCAM,VQA},
  file = {/Users/emilchri/Zotero/storage/V5YTAZM5/Tiong et al. - 2022 - Plug-and-Play VQA Zero-shot VQA by Conjoining Lar.pdf;/Users/emilchri/Zotero/storage/V99KMHHL/2210.html}
}

@article{tjoaSurveyExplainableArtificial2021,
  title = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}}): {{Toward Medical XAI}}},
  shorttitle = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Tjoa, Erico and Guan, Cuntai},
  date = {2021-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {11},
  pages = {4793--4813},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3027314},
  abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Artificial intelligence,Explainable artificial intelligence (XAI),interpretability,Machine learning,machine learning (ML),Machine learning algorithms,medical information system,Medical information systems,survey},
  file = {/Users/emilchri/Zotero/storage/ACDPS3DS/Tjoa og Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf;/Users/emilchri/Zotero/storage/G92XNF3F/Tjoa og Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf;/Users/emilchri/Zotero/storage/R2HRIN58/9233366.html}
}

@inproceedings{tonekaboniWhatCliniciansWant2019,
  title = {What {{Clinicians Want}}: {{Contextualizing Explainable Machine Learning}} for {{Clinical End Use}}},
  shorttitle = {What {{Clinicians Want}}},
  booktitle = {Proceedings of the 4th {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D. and Goldenberg, Anna},
  date = {2019-10-28},
  pages = {359--380},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v106/tonekaboni19a.html},
  urldate = {2022-10-31},
  abstract = {Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians’ trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the eld suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians’ trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.},
  eventtitle = {Machine {{Learning}} for {{Healthcare Conference}}},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/KXB6F5AA/Tonekaboni et al. - 2019 - What Clinicians Want Contextualizing Explainable .pdf}
}

@online{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  date = {2023-02-27},
  eprint = {2302.13971},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2023-03-28},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,large language models,llm},
  file = {/Users/emilchri/Zotero/storage/ZZI99FLN/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;/Users/emilchri/Zotero/storage/5B4XGKBT/2302.html}
}

@inproceedings{tranRichImageCaptioning2016,
  title = {Rich {{Image Captioning}} in the {{Wild}}},
  author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian and Carapcea, Cornelia and Thrasher, Chris and Buehler, Chris and Sienkiewicz, Chris},
  date = {2016},
  pages = {49--56},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/html/Tran_Rich_Image_Captioning_CVPR_2016_paper.html},
  urldate = {2022-10-25},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {/Users/emilchri/Zotero/storage/QP242BQZ/Tran et al. - 2016 - Rich Image Captioning in the Wild.pdf;/Users/emilchri/Zotero/storage/NNWK37TZ/Tran_Rich_Image_Captioning_CVPR_2016_paper.html}
}

@inproceedings{turingComputingMachineryItelligence,
  title = {Computing {{Machinery}} and {{Itelligence}}},
  author = {Turing, Alan M.},
  doi = {10.1093%2Fmind%2FLIX.236.433},
  url = {https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf},
  urldate = {2023-04-30},
  file = {/Users/emilchri/Zotero/storage/8UQ289DP/turing.pdf}
}

@article{turingComputingMachineryItelligence1950,
  title = {Computing {{Machinery}} and {{Itelligence}}},
  author = {Turing, Alan M.},
  date = {1950-10-01},
  journaltitle = {Mind},
  shortjournal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  urldate = {2023-04-30},
  keywords = {alan,turing},
  file = {/Users/emilchri/Zotero/storage/BJA9UMRW/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/Users/emilchri/Zotero/storage/7CEGWZVF/986238.html}
}

@online{turingSystemsLogicBased1938,
  title = {Systems of Logic Based on Ordinals},
  author = {Turing, Alan M.},
  date = {1938},
  url = {https://web.archive.org/web/20121023103503/https://webspace.princeton.edu/users/jedwards/Turing%20Centennial%202012/Mudd%20Archive%20files/12285_AC100_Turing_1938.pdf},
  urldate = {2023-04-29},
  annotation = {Princeton University. p. 8. Archived from the original https://webspace.princeton.edu/users/jedwards/Turing\%20Centennial\%202012/Mudd\%20Archive\%20files/12285\_AC100\_Turing\_1938.pdf},
  file = {/Users/emilchri/Zotero/storage/VMEG383D/2012 - Wayback Machine.pdf}
}

@online{UnderstandingConvolutionalNeural2023,
  title = {Understanding {{Convolutional Neural Networks}}: {{A Complete Guide}}},
  shorttitle = {Understanding {{Convolutional Neural Networks}}},
  date = {2023-01-18T21:59:14+00:00},
  url = {https://learnopencv.com/understanding-convolutional-neural-networks-cnn/},
  urldate = {2023-05-01},
  abstract = {In this post, we will introduce many concepts associated with Convolutional Neural Networks (CNN) in the context of an image classification problem.},
  langid = {american},
  keywords = {cnn,figure,image,vgg},
  file = {/Users/emilchri/Zotero/storage/6A4IS3FM/understanding-convolutional-neural-networks-cnn.html}
}

@online{universityAncientMythsReveal2019,
  title = {Ancient Myths Reveal Early Fantasies about Artificial Life},
  author = {University, Stanford},
  date = {2019-02-28T13:47:52-08:00},
  url = {https://news.stanford.edu/2019/02/28/ancient-myths-reveal-early-fantasies-artificial-life/},
  urldate = {2023-04-28},
  abstract = {In her latest research, Stanford classics scholar Adrienne Mayor highlights ancient Greek myths that contained ideas about creating artificial, lifelike creatures.},
  langid = {english},
  organization = {{Stanford News}},
  keywords = {bronze,greek,mythology,robots,talos},
  file = {/Users/emilchri/Zotero/storage/2BAZQZKW/ancient-myths-reveal-early-fantasies-artificial-life.html}
}

@article{valiantKnowledgeInfusionPursuit2008,
  title = {Knowledge {{Infusion}}: {{In Pursuit}} of {{Robustness}} in {{Artiﬁcial Intelligence}}},
  author = {Valiant, Leslie G},
  date = {2008},
  journaltitle = {IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science},
  abstract = {Endowing computers with the ability to apply commonsense knowledge with humanlevel performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one approach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some uncertainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that subsequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/PYK58VRX/Valiant - Knowledge Infusion In Pursuit of Robustness in Ar.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-03-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {attention,transformer},
  file = {/Users/emilchri/Zotero/storage/SDSH6EMU/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@online{Vgg16TorchvisionMain,
  title = {Vgg16 — {{Torchvision}} Main Documentation},
  url = {https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html},
  urldate = {2023-05-11},
  keywords = {pytorch,vgg16},
  file = {/Users/emilchri/Zotero/storage/TSPFMBRP/torchvision.models.vgg16.html}
}

@online{vincentMetaPowerfulAI2023,
  title = {Meta’s Powerful {{AI}} Language Model Has Leaked Online — What Happens Now?},
  author = {Vincent, James},
  date = {2023-03-08T13:15:00},
  url = {https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse},
  urldate = {2023-04-07},
  abstract = {Meta’s leaked AI language model could be a big deal.},
  langid = {american},
  organization = {{The Verge}},
  keywords = {leak,llama,llm,meta,verge},
  file = {/Users/emilchri/Zotero/storage/VLBJXLE4/meta-ai-language-model-llama-leak-online-misuse.html}
}

@article{vinyalsShowTellLessons2017,
  title = {Show and {{Tell}}: {{Lessons Learned}} from the 2015 {{MSCOCO Image Captioning Challenge}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2017-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {4},
  pages = {652--663},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2587640},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Computational modeling,Computer vision,Image captioning,language model,Logic gates,Natural languages,recurrent neural network,Recurrent neural networks,sequence-to-sequence,Training,Visualization},
  file = {/Users/emilchri/Zotero/storage/8JSIPJX6/Vinyals et al. - 2017 - Show and Tell Lessons Learned from the 2015 MSCOC.pdf;/Users/emilchri/Zotero/storage/K3FQNBNN/7505636.html}
}

@inproceedings{vinyalsShowTellNeural2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2015},
  pages = {3156--3164},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html},
  urldate = {2022-10-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/8YC3V834/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf;/Users/emilchri/Zotero/storage/Q72N6XK5/Vinyals_Show_and_Tell_2015_CVPR_paper.html}
}

@software{wangAlpacaLoRA2023,
  title = {Alpaca-{{LoRA}}},
  author = {Wang, Eric J.},
  date = {2023-04-08T15:35:21Z},
  origdate = {2023-03-13T21:52:36Z},
  url = {https://github.com/tloen/alpaca-lora},
  urldate = {2023-04-08},
  abstract = {Instruct-tune LLaMA on consumer hardware},
  keywords = {alpaca,git,github,lora}
}

@online{wangEndtoEndTransformerBased2022,
  title = {End-to-{{End Transformer Based Model}} for {{Image Captioning}}},
  author = {Wang, Yiyu and Xu, Jungang and Sun, Yingfei},
  date = {2022-03-29},
  eprint = {2203.15350},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.15350},
  urldate = {2022-11-06},
  abstract = {CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt SwinTransformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2\% (single model) and 141.0\% (ensemble of 4 models) CIDEr scores on `Karpathy' offline test split and 136.0\% (c5) and 138.3\% (c40) CIDEr scores on the official online test server. Trained models and source code will be released.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/U5VJ2WZX/Wang et al. - 2022 - End-to-End Transformer Based Model for Image Capti.pdf;/Users/emilchri/Zotero/storage/WU3Z3JYC/2203.html}
}

@article{wangMachineLearningRadiology2012,
  title = {Machine Learning and Radiology},
  author = {Wang, Shijun and Summers, Ronald M.},
  date = {2012-07-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Medical Image Analysis},
  volume = {16},
  number = {5},
  pages = {933--951},
  issn = {1361-8415},
  doi = {10.1016/j.media.2012.02.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1361841512000333},
  urldate = {2023-04-28},
  abstract = {In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers.},
  langid = {english},
  keywords = {Computer-aided detection and diagnosis,Image segmentation,Machine learning,ML,radiology,Radiology,Survey},
  file = {/Users/emilchri/Zotero/storage/GJI3QGGI/Wang og Summers - 2012 - Machine learning and radiology.pdf;/Users/emilchri/Zotero/storage/2YK95K2Q/S1361841512000333.html}
}

@online{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2022-12-20},
  eprint = {2212.10560},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10560},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2023-04-07},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/emilchri/Zotero/storage/F47RTX7B/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self G.pdf;/Users/emilchri/Zotero/storage/PT6H2PZG/2212.html}
}

@online{wangSimVLMSimpleVisual2022,
  title = {{{SimVLM}}: {{Simple Visual Language Model Pretraining}} with {{Weak Supervision}}},
  shorttitle = {{{SimVLM}}},
  author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  date = {2022-05-15},
  eprint = {2108.10904},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.10904},
  url = {http://arxiv.org/abs/2108.10904},
  urldate = {2023-05-05},
  abstract = {With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74\% vqa-score), NLVR2 (+1.17\% accuracy), SNLI-VE (+1.37\% accuracy) and image captioning tasks (+10.1\% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,learning,supervision,transfer,weak},
  file = {/Users/emilchri/Zotero/storage/2WT88CFW/Wang et al. - 2022 - SimVLM Simple Visual Language Model Pretraining w.pdf;/Users/emilchri/Zotero/storage/R3K56VSH/2108.html}
}

@online{wangYOLOv7TrainableBagoffreebies2022,
  title = {{{YOLOv7}}: {{Trainable}} Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
  shorttitle = {{{YOLOv7}}},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  date = {2022-07-06},
  eprint = {2207.02696},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.02696},
  url = {http://arxiv.org/abs/2207.02696},
  urldate = {2023-04-09},
  abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/emilchri/Zotero/storage/8QJIHWXW/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-o.pdf;/Users/emilchri/Zotero/storage/B24ALPU5/2207.html}
}

@unpublished{watsonAttackagnosticAdversarialDetection2021,
  title = {Attack-Agnostic {{Adversarial Detection}} on {{Medical Data Using Explainable Machine Learning}}},
  author = {Watson, Matthew and Moubayed, Noura Al},
  date = {2021-05-05},
  eprint = {2105.01959},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.01959},
  urldate = {2022-05-29},
  abstract = {Explainable machine learning has become increasingly prevalent, especially in healthcare where explainable models are vital for ethical and trusted automated decision making. Work on the susceptibility of deep learning models to adversarial attacks has shown the ease of designing samples to mislead a model into making incorrect predictions. In this work, we propose a model agnostic explainability-based method for the accurate detection of adversarial samples on two datasets with different complexity and properties: Electronic Health Record (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR datasets, we report a detection accuracy of 77\% against the Longitudinal Adversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88\%; significantly improving on the state of the art of adversarial detection in both datasets by over 10\% in all settings. We propose an anomaly detection based method using explainability techniques to detect adversarial samples which is able to generalise to different attack methods without a need for retraining.},
  version = {1},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,I.2,I.4},
  file = {/Users/emilchri/Zotero/storage/9XCD2GTA/Watson og Moubayed - 2021 - Attack-agnostic Adversarial Detection on Medical D.pdf;/Users/emilchri/Zotero/storage/JFYEUUZX/2105.html}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-04-12},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/emilchri/Zotero/storage/CSUITJZR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf;/Users/emilchri/Zotero/storage/CQ7ARLGU/2201.html}
}

@inproceedings{weldChallengeCraftingIntelligible2019,
  title = {The Challenge of Crafting Intelligible Intelligence},
  author = {Weld, Daniel S and Bansal, Gagan},
  date = {2019},
  series = {6},
  volume = {62},
  pages = {70--79},
  publisher = {{ACM New York, NY, USA}},
  url = {https://par.nsf.gov/servlets/purl/10074856},
  urldate = {2022-11-25},
  eventtitle = {Communications of the {{ACM}}},
  file = {/Users/emilchri/Zotero/storage/U3M2M6JL/10074856.pdf}
}

@online{WhatUnsupervisedLearning,
  title = {What Is {{Unsupervised Learning}}? | {{IBM}}},
  shorttitle = {What Is {{Unsupervised Learning}}?},
  url = {https://www.ibm.com/topics/unsupervised-learning},
  urldate = {2023-02-28},
  abstract = {Learn how unsupervised learning works and how it can be used to explore and cluster data},
  langid = {american},
  keywords = {machine learning}
}

@article{wickramanayakeFLEXFaithfulLinguistic2019,
  title = {{{FLEX}}: {{Faithful Linguistic Explanations}} for {{Neural Net Based Model Decisions}}},
  shorttitle = {{{FLEX}}},
  author = {Wickramanayake, Sandareka and Hsu, Wynne and Lee, Mong Li},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {2539--2546},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33012539},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4100},
  urldate = {2022-09-12},
  abstract = {Explaining the decisions of a Deep Learning Network is imperative to safeguard end-user trust. Such explanations must be intuitive, descriptive, and faithfully explain why a model makes its decisions. In this work, we propose a framework called FLEX (Faithful Linguistic EXplanations) that generates post-hoc linguistic justifications to rationalize the decision of a Convolutional Neural Network. FLEX explains a model’s decision in terms of features that are responsible for the decision. We derive a novel way to associate such features to words, and introduce a new decision-relevance metric that measures the faithfulness of an explanation to a model’s reasoning. Experiment results on two benchmark datasets demonstrate that the proposed framework can generate discriminative and faithful explanations compared to state-of-the-art explanation generators. We also show how FLEX can generate explanations for images of unseen classes as well as automatically annotate objects in images.},
  issue = {01},
  langid = {english},
  keywords = {Andrea},
  file = {/Users/emilchri/Zotero/storage/U65TKSV9/Wickramanayake et al. - 2019 - FLEX Faithful Linguistic Explanations for Neural .pdf}
}

@article{willeminkPreparingMedicalImaging2020,
  title = {Preparing {{Medical Imaging Data}} for {{Machine Learning}}},
  author = {Willemink, Martin J. and Koszek, Wojciech A. and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R. and Summers, Ronald M. and Rubin, Daniel L. and Lungren, Matthew P.},
  date = {2020-04},
  journaltitle = {Radiology},
  volume = {295},
  number = {1},
  pages = {4--15},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.2020192224},
  url = {https://pubs.rsna.org/doi/full/10.1148/radiol.2020192224},
  urldate = {2023-05-05},
  abstract = {Artificial intelligence (AI) continues to garner substantial interest in medical imaging. The potential applications are vast and include the entirety of the medical imaging life cycle from image creation to diagnosis to outcome prediction. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations). Current supervised AI methods require a curation process for data to optimally train, validate, and test algorithms. Currently, most research groups and industry have limited data access based on small sample sizes from small geographic areas. In addition, the preparation of data is a costly and time-intensive process, the results of which are algorithms with limited utility and poor generalization. In this article, the authors describe fundamental steps for preparing medical imaging data in AI algorithm development, explain current limitations to data curation, and explore new approaches to address the problem of data availability. © RSNA, 2020},
  keywords = {data,medical,quality},
  file = {/Users/emilchri/Zotero/storage/QU5DL24H/Willemink et al. - 2020 - Preparing Medical Imaging Data for Machine Learnin.pdf}
}

@inproceedings{xuFinegrainedImageClassification2018,
  title = {Fine-Grained {{Image Classification}} by {{Visual-Semantic Embedding}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xu, Huapeng and Qi, Guilin and Li, Jingjing and Wang, Meng and Xu, Kang and Gao, Huan},
  date = {2018-07},
  pages = {1043--1049},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/145},
  url = {https://www.ijcai.org/proceedings/2018/145},
  urldate = {2023-02-27},
  abstract = {This paper investigates a challenging problem, which is known as fine-grained image classification (FGIC). Different from conventional computer vision problems, FGIC suffers from the large intraclass diversities and subtle inter-class differences. Existing FGIC approaches are limited to explore only the visual information embedded in the images. In this paper, we present a novel approach which can use handy prior knowledge from either structured knowledge bases or unstructured text to facilitate FGIC. Specifically, we propose a visualsemantic embedding model which explores semantic embedding from knowledge bases and text, and further trains a novel end-to-end CNN framework to linearly map image features to a rich semantic embedding space. Experimental results on a challenging large-scale UCSD Bird-200-2011 dataset verify that our approach outperforms several stateof-the-art methods with significant advances.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/4RT8GIRQ/Xu et al. - 2018 - Fine-grained Image Classification by Visual-Semant.pdf}
}

@inproceedings{xuShowAttendTell2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  date = {2015-06-01},
  pages = {2048--2057},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/xuc15.html},
  urldate = {2023-05-03},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {attention,captioning,image,lstm},
  file = {/Users/emilchri/Zotero/storage/8CD47N43/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Genera.pdf}
}

@article{yangEmpiricalStudyGPT32022,
  title = {An {{Empirical Study}} of {{GPT-3}} for {{Few-Shot Knowledge-Based VQA}}},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {3},
  pages = {3081--3089},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i3.20215},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20215},
  urldate = {2023-03-28},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3’s power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  issue = {3},
  langid = {english},
  keywords = {Knowledge Representation And Reasoning (KRR),llm,VQA},
  file = {/Users/emilchri/Zotero/storage/XW9CLXLN/Yang et al. - 2022 - An Empirical Study of GPT-3 for Few-Shot Knowledge.pdf}
}

@article{yangImageCaptioningAsking2019,
  title = {Image {{Captioning}} by {{Asking Questions}}},
  author = {Yang, Xiaoshan and Xu, Changsheng},
  date = {2019-08-12},
  journaltitle = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  shortjournal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  volume = {15},
  pages = {1--19},
  issn = {1551-6857, 1551-6865},
  doi = {10.1145/3313873},
  url = {https://dl.acm.org/doi/10.1145/3313873},
  urldate = {2022-10-03},
  abstract = {Image captioning and visual question answering are typical tasks that connect computer vision and natural language processing. Both of them need to effectively represent the visual content using computer vision methods and smoothly process the text sentence using natural language processing skills. The key problem of these two tasks is to infer the target result based on the interactive understanding of the word sequence and the image. Though they practically use similar algorithms, they are studied independently in the past few years. In this article, we attempt to exploit the mutual correlation between these two tasks. We propose the first VQA-improved image-captioning method that transfers the knowledge learned from the VQA corpora to the image-captioning task. A VQA model is first pretrained on image--question--answer instances. Then, the pretrained VQA model is used to extract VQA-grounded semantic representations according to selected free-form open-ended visual question--answer pairs. The VQA-grounded features are complementary to the visual features, because they interpret images from a different perspective. We incorporate the VQA model into the image-captioning model by adaptively fusing the VQA-grounded feature and the attended visual feature. We show that such simple VQA-improved image-captioning (VQA-IIC) models perform better than conventional image-captioning methods on large-scale public datasets.},
  issue = {2s},
  langid = {english},
  file = {/Users/emilchri/Zotero/storage/EJ5J4WTM/Yang og Xu - 2019 - Image Captioning by Asking Questions.pdf}
}

@online{yangMMREACTPromptingChatGPT2023,
  title = {{{MM-REACT}}: {{Prompting ChatGPT}} for {{Multimodal Reasoning}} and {{Action}}},
  shorttitle = {{{MM-REACT}}},
  author = {Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  date = {2023-03-20},
  eprint = {2303.11381},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.11381},
  url = {http://arxiv.org/abs/2303.11381},
  urldate = {2023-03-28},
  abstract = {We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/},
  pubstate = {preprint},
  keywords = {ChatGPT,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GPT,Image,image caption,Large language models,LLM},
  file = {/Users/emilchri/Zotero/storage/RSU35DGL/Yang et al. - 2023 - MM-REACT Prompting ChatGPT for Multimodal Reasoni.pdf;/Users/emilchri/Zotero/storage/JNS3IY63/2303.html}
}

@inproceedings{youImageCaptioningSemantic2016,
  title = {Image {{Captioning With Semantic Attention}}},
  author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  date = {2016},
  pages = {4651--4659},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/You_Image_Captioning_With_CVPR_2016_paper.html},
  urldate = {2023-01-17},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/KS8HNH5G/You et al. - 2016 - Image Captioning With Semantic Attention.pdf}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016},
  pages = {2921--2929},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html},
  urldate = {2022-10-06},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/emilchri/Zotero/storage/CCHCT8LX/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf;/Users/emilchri/Zotero/storage/XHE5HEMV/Zhou_Learning_Deep_Features_CVPR_2016_paper.html}
}

@article{zhouUnifiedVisionLanguagePreTraining2020,
  title = {Unified {{Vision-Language Pre-Training}} for {{Image Captioning}} and {{VQA}}},
  author = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {13041--13049},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i07.7005},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/7005},
  urldate = {2022-10-18},
  abstract = {This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.},
  issue = {07},
  langid = {english},
  keywords = {image caption,VQA},
  file = {/Users/emilchri/Zotero/storage/VAKY2LXJ/Zhou et al. - 2020 - Unified Vision-Language Pre-Training for Image Cap.pdf}
}

@article{zhuangComprehensiveSurveyTransfer2021,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  date = {2021-01},
  journaltitle = {Proceedings of the IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2020.3004555},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Adaptation models,Covariance matrices,Data models,Domain adaptation,interpretation,learning,machine learning,Machine learning,Semisupervised learning,transfer,transfer learning,Transfer learning},
  file = {/Users/emilchri/Zotero/storage/7MUD3RR4/Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf;/Users/emilchri/Zotero/storage/IJ9KA3YR/9134370.html}
}

@online{zhuVisual7WGroundedQuestion2016,
  title = {{{Visual7W}}: {{Grounded Question Answering}} in {{Images}}},
  shorttitle = {{{Visual7W}}},
  author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  date = {2016-04-09},
  eprint = {1511.03416},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.03416},
  url = {http://arxiv.org/abs/1511.03416},
  urldate = {2023-05-08},
  abstract = {We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,vqa},
  file = {/Users/emilchri/Zotero/storage/NNX9JYBA/Zhu et al. - 2016 - Visual7W Grounded Question Answering in Images.pdf;/Users/emilchri/Zotero/storage/LKWTUNUK/1511.html}
}

@online{zouGeneralizedDecodingPixel2022,
  title = {Generalized {{Decoding}} for {{Pixel}}, {{Image}}, and {{Language}}},
  author = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
  date = {2022-12-21},
  eprint = {2212.11270},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.11270},
  url = {http://arxiv.org/abs/2212.11270},
  urldate = {2023-04-10},
  abstract = {We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly. X-Decodert takes as input two types of queries: (i) generic non-semantic queries and (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks. Further, our design enables seamless interactions across tasks at different granularities and brings mutual benefits by learning a common and rich pixel-level visual-semantic understanding space, without any pseudo-labeling. After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. Notably, it achieves (1) state-of-the-art results on open-vocabulary segmentation and referring segmentation on eight datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and VL tasks; and (3) flexibility for efficient finetuning and novel task composition (e.g., referring captioning and image editing). Code, demo, video, and visualization are available at https://x-decoder-vl.github.io.},
  pubstate = {preprint},
  keywords = {captioning,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,image,segmentation},
  file = {/Users/emilchri/Zotero/storage/PKWFBG29/Zou et al. - 2022 - Generalized Decoding for Pixel, Image, and Languag.pdf;/Users/emilchri/Zotero/storage/KNT4JKZN/2212.html}
}
