\label{sec:1_5_ethical_considerations}

\begin{comment}
Ethical considerations in research are a set of principles that guide your research designs and practices. Scientists and researchers must always adhere to a certain code of conduct when collecting data from people. For example, the goals of human research often include understanding real-life phenomena, studying effective treatments, investigating behaviors, and improving lives in other ways. What you decide to research and how you conduct that research involve key ethical considerations such as:

i) protect the rights of research participants (privacy); 
ii) enhance research validity, 
iii) maintain scientific integrity; etc. Thus include here a short description of an assessment of any relevant potential ethical considerations.
\end{comment}

\section{Ethical Considerations}



% Into and identify bias (main)
With more intuitive explanations provided by an \gls{xai} system, researchers and users have better insight into the inner workings and arguments of the underlying \gls{ai} method. This, in turn, will address one of the ethical concerns, which is dataset bias. 

\subsubsection{Dataset Bias}
\gls{ai} raises a number of ethical concerns, particularly when used in high-stakes domains such as healthcare, finance, and criminal justice. The concern is not specifically the error of the \gls{ai} model but rather the bias of datasets. Since \gls{ai} is used to condense complex datasets into understandable predictions, it becomes an inherited ethical issue if models learn unfair biases. 
Datasets used in deep learning are often large and complex, and \gls{ai} models are used to extract knowledge from the dataset. Therefore, it can be difficult for researchers and users to identify biases used in training the \gls{ai} models, especially when they do not explain why this prediction is correct. 

% Example of bias in prediction
One ethical problem is that \gls{ai} systems can make predictions that are directed against certain groups of people, such as those from marginalized communities. This could occur if the training data used to develop the system is biased or if the system's decision-making process is not transparent. \gls{xai} can therefore be an important tool to gain insights into our own biases and to help researchers and users avoid making decisions based on false premises. 


% Another ethical issue with \gls{xai} is the possibility that the system will be used for surveillance or other privacy-intrusive purposes. For example, an \gls{xai} system used to monitor people in public spaces could raise concerns about civil liberties and privacy rights. In addition, \gls{xai} systems capable of understanding and interpreting visual or textual information could also be used to profile individuals based on their race, gender, age, or other personal characteristics.



% Bias in VQA
\gls{vqa} datasets and image caption tasks also raise ethical concerns about bias and fairness. The task of \gls{vqa} and image caption rely on data in order to learn to generate textual explanations for visual inputs. However, if the training data is biased in any way, the resulting system will also be biased. For example, if a \gls{vqa} dataset contains mostly humans with one skin color, the system may not work as well with images of people of a different color. 
Similarly, if a dataset used for captioning contains a relationship between sexes and professions, the same bias will be carried over to the model.

% Another ethical issue related to \gls{vqa} and captions is the potential of the generated captions and explanations to perpetuate stereotypes or reinforce societal prejudice. For example, if a system is trained on images that portray people of a certain gender or race in a certain way, it can generate captions that perpetuate those stereotypes. In addition, when used in certain applications, such as facial recognition, the system could reinforce societal prejudice and perpetuate discrimination.

% How VQA bias can be mitigated
The work of Hirota et al. \cite{hirotaGenderRacialBias2022} analyzed gender and racial bias in five popular \gls{vqa} datasets and found unfavorable stereotypes in the samples. 
Various possible solutions to address this issue were explored in their work.
This includes not asking questions about race and sex when not required when making a dataset and collecting a more standardized distribution related to race and sex. 
They also propose an alternative to the manual screening that some \gls{vqa} datasets use since not all can justify the cost of manual annotation. The proposed solution to this automatic screening, followed by ethical guidance for annotators, and lastly, a feedback platform for users.


% LLM
\subsubsection{Large Language Models}
Regarding \glspl{llm}, there are some ethical considerations to address. \glspl{llm} are trained on a large corpus of text, most often collected from the Internet. The data gathered across the web are mostly written by humans, and the \glspl{llm} are trained on this wast dataset to extract a more general understanding of human knowledge and present this with a well-structured syntax. With the advancements and availability of models such as \gls{gpt}-4 \cite{openaiGPT4TechnicalReport2023}, combined with a user-friendly and easy-to-use user interface, like the one used by ChatGPT \cite{ChatGPT}, the public has never before had an advanced \gls{ai} so accessible in their everyday life. Given that users come from different backgrounds and cultures, the \gls{llm} should be able to adapt to its users to close the gap between user and \gls{ai} alignment. 
% something about the alignment problem.
\gls{ai} alignment is a subfield of \gls{ai} safety, with the goal of building safe and reliable \gls{ai} methods \cite{amodeiConcreteProblemsAI2016}. 
% Inner and outer alignment
The alignment problem is defined as the task of aligning the goals of the humans creating the system with the goals of the \gls{ai} system. Outer alignment is the overall goal of the system, and inner alignment ensures that the system behaves as expected in a robust manner \cite{ngoAlignmentProblemDeep2023}.

% Something about hallucinations

\glspl{llm} have billions of parameters and incomprehensible feature space for humans to understand fully. These models are mostly built today using the Transformer model proposed by Vaswani et al. \cite{vaswaniAttentionAllYou2017}, but because of their size and scale, they are very complex to understand \cite{liptonMythosModelInterpretability2017a}. 


% Lex Podcast with Sam
% Sam Altman, the founder and CEO of OpenAI, has in an interview said that solving the alignment problem in \glspl{llm} is not a separate task from improving the model, but solving the alignment problem will also make the models better and more usable for researchers and users. This statement is backed by research, from among other ??? and ???. Solving the alignment problem will give models that better align with humans' overall goal and have an intrinsic goal of following these goals. This can make models easier to adapt and fine-tune to different specific areas of interest, which also will make them more useful and widespread. 


% The open letter to pause training large models
There have been multiple attempts recently to slow the growth of or to regulate deep neural networks, like \glspl{llm}. The goal of this is to have time to address ethical and technical concerns before a new, larger, and better model gets released \cite{PauseGiantAI, REGULATIONEUROPEANPARLIAMENT2021}. The reasons why some argue not to continually make new models are many, but the most prominent concern is to understand better the models already developed. With a better understanding of these models, researchers are better equipped to tune models to better align with the overall goal of humans. An understanding of the inner workings of a model, including an \gls{llm}, can also give insight into how to make the model predictions more transparent, fair, and efficient. 

Ensuring the ability to detect \gls{ai}-generated content is a crucial aspect that deserves significant attention. As \gls{ai} becomes more proficient at autonomously searching the web and generating new datasets, it becomes increasingly important for humans to determine the origin of information and assess its reliability and credibility. These tools play a crucial role in providing transparency and allow users to differentiate between content generated by humans and \glspl{ai}. By integrating reliable detection tools, individuals can make informed decisions based on awareness of whether the information they encounter comes from \gls{ai} algorithms or human sources.


% Carbon Footprint
\subsubsection{Carbon Footprint}
Training large \gls{ai} models takes a considerable amount of energy and therefore produces a lot of greenhouse emissions.  
In the paper introducing the \gls{llama} model \cite{touvronLLaMAOpenEfficient2023}, which is the base model of the Alpaca-VQA model used in this work, they calculate the carbon footprint. This footprint can be estimated by the formula in \autoref{eq:carbon_footprint}.

\begin{equation}
    \text{Wh} = \text{GPU hours} \times (\text{GPU power consumption}) \times \text{PUE}
    \label{eq:carbon_footprint}
\end{equation}

Where \textit{PUE} is Power Usage Effectiveness, which is set to 1.1.
In order to generalize the carbon footprint, the authors of \gls{llama} use the US national averages carbon intensity factor corresponding to 0.385 kg CO\textsubscript{2}eq/KWh, where CO\textsubscript{2}eq is Carbon Dioxide Equivalents. By generalizing the carbon emissions per watt, it is easier to compare models trained in different locations.
The formula used for calculating the metric tonnes of CO\textsubscript{2}eq can be done with the formula in \autoref{eq:tonnes_co2}.

\begin{equation}
    \text{Metric tonnes of CO\textsubscript{2}eq} = \text{MWh used } \times 0.385 \text{kg CO\textsubscript{2}eq/KWh}
    \label{eq:tonnes_co2}
\end{equation}

 
Following this equation, the researchers estimate the development of \gls{llama} to use 2,638 MW in total, corresponding to 1,015 metric tones of CO\textsubscript{2}eq.
This is roughly the same amount as 216 passenger vehicles emit in a whole year \cite{usepaGreenhouseGasEmissions2016}.

In order to minimize the carbon footprint of the \gls{llm} used in this thesis,
the weights used are not from the original \gls{llama} but from the Stanford Alpaca model. 
The model is fine-tuned using optimizations that freeze the internal weights of the model, and a supplementary weight matrix is trained. This implementation is discussed in further detail in \autoref{sec:3_alpaca_vqa}. Also, the experiments follow an iterative design, which helps reduce unnecessary computing time and emissions.
Following the same formula, the fine-tuning of the Alpaca-VQA model used in this work can be calculated. 
It is estimated that the total \gls{gpu} processing time to be 30 hours, with \glspl{gpu} using a maximum of 250 W \cite{Nvidiaa100datasheet}. Using the same PUE as \gls{llama}, the total power consumption is 7.5 kW. 
Using the carbon emissions from the US, the experiments in this work have emitted 2.888 kg of CO\textsubscript{2}eq. 
However, as the servers were located in Norway, the emissions, in reality, were much less. It is estimated that the average emissions per kWh in Norway in 2022 were 0.019 kg CO\textsubscript{2}eq/KWh \cite{LavtKlimagassutslippKnyttet}. Therefore, the actual emissions are 0.143 kg of CO\textsubscript{2}eq, which is less than a kilometer driven in a fossil car \cite{HvaPavirkerUtslipp2017}.


As the fine-tuning used to conduct the experiments in this work is dependent on the base model, the emissions should therefore be seen in conjunction with the final model. 
Therefore it is an ethical concern to keep making larger models that consume more energy, especially when the energy used is not from renewable sources.


To address the issue of large computational needs, processor designers have made specialized chips for machine learning \cite{byunBenchmarkingDataAnalysis2017, jouppiMotivationEvaluationFirst2018, elsterNvidiaHopperGPU2022, kasperekComparisonUsabilityApple2022}. These can assist the development of even better models that also have reduced power consumption. An additional benefit of having reduced power consumption is that more lightweight devices can train and run models that help democratize machine learning. 






% Transformers output linearly, with no internal dialog (reasoning)

% Models are trained as imitators, humans are good at giving human features to non-humans (anthropomorphism). 

% Reinforcement learning from human feedback (RLHF) teaches models to give answers that the human appreciates and thumbs up, giving an incentive to answer something that the human would like to hear, or harmonizes with the humans' previous beliefs. Truth does not care if the human likes it or not, so giving this incentive would make the model pay less attention to giving correct answers, but rather giving answers humans like. 



% EU - The AI Act

% If humans continue to hype the methods recently proposed, like ChatGPT, but the models do not fulfill the hype, there may be another AI winter. One of the main reasons for the two AI winters, as discussed in context in chapter 2 - is background history.


\begin{comment}
    Tell how this thesis uses the ethical information above when doing research.
\end{comment}

In summary, \gls{ai}, datasets \gls{vqa}, and \glspl{llm} raise important ethical concerns about bias, fairness, and carbon footprint. Mainly these concerns stem from reliance on training data, which may be biased or perpetuate stereotypes. 
Researchers and users of these tools must consider the ethical concerns and work towards developing \gls{xai} systems that are transparent, fair, and respectful of humans and the climate.