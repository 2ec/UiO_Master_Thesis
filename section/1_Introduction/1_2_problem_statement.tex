\begin{comment}
In a short and precise way, state what your research is about in this thesis. It can be in the form of a (set of) research questions, goals/aims, or objectives (or a mix) - but it should clearly state what the problems or challenges you are addressing.

Alternatively, one can state a research hypothesis, but if so, it should follow the rules of what a hypothesis is. A hypothesis is a statement that introduces a research question and proposes an expected result. It is an integral part of the scientific method that forms the basis of scientific experiments. Therefore, you need to be careful and thorough when building your hypothesis, following the "rules".
\end{comment}

\section{Problem Statement}
    \label{sec:1_2_problem_statement}

% Intro
To be truly trustworthy, machine learning and computer vision must be understood. Researchers need to understand the model's inner workings to make improvements and uncover biases in the dataset. Users and domain experts using the models must be confident that the model makes accurate predictions and uses meaningful features when evaluating the task. 
The goal when designing models should be that they are able to achieve state-of-the-art performance without compromising the interpretability and explainability of their predictions.
% Some bridge here to transition over to the research questions
The more complex the models become in the pursuit of greater accuracy, the more difficult it becomes to interpret them. This also applies to multimodal models, as they combine information from multiple modalities into one prediction. However, since multimodal explanations can provide valuable and intuitive considerations, they are an important area of research.


\begin{comment}
% What this is and How I want to do it


% Questions to be answered
The main topics that will be explored to reach the research goal are: 
\begin{itemize}
    \item Does an explainable image caption model improve its answers when trained on a \gls{vqa} dataset?
    \item With only the explaining model choosing the captions, will the output be more intuitive for humans?
    \item Are the explanations, both captions, and answers from open-ended questions locally faithful to the underlying \gls{cnn} model?
    \item If there is enough time during this thesis, it would be an interesting task to see if the image caption and \gls{vqa} will perform better when using transfer learning  on a language model pretrained on a large language dataset. 
\end{itemize}

\end{comment}

    
% Research questions:
%\subsection{Research Questions}

This thesis will examine how explanatory models in the linguistic and visual domain can contribute to gaining new and important insights into the functioning of the \gls{ai} methods.\\
Formally, the goal can be written as follows:
\begin{itemize}
    \item To what extent can \gls{vqa} with explanatory models in different domains provide additional insights into the underlying data?
\end{itemize}

Although the following research questions are specified, they should be viewed as guidelines for the experiments rather than defining rules. 
This is because the overarching goal is to investigate how explanatory models can provide additional insights into different modalities. The goal is, therefore, to explore this topic in more detail to gain new insights rather than to answer a specific problem.


% FLEX-VQA
%\subsubsection{Visual Domain: FLEX-VQA}
Since the task of \gls{vqa} combines both linguistic and visual understanding, the visual part benefits from being explained, as it contains significant information regarding the task. 
A locally accurate explanation ensures that the reason is based on the underlying model's evaluation.
However, for an explanation to be locally accurate, some explanatory models work on features on a low level without explaining the task as a whole. 
As humans often evaluate the contents of an image as a whole instead of low-level features, the explanation model could benefit from using locally accurate features and extrapolating so that they describe the image on a high level.\\
More precisely, the research question for the visual domain can be formulated as follows:

\begin{itemize}
    \item Will the answers given by a \gls{vqa} system be more intuitively explained with additional locally accurate image descriptions?
\end{itemize}

With the knowledge from answering these questions, better explanations can be made, which makes it easier to develop and interact with models interpreting images. 


% Alpaca-VQA
%\subsubsection{Linguistic Domain: Alpaca-VQA}
To have an explanatory model in the linguistic domain, it should only evaluate text features. Since \glspl{llm} are trained on large text corpora, they can learn connections and gain valuable insights into how language works. \\
Specifically, the research questions in the linguistic field can be formulated as follows:

\begin{itemize}
    \item To which degree can an \gls{llm} fine-tuned on a new modality bring new insights from its pertaining?

    \item What insights can additional explanatory methods bring from an \gls{llm} after training is complete?
\end{itemize}

The insights gained from answering these questions can be used to create \glspl{llm} that bring knowledge from previous training to a specific task while being explained by supplementary explanatory methods. \glspl{llm} with additional methods that give an understanding of how they work helps researchers and users gain a more intuitive understanding of how these complex models work and interpret data samples. 





% What is the result
With these research questions answered, the knowledge gained from this work will make it easier to develop models that can give insight into how models understand broad concepts in vision and language. 
%This understanding can make computers understand a more complex worldview and help humans understand what it sees and value.
Further on, this will help humans understand what these complex models see and value, potentially leading to increased trust.


